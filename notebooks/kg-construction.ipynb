{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97493594",
   "metadata": {},
   "source": [
    "# Knowledge Graph construction and query with extracted software metadata\n",
    "\n",
    "This notebook first generates a knowledge graph from the information extracted about software repositories. It is later queried to assess the good practices followed by the extracted repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16cf3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import morph_kgc\n",
    "import pyoxigraph\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0436b4",
   "metadata": {},
   "source": [
    "## KG Construction\n",
    "The knowledge graph is generated using Morph-KGC, that uses RML mappings to transform the JSON file into RDF. This tool requires some configuration parameters, where we indicate the desired output serialisation and the name and path to the RML mapping file. Then, the kg is generated and stored as a oxigraph store in the variable `graph`, that it is also saved as a `.nq` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f142c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "             [CONFIGURATION]\n",
    "             output_format=N-QUADS\n",
    "             \n",
    "             [SOMEF-json]\n",
    "             mappings=../mappings/mapping-somef-star.ttl\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "053b4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | 2023-07-04 09:43:53,297 | 145 mapping rules retrieved.\n",
      "INFO | 2023-07-04 09:43:53,305 | Mappings processed in 1.267 seconds.\n",
      "INFO | 2023-07-04 09:44:03,132 | Number of triples generated in total: 19394.\n"
     ]
    }
   ],
   "source": [
    "graph = morph_kgc.materialize_oxigraph(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e1312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add(pyoxigraph.Quad(\n",
    "    pyoxigraph.NamedNode('https://w3id.org/okn/i/graph/20230628'),\n",
    "    pyoxigraph.NamedNode('http://purl.org/dc/terms/created'),\n",
    "    pyoxigraph.Literal('2023-06-28 00:00:00', datatype=pyoxigraph.NamedNode('http://www.w3.org/2001/XMLSchema#dateTime')),\n",
    "    pyoxigraph.NamedNode('https://w3id.org/okn/i/graph/default')))\n",
    "graph.add(pyoxigraph.Quad(\n",
    "    pyoxigraph.NamedNode('https://w3id.org/okn/i/graph/20230628'),\n",
    "    pyoxigraph.NamedNode('http://www.w3.org/ns/prov#wasAttributedTo'),\n",
    "    pyoxigraph.Literal('SOftware Metadata Extraction Framework (SOMEF)', datatype=pyoxigraph.NamedNode('http://www.w3.org/2001/XMLSchema#string')),\n",
    "    pyoxigraph.NamedNode('https://w3id.org/okn/i/graph/default')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5c94be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/aiglesias/GitHub/oeg-software-graph/data/somef-kg.nq', 'w') as result:\n",
    "    result.write(str(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e6b63",
   "metadata": {},
   "source": [
    "## KG querying - FAIRness assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67ee97",
   "metadata": {},
   "source": [
    "Counting all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1b1b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories: 270\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?s) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?s a sd:Software\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories:\", solution['count_software'].value)\n",
    "    result_list['total_repos'] = solution['count_software'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d11b4e",
   "metadata": {},
   "source": [
    "### BP 1: Description is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df6b0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<https://www.w3id.org/okn/i/Software/oeg-upm/delta-ontology> \"DELTA ontology \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/rmlc-statistic> \"RMLC iterator\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/S4WATR> \"Water Ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-streams-web> \"morph-web\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"A curated list of various semantic web and linked data resources. \\nTo add something to the list please either submit a pull request or add a comment with a link to [issues/awesomelets](https://github.com/semantalytics/awesome-semantic-web/issues/81). Pull requests will be evaluated immediately for inclusion while posts while awesomelets will be evaluated at some indeterminate time in the future.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LDP4RO> \"Project to create Research Objects following the LDP model and using LDP4J\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"This repo contains all the projects developed in the context of the project EasyTV \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/devops-infra> \"This ontology network aims at representing the main sets of entities and relationships used in the context of DevOps infrastructure.  \\nIt is the result of a collaboration between Huawei Research Ireland and the Ontology Engineering Group at Universidad Politécnica de Madrid. It originally started from an analysis of the Configuration Management Databases used by Huawei Research Ireland for the management of a large part of its DevOps infrastructure, and has evolved into an ontology that may be used as a starting point for the standardisation of the representation of CMDB-related data across vendors.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/transmodel-ontology> \"The material generated in the different activities carried out during the development of the vocabulary, use cases, user stories, glossary of terms, etc., is available in the [Vocabulary Wiki](https://github.com/oeg-upm/transmodel-ontology/wiki)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Metaphacts](http://metaphacts.com) - ($) End-to-end platform to create and utilize enterprise knowledge graphs.\\n- [OntoWiki](https://github.com/AKSW/OntoWiki) - (OS) Semantic data wiki as well as Linked Data publishing engine.\\n- [GNOSS-Sherlock](https://www.gnoss.com/en/semantic-framework/knowledge-graph-management) - ($) Cognitive Intelligence tool for machines to understand us.\\n- [Wikibase](http://wikiba.se) - (OS) Collection of applications and libraries for creating, managing and sharing structured data.\\n- [eccenca Corporate Memory](https://www.eccenca.com) - build, explore and consume Knowledge Graphs\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-qq> \"TADA for numeric column. It focuses on distributions to label numeric columns in tabular data.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19> \"Tools to extract knowledge from scientific publications on COVID-19\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-morph-kgc> \"Set of values to be interpreted as NULL when retrieving data from the input sources. The valid values are a list of values separated by commas.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"Helio repository for plugins\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_thing_manager_module> \"Thing Manager module for the COGITO project.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fair_ontologies> \"Our [ISWC 2021 demo paper](html_client/assets/iswc_2021_demo.pdf) (**best demo award**) provides an overview of the FOOPS! service. Please cite our work as follows: \\nIf you are interested in more information, check [our slides](https://www.slideshare.net/dgarijo/foops-an-ontology-pitfall-scanner-for-the-fair-principles) with the rationale of FOOPS! and the [teaser video](https://www.youtube.com/watch?v=s8FaFl8i6yQ&ab_channel=OEG-UPM) we made for ISWC 2021. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/linked-gtfs> \"Separate repository for the Linked GTFS vocabulary\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-reduce-combine> \"This belongs to the project tada-gam. This expects Lc scores of types and type hierarchy of different slices of columns and it will combine them and compute the likelihood of the class of the given slices of a column.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/IEBrain> \"IE Brain project is about connecting the AI market\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"Once the Fountain has identified all nodes from the vocabularies, it is prepared to search for the incoming (references) and outgoing (properties) edges for each of them. To do so, it creates and keeps a tuple map that puts nodes and their properties together:\\n```sql\\nSELECT DISTINCT ?c ?p WHERE {\\n    { ?c rdfs:subClassOf [ owl:onProperty ?p ] }\\n    UNION\\n    { ?p rdfs:domain ?c }\\n    FILTER (isURI(?p) && isURI(?c))\\n}\\n```\\n \\nHaving such map in memory, it is trivial to filter the properties of each node (fixing a value for *n*).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"Project to deploy an API REST with the annotator and the rdfy\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oops-plugin> \"A plugin for the Protégé ontology editor software that evaluates ontologies to detect the use of bad practices.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [r43ples](https://github.com/plt-tud/r43ples) - Revision Management for the Semantic Web.\\n- [RDFUnit](https://github.com/AKSW/RDFUnit) - RDF Unit testing and validation framework.\\n- [rdf-toolkit](https://github.com/edmcouncil/rdf-toolkit) - RDF Serializer, to be used in a git commit-hook to force automatic correct rewrite of every OWL ontology.\\n- [TripleChecker](https://github.com/cgutteridge/TripleChecker) - Look for common errors in an RDF Document.\\n- [owl2vcs](https://github.com/utapyngo/owl2vcs) - owl2vcs is a set of tools designed to facilitate version control of OWL 2 ontologies using version control systems.\\n- [dowl](https://github.com/ldodds/dowl) - Generate docs for RDF/OWL Schema.\\n- [rdf-pipeline](https://github.com/rdf-pipeline)\\n- [rdfpatch](https://github.com/pchampin/ld-patch-py)\\n- [TurtleValidator](https://github.com/mmlab/TurtleValidator) - A Turtle validator on command line and in browser.\\n- [shi3ld-http](https://github.com/lukostaz/shi3ld-http) - Shi3ld for HTTP: Access control for HTTP operations on Linked Data.\\n- [babel](https://github.com/aidhog/blabel/) - A library for skolemising (or canonicalising) blank node labels in RDF graphs.\\n- [rdf.sh](https://github.com/seebi/rdf.sh) - A multi-tool shell script for doing Semantic Web jobs on the command line.\\n- [QuitStore](https://github.com/AKSW/QuitStore) - Quads in Git - Distributed Version Control for RDF Knowledge Bases.\\n- [QuitDiff](https://github.com/AKSW/QuitDiff) - Git diff into SparQL / Eccrev vocabulary.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/btn100> \"Repository where we will be loading data related to BTN100 from Instituto Geográfico Nacional\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"To retrieve data from a HTTP_GET request, simply create the file `resources/movies/get.sparql` with the following content: \\nA new variable named `results` is always available from this template. It has all values retrieved in the sparql query so can be iterated to create a list of resources. In our example, a list of movies is create with two fields: `title` and `director`. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/201612-clarityhackathon-upm> \"Work done by the UPM-Zaragoza team on public services during the CLARITY project sprint week\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lubm4obda> \"The LUBM4OBDA Benchmark\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/software_catalog> \"Catalog of software tools developed at the Ontology Engineering Group\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Pellet](https://github.com/stardog-union/pellet)\\n- [openllet](https://github.com/Galigator/openllet)\\n- [FaCT++](https://github.com/ethz-asl/libfactplusplus)\\n- [HermiT](http://www.hermit-reasoner.com/)\\n- [ELK](https://github.com/liveontologies/elk-reasoner)\\n- [Whelk](https://github.com/balhoff/whelk)\\n- [OWL-RL](https://github.com/RDFLib/OWL-RL)\\n- [RacerPro](https://franz.com/agraph/racer/)\\n- [Manchester List of Reasoners](http://owl.cs.manchester.ac.uk/tools/list-of-reasoners/)\\n- [elephant-reasoner](https://github.com/sertkaya/elephant-reasoner)\\n- [HyLAR](https://github.com/ucbl/HyLAR-Reasoner)\\n- [ruby-rdf/rdf-reasoner](https://github.com/ruby-rdf/rdf-reasoner)\\n- [cel](https://github.com/julianmendez/cel) -A lightweight Description Logic reasoner for large-scale biomedical ontologies.\\n- [reasonable](https://github.com/gtfierro/reasonable) - OWL 2 Reasoner built on DataFrog\\n- [eye](https://github.com/josd/eye) - Euler Yet another proof Engine.\\n- [Sequoia](https://github.com/andrewdbate/Sequoia) - Sequoia is a consequence-based OWL 2 DL Reasoner supporting multithreaded reasoning.\\n- [konclude](http://www.derivo.de/en/produkte/konclude.html) - Konclude is a high-performance reasoner for large and expressive ontologies.\\n- [owlproofs](https://github.com/klinovp/owlproofs) - Extension to the OWL API to request proofs of entailments from the reasoner.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-gam> \"A scalable version of tada entity using the MapReduce framework\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/hydrontology> \"Repository for the new version of Hydrontology. \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/astrea> \"Astrea is a software that generates SHACL shapes for one or more OWL ontologies using a set of SPARQL queries that hold the equivalence between those two specifications\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"Similarly to the process of node extraction, the detection of *valid* edges for the link graph in an ontology is built on the following query:\\n```sql\\nSELECT DISTINCT ?p WHERE {\\n    { ?p a rdf:Property }\\n    UNION\\n    { ?p a owl:ObjectProperty }\\n    UNION\\n    { ?p a owl:DatatypeProperty }\\n    UNION\\n    {\\n        [] a owl:Restriction ;\\n           owl:onProperty ?p .\\n    }\\n    FILTER(isURI(?p))\\n}\\n```\\n \\nThe result set of the corresponding query is composed of all the URIs that have been described in such a way that they can be considered as edges. The corresponding matching rules for edges are: \\n* It is a `rdf:Property`, an `owl:ObjectProperty` or an `owl:DatatypeProperty`.\\n* There is some restriction on it as a property.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/weather1> \"Ontology for weather phenomenon\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Massive-ROs-Creator> \"Massive ROs Creator is a python program that, given a group of search parameters, navigates to the Norwegian Research Data Archive (NIRD), realizes an advanced search and recovers data from the resources obtained by this search. The program is set to insert this data into ROs in the ROHub platform. A functionality that will be added in posterior versions. The program uses automated web navigation in the local machine where it is being excuted.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/doccano_formatter> \"Project to transform doccano outputs to different formats for research purposes \\n1. JSONL to CONLL \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/seed-sensors-oeg> \"The repository for the sensors in the lab\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/licensius> \"Java API to manipulate ODRL2 expressions.\\nThis API permits creating ODRL expressions in its RDF serialization by using simple Java methods.\\nFor more info, see http://oeg-upm.github.io/odrlapi/\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-annotator> \"# easytv-annotator\\nSign language annotator library for the EASYTV european project \\n\\nTRANSLATOR PROJECT \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"````\\nJsonObject jsonTD = JTD.parseJson(strJsonTD);\\nThing thing = Thing.fromJson(jsonTD);\\nthing = (Thing) JTD.instantiateFromJson(jsonTD, Thing.class); # Alternativelly\\n````\\nNotice that using the method `JTD.instantiateFromJson(jsonTD, Thing.class)` any other class from the model can be serialised.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loom-ld> \"It is a spring boot application for providing loom-ld services.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fuzzy-c-means> \"This implementation is based on the paper\\n**FCM: The fuzzy c-means clustering algorithm**  by: *James C.Bezdek, Robert Ehrlich, and  William Full*\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-tutorial-iswc2020> \"Material for KGC2020 tutorial at ISWC2020\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SendEmailWebApp> \"SendEmailWebApp is a web application with a formulary to send emails to a specified destinatary.  \\nThis page is designed to be added at other page and the people can send emails at specified email adress.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/soca> \"Click [here](https://oeg-upm.github.io/soca/example/oeg-upm/index.html) to see an interactive example generated by using the `oeg-upm` organization as input for SOCA.   \\nClick [here](https://oeg-upm.github.io/soca/example/KnowledgeCaptureAndDiscoveryANDmintproject/index.html) to see an interactive example generated by using the `KnowledgeCaptureAndDiscovery` and `mintproject` organization as input for SOCA.   \\nClick [here](https://oeg-upm.github.io/soca/example/LinkedEarth/index.html) to see an interactive example generated by using the `LinkedEarth` organization as input for SOCA.   \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/DBpedia-downloader> \"An app to help download DBpedia\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mappingpedia-userinterface> \"This is an interface for mappingpedia engine.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Instituto-Estudios-Fiscales-ontologias> \"Repositorio de las ontologías a desarrollar para el Instituto de Estudios Fiscales\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"In order to publish a release of the plugin in the official Helio repository the code of the plugin must be pushed with a Pull Request as previously explained. After that, the managers of the Helio plugins repository will check the code, compile it, and publish the release. Additionally, they will update the current README including the plugin in the table of the available ones, for which end a good documentation of the plugin is required. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Instituto-Estudios-Fiscales-ontologias> \"To manage those incidents or suggested improvements with respect to the vocabulary, we recommend you to follow\\nthe guides provided in [Issues Management](https://github.com/nombre-repositorio/wiki/issues-management) to\\ngenerate an issue (work in progress)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ar2dtool-oegfork> \"Another RDF to diagram tool\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [anthelion](https://github.com/yahoo/anthelion) - A plugin for Apache Nutch to crawl semantic annotations within HTML pages.\\n- [SolRDF](https://github.com/agazzarini/SolRDF) - An RDF plugin for Solr.\\n- [sesame-spring](https://github.com/ameingast/sesame-spring) - Spring integration for OpenRDF/Sesame.\\n- [HydraBundle](https://github.com/lanthaler/HydraBundle) - Symfony2 bundle which shows how easily Hydra can be integrated in modern Web frameworks.\\n- [SARQ](https://github.com/castagna/SARQ) - Free Text Indexing for SPARQL using a remote Solr server.\\n- [EARQ](https://github.com/castagna/EARQ) - EARQ is a combination of ARQ and ElasticSearch.\\n- [sesametools](https://github.com/joshsh/sesametools) - A collection of utilities for use with OpenRDF Sesame.\\n- [Imperium](https://github.com/mhgrove/Imperium) - Imperium is a plugin for the Play! framework similar to the existing JPA plugin that allows the use of Empire seamlessly in a Play! based application.\\n- [jekyll-rdf](https://github.com/white-gecko/jekyll-rdf) - A Jekyll plugin for including RDF data in your static site.\\n- [RightField](https://github.com/myGrid/RightField) - RightField is an open-source tool for adding ontology term selection to Excel spreadsheets.\\n- [mu-semtech](https://github.com/mu-semtech) - An Ecosystem of User-facing Microservices supported by Semantic Models.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/valkyr-ie-py> \"Valkyr meets transformers\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-hive> \"The WoT Hive an implementation of a WoT Directory described in the W3C WoT Discovery standard\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-materialiser> \"# [Helio Materialiser](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-Users)\\n### **Creators:** [Andrea Cimmino](https://scholar.google.es/citations?user=_6U9WMcAAAAJ&hl=es&oi=ao) and [Raúl García Castro](http://www.garcia-castro.com/)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"A fork is a copy of the parent repository, which however, remains synchronised with the former repository. When new modifications are commited to the forked repository these are marked as changes from the original repository, and thus, can be later be pushed to the original repository requesting a Pull Request. The bottom line idea is to fork the Helio plugins repository, include the new code in the fork, and then create a Pull Request to merge the new plugin code in the original repository. To fork the Helio plugins repository click in the \\\"Fork\\\" button on the upper right-corner of the repository page. \\nOnce forked the a copy of the Helio plugins repository must appear in the private account of the user who forked the repotiroty.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-metadata> \"Repository for the BIMERR Metadata Ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/snap-docs> \"SNAP Documentation\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-hive> \"| Endpoint \t| Method \t| Headers \t| Reference \t| Description \t|\\n|---\t|---\t|---\t|---\t|---\t|\\n| `/.well-known/wot` \t| `GET` \t| `N/A` \t| [Introduction Mechanism](https://w3c.github.io/wot-discovery/#introduction-well-known) \t| Provides the Thing Description of the WoT Hive directory \t|\\n| `/.well-known/core` \t| `GET` \t| `N/A` \t| [Introduction Mechanim](https://w3c.github.io/wot-discovery/#introduction-core-rd-sec) \t| Exposes the directory's Thing Description using the CoRE Link Format\t|\\n| `/configuration` \t| `GET` \t| `N/A` \t| [Management](https://w3c.github.io/wot-discovery/#exploration-directory-api-management) \t| Provides a JSON with the all the configurations of the WoT Hive \t|\\n| `/configuration` \t| `POST` \t| `N/A` \t| [Management](https://w3c.github.io/wot-discovery/#exploration-directory-api-management) \t| The body of the request must contain a JSON with all the configurations of the WoT Hive. \t|\\n| `/api/status`   | `GET`   |  `N/A`   |   `N/A`  | Provides a  health check of the service  |\\n| `/api/things{?offset,limit,sort_by,sort_order}` \t| `GET` \t| `Accept`: `application/td+json`or `text/turtle`  \t| [Listing](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-listing) \t| Provides a listing of the stored Thing Descriptions in JSON-LD framed or Turtle \t|\\n| `/api/things` \t| `POST` \t| `Content-Type`: `application/td+json` \t| [Creation (Anonymous)](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-creation) \t| Creates an [anonymous Thing Description](https://w3c.github.io/wot-discovery/#dfn-wot-anonymous-thing-description), provided in the body as JSON-LD framed. The generated `:id` is output in the response headers  \t|\\n| `/api/things/{:id}` \t| `GET` \t| `Accept`: `application/td+json`or `text/turtle` \t| [Retrieval](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-retrieval) \t| Retrieves the Thing Description with the provided id, in either JSON-LD framed or turtle \t|\\n| `/api/things/{:id}` \t| `PUT` \t| `Content-Type`: `application/td+json`or `text/turtle` \t| [Creation](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-creation) or [Update](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-update) \t| Creates an Thing Description, provided in the body as JSON-LD framed or turtle \t|\\n| `/api/things/{:id}` \t| `PATCH` \t| `Content-Type`: `application/merge-patch+json` \t| [Partial Update](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-update) \t| Partially updates an existing Thing Description, the updates must be provided in JSON-LD framed \t|\\n| `/api/things/{:id}` \t| `DELETE` \t| `N/A` \t| [Deletion](https://w3c.github.io/wot-discovery/#exploration-directory-api-registration-deletion) \t| Partially updates an existing Thing Description, the updates must be provided in JSON-LD framed \t|\\n| `api/search/jsonpath{?query}` \t| `GET` \t| `N/A` \t| [JSON path search](https://w3c.github.io/wot-discovery/#jsonpath-semantic) \t| Filters existing Thing Descriptions based on the provided JSON path, the output will be always in JSON-LD framed \t|\\n| `api/search/sparql{?query}` \t| `GET` \t| `Accept` : `application/sparql-results+json`, `application/sparql-results+xml`, `text/csv`, or `text/tab-separated-values` \t| [SPARQL search](https://w3c.github.io/wot-discovery/#search-semantic) \t| Solves a SPARQL query following the [standard](https://www.w3.org/TR/sparql11-protocol/<br>), results format are in JSON by default if no header is specified. Otherwise available formats are JSON(application/sparql-results+json), XML (application/sparql-results+xml), CSV (text/csv), or TSV (text/tab-separated-values)  \t|\\n| `api/search/fed-sparql{?query}{?endpoint}` \t| `POST` \t| `Accept` : `application/sparql-results+json`, `application/sparql-results+xml`, `text/csv`, or `text/tab-separated-values` \t| [SPARQL search](https://w3c.github.io/wot-discovery/#search-semantic) \t| Solves a SPARQL query SELECT or CONSTRUCT over a list of endpoints provided following the [standard](https://www.w3.org/TR/sparql11-protocol/<br>), results format are in JSON by default if no header is specified. Otherwise available formats are JSON(application/sparql-results+json), XML (application/sparql-results+xml), CSV (text/csv), or TSV (text/tab-separated-values)  \t|\\n| `api/events{?diff}` \t| `GET` \t| `N/A` \t| [Notifications](https://w3c.github.io/wot-discovery/#exploration-directory-api-notification) \t| Subscribe to all the events of the service (`create`, `update`, and `delete`) using the Server-Sends-Events (SSE) protocol \t|\\n| `api/events/create{?diff}` \t| `GET` \t| `N/A` \t| [Notifications](https://w3c.github.io/wot-discovery/#exploration-directory-api-notification) \t| Subscribe to all the `create` events of the service using the Server-Sends-Events (SSE) protocol \t|\\n| `api/events/update{?diff}` \t| `GET` \t| `N/A` \t| [Notifications](https://w3c.github.io/wot-discovery/#exploration-directory-api-notification) \t| Subscribe to all the `update` events of the service using the Server-Sends-Events (SSE) protocol \t|\\n| `api/events/delete{?diff}` \t| `GET` \t| `N/A` \t| [Notifications](https://w3c.github.io/wot-discovery/#exploration-directory-api-notification) \t| Subscribe to all the `delete` events of the service using the Server-Sends-Events (SSE) protocol \t| \\n[Validation](https://w3c.github.io/wot-discovery/#validation) can  be configured to ran using the JSON schema of the Thing Descriptions and/or their SHACL shapes. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-materials> \"In [this folder](./Examples) we can see an example of an [idf file](./Examples/ASHRAE_2005_HOF_Materials.idf), which has been transformed into a [json file](./Examples/ASHRAE_2005_HOF_Materials.json), with which the corresponding [RDF files](./RDF_Examples/ASHRAE_2005_HOF_Materials.ttl) have been obtained by means of declarative mappings. These files contain information about materials that have been used in constructions and buildings.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"Thus, there are some rules that must be taken into account in order to let the Fountain *detect* nodes in ontologies. That is, nodes are all URIs that match at least one of the following: \\n* It is a class, either an `owl:Class` or a `rdfs:Class`.\\n* It has at least one subclass in the ontology or it is the superclass of any other.\\n* It belongs to the domain of a datatype property.\\n* Given an object property,\\n    * it is a class that belongs to its range or/and domain.\\n    * there may be a set of things for which such property may have values of it. \\nIt is important to note that no automatic reasoning is performed in this process. All required information must be materialized in the ontology description that is being submitted. Furthermore, existing conflicts and/or inconsistencies in definitions will not be treated; neither a warning nor an error message will be generated.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [protégé](http://protege.stanford.edu) - Ontology editor and framework for building intelligent systems.\\n- [OntoVerbal](https://github.com/TheOntologist/OntoVerbal) - OntoVerbal is a Protege 4.2 plugin that generates natural language descriptions for classes for an ontology written in OWL.\\n- [OTTR](https://ottr.xyz) - Reasonable Ontology Templates.\\n- [dosdp-tools](https://github.com/INCATools/dosdp-tools/) - dead simple owl design patterns (template tool)\\n- [RDFSharp.Semantics](https://github.com/mdesalvo/RDFSharp.Semantics) - .NET library for OWL-DL/SKOS ontology modeling, validation and reasoning\\n- [Ontology Development Kit](https://github.com/INCATools/ontology-development-kit/) - set up a git repo for developing an ontology\\n- [ROBOT](http://robot.obolibrary.org/) - command line swiss-army knife for ontology developers\\n- [grafo](http://gra.fo/) - Visual graph development\\n- [OOPS! (Ontology Pitfall Scanner!)](http://oops.linkeddata.es/) - a web application to detect (semi)automatically 33 pitfalls or errors in ontologies. A web service is also provided.\\n- [Cameo Concept Modeler](https://www.nomagic.com/product-addons/magicdraw-addons/cameo-concept-modeler-plugin#key-benefits) - a cross-platform app for OWL ontology modeling, visualization, and natural-language validation\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/city4age> \"Repositorio para la publicación de los trabajos realizados en el proyecto City4Age\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/outlinejs> \"Draw an outline or roadmap for a workflow\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oatapi> \"OATAPI takes two ontology artefacts (a set of competency questions (CQs) and the related ontology serialization) and generates the API paths and SPARQL queries that allow these questions  to be solved. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/websiteFooterLogos> \"An HTML footer to integrate the OEG,UPM and FI logos.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [tawny-owl](https://github.com/phillord/tawny-owl) - Build OWL Ontologies in a Programmatic Environment.\\n- [Widoco](https://github.com/dgarijo/Widoco) - A Wizard for documenting and publishing ontologies on the Web.\\n- [sesame-vocab-builder](https://github.com/tkurz/sesame-vocab-builder) - Sesame Vocab Builder provides a command line tool that allows to create constants for RDF primitives for a given namespace out of RDF ontology files.\\n- [HydraConsole](https://github.com/lanthaler/HydraConsole)\\n- [qonsole](https://github.com/epimorphics/qonsole) - A simple console for running SPARQL queries and displaying results.\\n- [ntcat](https://github.com/cgutteridge/ntcat) - Command line tool for concatenating NTriples documents.\\n- [ripple](https://github.com/joshsh/ripple) - Semantic Web scripting language.\\n- [schema_salad](https://github.com/common-workflow-language/schema_salad) - Semantic Annotations for Linked Avro Data.\\n- [RDFConvert](https://sourceforge.net/projects/rdfconvert/) - RDFConvert is a simple command-line tool for converting RDF file betweeen different syntax formats.\\n- [How to diff RDF](https://www.w3.org/2001/sw/wiki/How_to_diff_RDF)\\n- [grlc](https://github.com/CLARIAH/grlc) - Web APIs from SPARQL queries.\\n- [Openlink Structured Data Sniffer](http://osds.openlinksw.com/) - Browser extension for Google Chrome, Microsoft Edge, Mozilla Firefox, Opera, and Vivaldi that unveils structured metadata embedded within HTML documents and web pages. \\n- [ShacShifter](https://github.com/AKSW/ShacShifter) - Shapes Constraint Language (SHACL) to various other format.\\n- [prefix.cc](https://prefix.cc) - namespace lookup for RDF developers\\n- [rdf2rdf](https://github.com/knakk/rdf2rdf) - Tool for converting between different RDF serialization formats.\\n- [Web-Client](https://github.com/AtomGraph/Web-Client) - Generic Linked Data browser and UX component framework.\\n- [CEDAR Workbench](https://metadatacenter.org) - Center for Expanded Data Annotation and Retrieval offers full life cycle management for semantically linked metadata\\n- [OnToology](https://github.com/OnToology/OnToology) - A system for collaborative ontology development. Given a GitHub repository with an OWL file, OnToology will survey it and produce diagrams, a complete documentation and validation based on common pitfalls.\\n- [OBA](https://github.com/KnowledgeCaptureAndDiscovery/OBA/) - Automatically create OpenAPI specifications from OWL and launch a server that serves JSON objects according to your ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-kpi> \"BIMERR ontology for KPI data\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"For the next examples, let's assume the following java variables containing the same Thing description:\\n````\\nString strJsonTD = \\\"{ \\\\\\\"@context\\\\\\\": \\\\\\\"https://www.w3.org/2019/wot/td/v1\\\\\\\",\\\\n\\\" +\\n\\\" \\\\\\\"id\\\\\\\": \\\\\\\"urn:dev:ops:32473-WoTLamp-1234\\\\\\\",\\\\n\\\" +\\n\\\" \\\\\\\"title\\\\\\\": \\\\\\\"MyLampThing\\\\\\\",\\\\n\\\" +\\n\\\" \\\\\\\"securityDefinitions\\\\\\\": { \\\\\\\"nosec_sc\\\\\\\": { \\\\\\\"scheme\\\\\\\": \\\\\\\"nosec\\\\\\\" }},\\\\n\\\" +\\n\\\" \\\\\\\"security\\\\\\\": \\\\\\\"nosec_sc\\\\\\\",\\\\n\\\" +\\n\\\" \\\\\\\"properties\\\\\\\": {\\\\n\\\" +\\n\\\"     \\\\\\\"status\\\\\\\": {\\\\n\\\" +\\n\\\"         \\\\\\\"type\\\\\\\": \\\\\\\"string\\\\\\\",\\\\n\\\" +\\n\\\"         \\\\\\\"forms\\\\\\\": [{\\\\\\\"href\\\\\\\": \\\\\\\"https://mylamp.example.com/status\\\\\\\"}]\\\\n\\\" +\\n\\\"     }\\\\n\\\" +\\n\\\" }\\\\n\\\" +\\n\\\"}\\\";\\n````\\n\\n````\\nModel modelTD = ModelFactory.createDefaultModel();\\nString strRdfTD = \\\"@prefix dc: <http://purl.org/dc/terms/> .\\\\n\\\" +\\n\\\"@prefix td: <https://www.w3.org/2019/wot/td#> .\\\\n\\\" +\\n\\\"@prefix jsonschema: <https://www.w3.org/2019/wot/json-schema#> .\\\\n\\\" +\\n\\\"@prefix hctl: <https://www.w3.org/2019/wot/hypermedia#> .\\\\n\\\" +\\n\\\"\\\\n\\\" +\\n\\\"<urn:dev:ops:32473-WoTLamp-1234>\\\\n\\\" +\\n\\\"  dc:title \\\\\\\"MyLampThing\\\\\\\" ;\\\\n\\\" +\\n\\\"  td:hasPropertyAffordance [\\\\n\\\" +\\n\\\"      a <https://www.w3.org/2019/wot/json-schema#StringSchema> ;\\\\n\\\" +\\n\\\"      jsonschema:propertyName \\\\\\\"status\\\\\\\" ;\\\\n\\\" +\\n\\\"      td:hasForm [ hctl:hasTarget <https://mylamp.example.com/status> ]\\\\n\\\" +\\n\\\"  ] ;\\\\n\\\" +\\n\\\"  td:hasSecurityConfiguration <https://json-ld.org/playground/nosec_sc> ;\\\\n\\\" +\\n\\\"  td:securityDefinitions [ td:scheme \\\\\\\"nosec\\\\\\\" ] .\\\";\\n\\n##### Read the string variable into the jena model\\nmodelTD.read(new ByteArrayInputStream(strRdfTD.getBytes()), null, \\\"Turtle\\\");\\n````\\n\\n\\n\\nThe following serialisation operations consists of building a JTD object Thing from either a JSON-LD framed representation or a set of RDF triples. \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-hdt-entity-experiment> \"Run experiments using tada-hdt-entity library\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/esuk> \"Enhanced Summaries using KGs\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/eWoT> \"In order to transparently query the ecosystem a SPARQL query must be issued to the endpoint *http://localhost:9000/sparql*. Alternativelly, if an user navigates with the browser to the very same address a GUI will be displayed where the SPARQL query could be written and its results displayed.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vicinity-ontologies> \"Ontologies developed during the VICINITY European project about IoT and related domains.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [swift-sparql-syntax](https://github.com/kasei/swift-sparql-syntax) - SPARQL 1.1 Parser.\\n- [URITemplate](https://github.com/kasei/URITemplate) - Swift implementation of URI Template ([RFC6570](https://tools.ietf.org/html/rfc6570)).\\n- [swift-serd](https://github.com/kasei/swift-serd) - Swift package wrapper for the [Serd RDF library](http://drobilla.net/software/serd).\\n- [kineo](https://github.com/kasei/kineo) - A SPARQL endpoint and quadstore written in Swift.\\n- [swift-hdt](https://github.com/kasei/swift-hdt) - An [HDT](http://www.rdfhdt.org/) RDF Parser.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Devos> \"Depicting Vocabulary Summaries(**Devos**) is a tool that generates a visual summary from a given ontology. Devos is built on top of [Mermaid](https://mermaid.js.org/) syntax which is a Markdown-inspired tool that renders text in diagrams and it uses **[SPARQL](https://www.w3.org/TR/rdf-sparql-query/) Query Language** over the ontology generating a visual summary. It's based on three approaches. \\n1. OntMet. Uses Ontology Meta Data to find matching classes.\\n2. ClaFreq. Relies on Class Frequency in the ontology as the importance signal.\\n3. LabLen. Utilizes Label Length of the classes as the importance signal. The intuition is that importance classes have richer metadata than less important classes. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vocabTest> \"While the system is working you can check the status of the server and jobs: http://jarsomatic.linkeddata.es/ \\nWhen the system finish you have to accept the pull request is generates in order to see the generated pre-visualization of your ontology. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ssspotter> \"* `table`: the file\\n* `technique`: the technique to be use to identify the subject column (see below).\\n* `callback`: a `POST` callback url to be called by the system when the subject column is identified. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-skyline> \"Virtual Knowledge Graph Access for Skyline Queries\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/owl2yarrrml> \"Basic script that extracts information from the ontology to generate a YARRRML template\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"`````\\nList<Thing> things = fromRDF(modelTD)\\n`````\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-hdt-numeric> \"This application annotate numeric columns in tabular data with properties from HDT-compressed knowledge bases.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The Fountain is the *place* where all navigational paths found in known vocabularies are exposed, taking into account a number of heterogeneous seeds to be later on proposed as starting points for search plans.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-rdb> \"Morph-RDB (formerly called ODEMapster) is an RDB2RDF engine developed by the Ontology Engineering Group, that follows the R2RML specification (http://www.w3.org/TR/r2rml/).  \\nMorph-RDB supports two operational modes: data upgrade (generating RDF instances from data in a relational database) and query translation (SPARQL to SQL). Morph-RDB employs various optimisation techniques in order to generate efficient SQL queries, such as self-join elimination and subquery elimination.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gwt-blocks> \"This respository aims to help for the development of GWT projects.  \\r\\nAdd to your projects 3 new widgets: loading box, prettypopup, togglebutton.  \\r\\nAdd new events for togglebutton: toggleEvent and hasToggleEvent.  \\r\\nManages your URL places with PlaceManager.  \\r\\nBetter abstraction from presenter-display model.\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [R2RML-Parser](https://github.com/nkons/r2rml-parser) - An R2RML implementation that can export relational database contents as RDF graphs.\\n- [Morph-RDB](https://github.com/oeg-upm/morph-rdb) - An R2RML processor.\\n- [MusicBrainz-R2RML](https://github.com/LinkedBrainz/MusicBrainz-R2RML) - R2RML mappings for the MusicBrainz schema.\\n- [ontop](https://github.com/ontop/ontop) - Ontop is a platform to query relational databases as Virtual RDF Graphs using SPARQL. It's fast and is packed with features.\\n- [db2triples](https://github.com/antidot/db2triples) - Antidot implementations of R2RML and Direct Mapping specifications.\\n- [ADAPT-R2RML](https://opengogs.adaptcentre.ie/debruync/r2rml)\\n- [R2RML-api](https://github.com/R2RML-api/R2RML-api)\\n- [R2RML-kit](https://github.com/d2rq/r2rml-kit)\\n- [Juma](https://opengogs.adaptcentre.ie/crottija/juma-r2rml/) - Juma, jigsaw puzzles for representing mapping, is a method that applies the block metaphor to mapping languages.\\n- [pyrdb2rdf](https://github.com/nisavid/pyrdb2rdf) - A Python library for RDB2RDF Direct Mapping and R2RML.\\n- [sparqlmap](https://github.com/tomatophantastico/sparqlmap)\\n- [rdf2rml](https://github.com/VladimirAlexiev/rdf2rml) - R2RML Generation from simple examples.\\n- [ultrawrap](https://capsenta.com/) - ($)\\n- [AutoMap4OBDA](https://github.com/arc-lasalle/AutoMap4OBDA) - AutoMap4OBDA: Automated Generation of R2RML Mappings for OBDA.\\n- [Map-On](https://github.com/arc-lasalle/Map-On) - A web-based editor for visual ontology mapping for R2RML documents. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/virtuoso-triple-store> \"Virtuoso triple store for the storage of the rdf data in the digital twin platform.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/astrea> \"Astrea is a software capable of generate SHACL shapes from one or more given ontologies. It relies on a set of equivalences between the [OWL2](https://www.w3.org/TR/owl2-overview/) constructs and the [SHACL](https://www.w3.org/TR/shacl/) constructs, which are exploit by means of a set of [SPARQL queries](https://www.w3.org/TR/sparql11-query/). The idea behind Astrea is to rely on a set of mappings between such specifications, and a list of queries so by just applying the queries over one or more owl files the associated shapes can be generated. \\nIn this repository the software provided imports all the ontologies associated to the construct owl:imports for an input ontology, in addition in its methods counts with the option of importing more than one ontology URL to generate their shapes. Consider that the shapes generated are associated only to the types and properties specified in the ontology, therefore providing more than one URL can be useful if one ontology references elements from another but it does not import it. \\nThe [Astrea resources](https://github.com/oeg-upm/Astrea/tree/master/material), besides the java library which latest version [can be downloaded from the releases tab](https://github.com/oeg-upm/Astrea/releases), include the following elements:\\n* Mappings.xlsx: a set of mappings that hold the equivalences between OWL and SHACL constructs.\\n* OWL.csv, RDFS.csv, SHACL.csv, XSD.csv: the constructs from the OWL, RDFS, XSD, and SHACL that exists in these specifications\\n* Queries.csv: the queries to generate the SHACL shapes, this file contains also all the statements that are required from the OWL, RDFS, and XSD to generate a shapes, as well as, all the constructs belonging to SHACL that the output shape contains.\\n* astrea-dataset.zip: is a RDF dataset that contains all the Astrea resources modelled according to the [Astrea ontology](https://w3id.org/def/astrea#). This dataset is also available at [https://astrea.helio.linkeddata.es/](https://astrea.helio.linkeddata.es/) for live queries or to download.\\n* Java Documentation: is available at https://oeg-upm.github.io/Astrea/ \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AI4EU_raidologist> \"r.AID.ologist is a framework designed for radiologists to provide assistance on the generation of medical reports. \\nIt is devised as a Case-Based Reasoning model, powered by several deep learning models which have been included to improve its performance, as well as providing additional features.   \\nWhile CBR is integrated as a continuous cycle, its subparts are designed to be fully modular, thus can be easily substituted to fit new data. The following diagram provides an overview on the system. Modular elements are represented by bricks, implying that they can be replaced by different models as long as they meet the same objective.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/eWoT> \"eWoT enables semantic interoperable IoT ecosystems\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/solarchem-ontology> \"Repository for the ontology that allows representing photocatalysis experiments\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lubm4obda> \"The **Univ-Bench ontology** is available in the **[ontology](https://github.com/oeg-upm/lubm4obda/blob/main/ontology/univ-bench.owl)** directory of this GitHub repository.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [CSPARQL-engine](https://github.com/streamreasoning/CSPARQL-engine)\\n- [Triplewave](https://github.com/streamreasoning/TripleWave)\\n- [morph-streams](https://github.com/jpcik/morph-streams)\\n- [Katts](https://github.com/uzh/katts) - Katts is A Triple Torrent Sieve.\\n- [WAVES](https://www.waves-rsp.org/)\\n- [Strider](https://github.com/renxiangnan/strider)\\n- [cqels](https://github.com/KMax/cqels)\\n- [morph](https://github.com/jpcik/morph) - Sparql-stream sensor queries.\\n- [morph-web](https://github.com/jpcik/morph-web)\\n- [sepa](https://github.com/arces-wot/SEPA) - A JAVA implementation of the SPARQL Event Processing Architecture including the engine, APIs and tools.\\n- [StreamingMASSIF](https://github.com/IBCNServices/StreamingMASSIF)\\n- [streaming-sparql](https://github.com/weblyzard/streaming-sparql)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-nlp> \"NLP-based resources created to support the [Drugs4Covid19](https://drugs4covid.oeg-upm.net) project. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Morph-OME> \"Online Mapping Editor\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Conceptual-Mapping> \"To check that a mapping written with the Conceptual Mapping is correct, shapes in ShEx and SHACL are provided in the [shapes folder](https://github.com/oeg-upm/Conceptual-Mapping/tree/main/shapes). Special thanks to José Emilio Labra Gayo ([@labra](https://github.com/labra)) for providing the ShEx shapes.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"The `-v` parameter sets a local folder where the resources that are published in the API are defined. \\nThe `-p` parameter defines the port where the service will be listening. In this case we've set 8080, so let's go to [http://localhost:8080/](http://localhost:8080/) from a web browser and see a welcome message like this:\\n```\\n  Welcome to R4R ;)\\n\\n```\\n \\nIn order to continue with the following steps, it is recommended to use a text editor such as [Atom](https://atom.io) ( with [velocity](https://atom.io/packages/atom-language-velocity), [json](https://atom.io/packages/pretty-json) and [sparql](https://atom.io/packages/language-sparql) plugins), to easily handle JSON and Sparql files.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FAIR-Research-Object-API> \"API for the FAIR Research Object assessment service\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/pcake> \"An online application to show the distribution of a given class/property pairs of a SPARQL endpoint \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Morph-OME> \"An Online Mapping Editor to generate R2RML, RML, and YARRRML without writing a single line.\\nIt also supports automatic suggestions of the subject and property columns using \\nthe APIs of . \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"`````\\nThing thing = fromRDF(modelTD, \\\"urn:dev:ops:32473-WoTLamp-1234\\\")\\n`````\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/personal-repo> \"Personal Maven Repository\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/geo.linkeddata.es-TripleGeoKettle> \"Repository where the integration of TripleGeo and GeoKettle is performed, used in the datos.ign.es project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TPool> \"Thread Pool for python 2 and 3 with multiple parameters\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [SPARQLKit](https://github.com/kasei/SPARQLKit) - An implementation of the SPARQL 1.1 query language in Objective-C.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vicinity-ontologies> \"This repository is an aggregator for all the ontologies developed during the project.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The three main benefits of the Web of Data are: \\n* Feasibility to perform live-querying over a dataspace that integrates a large number of interlinked datasets as if it was a huge multidatabase system.\\n* Data sources may be considerably lighter, scalable and maintanable than (reliable) SPARQL endpoints. They can be interfaced as just RESTful APIs that provide RDF by dereferencing known resources.\\n* Enables freshness and serendipitous discovery of data sources and results.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Chowlk> \"Tool to transform an ontology diagram into OWL code.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/yarrrml-validation> \"Human-readable mapping serializations and translators\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/seed-sensors-oeg> \"The repository for the sensors in the lab\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/WordPress-RDFa> \"Resultado de un TFG que permite realizar anotaciones en Wordpress con el formato RDFa\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-VehicleCharger-ontology> \"This repository contains the code and documentation generated for the the AURORAL Vehicle Charger ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/BO2DM> \"BIMERR Ontology to Data Model Converter\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [d2rq](https://github.com/d2rq/d2rq) - Database to RDF mapping engine and SPARQL server.\\n- [Sparqlify](https://github.com/AKSW/Sparqlify) - Sparql -> SQL Rewriter enabling virtual RDB -> RDF mappings.\\n- [Sparqlify-Extendsions](https://github.com/AKSW/Sparqlify-Extensions) - Extension projects for Sparqlify.\\n- [quetzal](https://github.com/Quetzal-RDF/quetzal) - SPARQL to SQL translation engine for multiple backends, such as DB2, PostgreSQL and Apache Spark. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/subject_column_election> \"Given a set of subject column suggestions from subject column spotters, it elects the most probable one.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oops-plugin> \"A plug-in for the [Protégé Desktop](http://protege.stanford.edu/) ontology editor software that evaluates ontologies to detect the use of bad practices. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-building> \"BIMERR ontology for the building domain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/geo-agreement> \"This repository is focused on developing software and techniques to be used to integrated open geospatial data sources (e.g., OpenStreetMap, public open data, crowdsourced data, etc.) and understand and visualise the level of agreement\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"To request a Pull Request there are several options. The easiest is to open in a browser the forked repository and click the button *Compare & Pull Request* that will appear, as shown in the following capture in red. \\nAfter clicking the *Compare & Pull Request* the plugin's branch from the forked repository will appear compared against the master from the original (or the master branch from the forked repository against the master of the original if no additional branch was created). In the image below the red square indicates the  master from the original repository, and the red square the brach of the forked repository. Add an meaninful message for the Pull Request and submit the request. \\nIf the whole process was correctly carried out, as depicted in the figure below, in the tab section *Pull requests* (marked in blue in the figure) of the Helio plugins repository must appear the new created Pull Request (marked in red in the figure).  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-privacy-ontology> \"This repository contains the code and documentation generated for the the AURORAL privacy ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_final_thing_manager> \"Modular Thing Manager for the COGITO project\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/spread-sheet-space-apis> \"API wrapper for Spread Sheet Space (http://spreadsheetspace.net/)\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"The account access token, it is taken from `${{ secrets.GITHUB_TOKEN }}`.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/PPool> \"PPool is a Pool for Processes in Python with locks. The main advantage is the ability to use `Lock`s from  `multiprocessing` \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"ROCrate_enrichment_service is a metadata enrichment service for research objects created in RO-Crate format. The service offers a RESTFUL API built with the FLASK library for python. It receives a json/jsonld file and uses the API of the service of OpenAire to associate more metadata to the original file before it returns another RO-Crate in jsonld format. Signing up to the service is a manual process managed locally by the service provider. Passwords are encrypted using the sha256 algorithm. However, the rest of the operations are available through the public API.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TINTO> \"The growing interest in the use of algorithms-based machine learning for predictive tasks has generated a large and diverse development of algorithms. However, it is widely known that not all of these algorithms are adapted to efficient solutions in certain tidy data format datasets. For this reason, novel techniques are currently being developed to convert tidy data into images with the aim of using Convolutional Neural Networks (CNNs). TINTO offers the opportunity to convert tidy data into images through the representation of characteristic pixels by implementing two dimensional reduction algorithms: PCA and _t_-SNE. Our proposal also includes a blurring technique, which adds more ordered information to the image and can improve the classification task in CNNs.\\n\\n<div>\\n<p align = \\\"center\\\">\\n<img src=\\\"imgs/tinto-framework.png\\\" alt=\\\"Logo\\\" width=\\\"650\\\">\\n</p>\\n</div>\\n\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ssn-resource-center> \"This repository contains all types of material that we consider useful for the management and usage of the W3C Semantic Sensor Network Ontology\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_final_thing_manager> \"Modular Thing Manager for the COGITO project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/coppola> \"| Endpoint | Method | Description |\\n|--|--|--|\\n| `/api`  |  `GET` | Returns the list of SHACL shapes stored in Coppola |\\n| `/api/:id`  |  `GET` | Returns the SHACL shape stored in Coppola with the provided `:id`  |\\n| `/api/:id`  |  `PUT` | Stores a SHACL shape provided in the `body`of the request with the specified `:id`   |\\n| `/api/:id`  |  `DELETE` | Deletes the SHACL shape stored in Coppola with the provided `:id`   |\\n| `/api/:id`  |  `POST` | Validates a sample payload provided in the `body`of the request using the SHACL shape related to the provided `:id`. The request must specify the format of the sample payload using the argument format, in case is a JSON-LD 1.1 payload (`?format=json-ld 1.1`) or Turtle (`?format=turtle`) | \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-coppola> \"| Endpoint | Method | Description |\\n|--|--|--|\\n| `/api`  |  `GET` | Returns the list of SHACL shapes stored in Coppola |\\n| `/api/:id`  |  `GET` | Returns the SHACL shape stored in Coppola with the provided `:id`  |\\n| `/api/:id`  |  `PUT` | Stores a SHACL shape provided in the `body`of the request with the specified `:id`   |\\n| `/api/:id`  |  `DELETE` | Deletes the SHACL shape stored in Coppola with the provided `:id`   |\\n| `/api/:id`  |  `POST` | Validates a sample payload provided in the `body`of the request using the SHACL shape related to the provided `:id`. The request must specify the format of the sample payload using the argument format, in case is a JSON-LD 1.1 payload (`?format=json-ld 1.1`) or Turtle (`?format=turtle`) | \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-information-objects> \"Repository for the Information Objects Ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FAIR-Research-Object> \"Tool to calculate the FAIRness of a Reserch Objects\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LOT-resources> \"This repository contains the resources associated to LOT methodology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-health-security> \"This repository contains the code and documentation generated for the health and security issues ontology which is available at: http://bimerr.iot.linkeddata.es/def/health-security-issues \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/BO2DM> \"Repository for the BIMERR to Data Model Converter which transforms an OWL ontology into the data model format of the BIMERR project.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/delta-ontology> \"Repository for collaborative edition of the DELTA ontology. This ontology is being developed within the context of the [DELTA H2020 project](https://www.delta-h2020.eu/). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gtfs-csv2rdf> \"Mapping script which transforms GTFS CSV into GTFS RDF\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ai4gov-website> \"Repository for the AI4Gov website\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OWL-To-OAS-Specification> \"This repository describes how to do a mapping between [OWL](https://www.w3.org/TR/2004/REC-owl-guide-20040210/) and [OpenAPI Specification (OAS)](http://spec.openapis.org/oas/v3.0.3). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SendEmailWebApp> \"Webapp with formulary to send email a specified destinatary.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ttla> \"\\nThis application is meant to be an automated experiment and not\\nan application by it self to annotated numeric columns. Nonetheless, \\nwe are planning to create an application based on this approach\\ndetails will be mentioned here once we start.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-annotation-objects> \"BIMERR ontology for the annotated information objects domain\\r\\n\\r\\nThis repository contains the code and documentation generated for the annotated information objects ontology which is available at: http://bimerr.iot.linkeddata.es/def/annotation-objects\\r\\n\\r\\nCurrent version of the ontology model\\r\\n![Current version of the model](https://github.com/oeg-upm/bimerr-annotation-objects/blob/master/diagrams/ontology.svg \\\"Building model\\\")\\r\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/astrea-web> \"This repository is a web service that publishes the services of [Astrea]([https://github.com/oeg-upm/Astrea](https://github.com/oeg-upm/Astrea)), providing a user-friendly interface to generate SHACL shapes from one or more ontologies, and also a REST API to be used by third-party services.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Optum](https://www.optum.com) - Health related, known to use semantic graphs (marklogic).\\n- [DarkLight](https://www.darklight.ai) - DarkLight is an Artificial Intelligence Expert System for Active Cyber Defense and           Trusted Information Sharing.\\n- [Volkswagen UK](https://www.volkswagen.co.uk)\\n- [Siemens](https://www.siemens.com)\\n- [IBM](http://www.ibm.com)\\n- [Elsevier](https://www.elsevier.com) - Global information analytics business that helps institutions and professionals advance healthcare, open science and improve performance for the benefit of humanity\\n- [BestBuy](http://bestbuy.com)\\n- [Google](http://google.com)\\n- [Facebook](http://facebook.com)\\n- [BBC](https://www.bbc.com)\\n- [NASA](https://www.nasa.gov)\\n- [K Health](https://khealth.ai) - Self diagnosing app.\\n- [Schneider Electric](https://www.schneider-electric.com/ww/en/)\\n- [Osthus](https://www.osthus.com)\\n- [DataLanguage](https://datalanguage.com/)\\n- [Eccenca](https://eccenca.com/en/)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fair_ontologies> \"FOOPS! is an application for validating whether a vocabulary (OWL or SKOS) conforms with the FAIR data principles. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lubm4obda> \"The **[mappings](https://github.com/oeg-upm/lubm4obda/tree/main/mappings)** directory of this GitHub repository contains all the R2RML and RML documents. The following mappings are provided: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [CDT](https://ci.mines-stetienne.fr/lindt/v2/custom_datatypes.html)\\n- [QUDT](http://www.qudt.org)\\n- [RDF Datatyping](http://infolab.stanford.edu/~melnik/rdf/datatyping/) - This document summarizes the common understanding of the RDF Core Working Group (further referred to as WG) with regards to the theoretical foundation for datatyping of literal values and serves as a basis of definition, discussion, and comparison of all proposed schemes for achieving a complete datatyping solution which are to be considered by the WG.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"The JDT library implement as Java objects the whole model, and its restrictions, defined in the [Thing Description standard](https://www.w3.org/TR/wot-thing-description/). The overview of the model is the following:\\n\\n![Thing Description model](https://www.w3.org/TR/wot-thing-description/visualization/td.png)\\n\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/weather1> \"Ontology for weather phenomenon. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/CODICE-extractor> \"Extractor of data from CODICE into CSVs. Created as part of a TFG on 2021\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-metadata> \"Repository for the BIMERR Metadata Ontology\\r\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fcm-cpp> \"\\nThis project implements Fuzzy c-means from the original paper by James Bezdek.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-iot-ontology> \"This repository contains the code and documentation generated for the COGITO IoT ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-annotation-objects> \"BIMERR ontology for Annotated Information Objects domain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/astrea-web> \"Astrea-web is a service that offers a friendly interface for the Astrea core library, as well as, a Rest API for third-party services\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-renovation-process> \"This repository contains the code and documentation generated for the Renovation Process ontology which is available at:\\r\\nhttp://bimerr.iot.linkeddata.es/def/renovation-process\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/w3id.org> \"For the technically savvy, the preferred way to create the redirect yourself is\\nby following these steps: \\n1. Fork the [perma-id/w3id.org](https://github.com/perma-id/w3id.org) \\n   source code repository.\\n2. Add a new re-direct entry. For a simple example, see\\n   [security/.htaccess](security/.htaccess) \\n3. (Optional) Add a `README.md` detailing contact persons and \\n   (a subset of) your permanent identifiers. For an example, \\n   see [rdw/README.md](rdw/README.md)\\n4. Commit your changes and submit a \\n   [pull request](https://github.com/perma-id/w3id.org/pulls).\\n5. w3id.org administrators will review your pull request and merge it if \\n   everything looks correct. Once the pull request is merged, the changes go\\n   live immediately. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AI4EU_raidologist> \"The implementations of the different modules are contained in the *functions.py* file from the internal_functions folder. By default, the following models are employed for each of the denoted modules: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-eval> \"Evaluation of Knowledge Graph Construction Engines\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-kgg> \"Knowledge Graph Generator for the COGITO Project \\nSELECT ?space \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/licensius> \"This software project is a an evalutor of ODRL expressions according to the W3C ODRL Recommendations: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-type-graph> \"\\nThis piece of code is to be used to build type graphs. This is not meant to be used alone.  \\nIt include pieces of code to compute the *specificity* and *coverage* but extra code to coverage\\nin the first place need to exist in the application using this library.\\n   \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oatapi> \"Ontology Artefacts to API\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"The owner of the repository, it is taken from `${{ github.repository_owner }}`.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"A curated list of various semantic web and linked data resources.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-hdt-entity> \"Add classes to entity columns in tabular data using HDT as the knowledge source\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loom-ld> \"It is a java project using maven to manage dependencies.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OWL-To-OAS-Specification> \"A repository for describing the mapping between OWL and OAS\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Widoco> \"We are working on the following features:\\n* Means to add examples to your ontology terms.\\n* Previsualization of the terms that will be generated. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"`None` indicates that the results are not stored in the repository. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ssspotter> \"\\n|endpoint|method|description|\\n|---------|---------|------------|\\n|`/spot`| `GET`|a view that allow uploading a table and the system will spot the subject column|\\n|`/spot`| `POST`| To spot the subject column of a given table (see the parameters below)| \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-weather> \"Repository for the BIMERR weather ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Turtle and SPARQL syntax highlighter](https://github.com/abcoates/sublime-text-turtle-sparql)\\n- [Linked Data syntaxes](https://github.com/blake-regalia/linked-data.syntaxes) - Syntax highlighting for SPARQL 1.1/SPARQL*, Turtle/Turtle*, TriG, N-Triples, N-Quads, Notation3, and ShExC.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"This repository contains the code and releases of the official plugins that can be used with both the Helio Materialiser [for users](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-Users) or [for developers](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-developers) and the [Helio Publisher](https://github.com/oeg-upm/helio/wiki/Helio-Publisher). The following table summarises the plugins available by their type: \\n| Plugin Type   | Plugin       | Description                                                                                                                                                 |\\n|---------------|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| Data Provider | [BashProvider](https://github.com/oeg-upm/helio-plugins/tree/master/providers/bashprovider#helio-bash-provider-plugin) | This plugin allows to run a set of bash commands, and then, read a file with potential results; e.g., run a docker process and feed Helio with some results |\\n| Data Provider | [EthereumProvider](https://github.com/oeg-upm/helio-plugins/tree/master/providers/ethereum-provider) | This plugin allows to collect a block or a set of blocks of an [Ethereum Blockchain](https://ethereum.org/en/). Those blocks contain all the original data and meta-data, the data of the block is expressed in JSON |\\n|               |              |                                                                                                                                                             | \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/website> \"Official repository with all source code to generate the OEG-UPM website\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/pycpulimit> \"Limit CPU usage for your processes by name using `cpulimit`\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [fedora](https://duraspace.org/fedora/) - Repository platform with native linked data support.\\n- [warp](https://github.com/linkeddata/warp) - Warp an LDP file manager.\\n- [Marmotta](https://github.com/apache/marmotta) - Apache linked data platform implementation.\\n- [Elda](https://github.com/epimorphics/elda) - Linked data platform from Epimorphics.\\n- [LDP4j](https://github.com/ldp4j/ldp4j)\\n- [gold](https://github.com/linkeddata/gold) - Linked Data server for Go.\\n- [CarbonLDP](https://github.com/CarbonLDP) - ($)\\n- [trellis](https://github.com/trellis-ldp/trellis)\\n- [Metreeca/link](https://github.com/metreeca/link)\\n- [ldpserver](https://github.com/hectorcorrea/ldpserver) - A mini LDP Server written in Go.\\n- [ldp-coap-framework](https://github.com/sisinflab-swot/ldp-coap-framework) - Linked Data Platform for the Constrained Application Protocol \\n- [cavendish](https://github.com/cavendish-ldp/cavendish) - A LDP Implementation backed by BlazeGraph.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/GEnI> \"Headquarters of GEnI: A framework for the Generation of Explainations and Insights for KGE predictions.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"Now your local environment and IDE are ready to start developing the plugin's code. For this end create a new java class for the plugin and extend any of the interfaces that Helio supports as pluggable. These interfaces and the goal of the code that implements them are the following:  \\n* *DataProvider* this kind of components retrieve and fetch data from a new data source, regarless the format of such data. For instance, a new provider could retrie the data from an MQTT broker regardless if the format of such data is json, xml, csv, or any other.\\n* *DataHandler* this kind of components are used to filter and handle the data relying on their format. For instance, a new handler could select some values from an XML file using Json Path expressions, or even iterate over a list in XML using these Json Path expressions.\\n* *HelioCache* this kind of components are used to store the RDF generated by Helio. For instance, a new cache could store in Git Hub the RDF been generated by Helio providing versioning capabilities to the RDF been produced by Helio in time.\\n* *Functions* this kind of components are used to extend the functions that can be called from the mappings in Helio.\\n* *MappingTranslator* this kind of components are used to extend the mapping languages that Helio understands. \\nOnce specified the interface to be implemented the IDE will require a set of methods to be implemented (to know more about these methods and their socope check the [Helio Java Docs](https://oeg-upm.github.io/helio-framework/), otherwise, the plugins in this repository can be taken as an example). As a result, these methods will implement the functionalities of the new plugin. \\nFinally, **it is required to create a file README.md documenting the plugin**, especially, indicating the structure of the expected json document to setup the new plugin. Once all the code is developed, register the new changes in the git branch of the new plugin. For this end, follow this excerpt of commands. \\nAfter pushing the changes to Git Hub, the last step to publish the plugin's code in the official Helio repository is opening a Pull Request \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/S4WATR> \"This repository contains the code and documentation generated for the SAREF extension for Water is published at https://w3id.org/def/S4WATR.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/pcake> \"An online application to show the distribution of a given class/property pairs of a SPARQL endpoint \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"In order to recover the information of a specific resource it is enough to add the following files: \\nA variable `?sid` is also available with a short version of the id (i.e without the namespace) \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/astrea-web> \"In order to use our REST API, check our [online documentation](https://astrea.linkeddata.es/swagger-ui.html)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The range of an edge *e* is composed by: \\n* All those nodes for which *e* is a reference.\\n* Datatype URIs that appear in a data-range restriction of *e* for a certain node.\\n```sql\\nSELECT DISTINCT ?e ?r WHERE {\\n    {?e rdfs:range ?r}\\n    UNION\\n    {\\n        ?d owl:onProperty ?e.\\n        { ?d owl:allValuesFrom ?r }\\n        UNION\\n        { ?d owl:someValuesFrom ?r }\\n        UNION\\n        { ?d owl:onClass ?r }\\n        UNION\\n        { ?d owl:onDataRange ?r }\\n    }\\n    FILTER(isURI(?e) && isURI(?r))\\n}\\n```\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-senML> \"BIMERR ontology for sensor data \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-map-score> \"Part of tada-gam project. This is responsible for assigning scores to each type given a column and a knowledge graph\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [MMOntologies](https://github.com/gatemezing/MMOntologies) - Multimedia ontologies studied for the paper \\\"The Landscape of Multimedia Ontologies in the last Decade\\\".\\n- [Wine](https://www.quora.com/What-is-wine-ontology) - Wine Ontology is a popular example of an OWL ontology.\\n- [Pizza](http://owl.cs.manchester.ac.uk/publications/talks-and-tutorials/protg-owl-tutorial/) - A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.\\n- [W3C Best Practices for Publishing Linked Data](https://www.w3.org/TR/ld-bp/)\\n- [Coursera - Web of Data](https://www.coursera.org/learn/web-data/) - A joint initiative between EIT Digital, Université de Nice Sophia-Antipolis / Université Côte d'Azur and INRIA - introduces the Linked Data standards and principles that provide the foundation of the Semantic web.\\n- [Linked Data Patterns](http://patterns.dataincubator.org/book/index.html)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fair_ontologies> \"Code for the OEG FAIR ontologies validator\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Jarsomatic> \"An application to run specific apps when a specific file(s) are changed on a repo in GitHub\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/valkyr-ie-gate> \"Valkyr-IE-Gate is a library for information extraction using the library GATE. The idea is to ease and encapuslate the complexity of the library to work with a simple API to exploit the two main components of the original library: Gazetteers and Jape Rules. \\nThe idea of the library is to work as a microservice web API in which you configure only declare and configure the main processes and you receive the annotations of the document in a JSON document. Easy to integrate and use with other NLP libraries of the domain to be invoked by other languages as Python.   \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_wrapper_module> \"Wrapper containing preprocessing file process and Helio translation.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-publisher> \"Helio publisher enables a Linked Data service providing a Web layer for interacting with the Helio integrator API. In other words, the publisher provides a REST API, and a set of HTML views, for reading the RDF of a resource, the whole RDF produced underneath, and enables a SPARQL endpoint. Additionally, although Helio already provides the HTML views, a user can manage the existing views and define new views. The views created by the user can have RDF embedded, and thus, publish RDFa documents.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/webODE> \"WebODE is an extensible ontology-engineering suite based on an application server, whose development started in 1999 and whose support was discontinued in 2006.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-entity> \"Tabular Data Annotation of entity columns\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-quality-ontology> \"This repository contains the code and documentation generated for the COGITO Quality ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor-incibe> \"This work contains the library JATE 2.0  and an extension to work with Spanish documents \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor> \"This work contains the library JATE 2.0  and an extension to work with Spanish documents \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-annotator> \"First build the project with maven.\\nThe compiled jar of the project is copied to the /dist folder, with all the necessary libraries.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/rdf-star-generation> \"The  SPARQL-anything test cases are used to determine the conformance to the [RDF-star specification](https://w3c.github.io/rdf-star/cg-spec/editors_draft.html) of tools that execute SPARQL-anything queries. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor-incibe> \"Terminology extractor based on JATE 2.0 for Cibersecurity corpora\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/beto-covid-sentiment-analysis> \"- ### _RetrieveTweets.py_:\\n  This script creates a dataset of tweets based on the keywords given and the number of tweets to be retrieved.\\n  - How to use it:\\n    ```\\n    python RetrieveTweets.py --search <keyword> --amount <number_of_tweets> --save_tweets_on_directory <directory> --twitter_token <your_token>\\n    ```\\n    - _--search_: parameter to search the keyword or keywords. If more than one keyword is going to be used then you need to quote them ('your keywords').\\n    - _--amount_: parameter to specify the number of tweets to retrieve (be aware that if considerable amount of tweets are going to be solicited, you may exceed Twitter's limitation of queries per minute).\\n    - _--save_tweets_on_directory_: parameter to specify where to store the tweets retrieved.\\n    - _--twitter_token_: parameter to specify your Twitter developer access token.\\n- ### _Preprocesing.py_:\\n  This script cleans the previous dataset obtained with _RetrieveTweets.py_ for it to be suitable to be used on _SentimentTweets.py_.\\n  - How to use it:\\n    ```\\n    python Preprocesing.py --dataset <directory> --save_directory <directory> [--merge [name][description]]\\n    ```\\n    - _--dataset_: parameter to specify the dataset to be cleaned.\\n    - _--save_directory_: parameter to specify where to store the cleaned dataset.\\n    - _--merge_: parameter to specify which columns will be merged with the _tweet_text_ column. It can be both (_name_ and _description_), just one of them or none.\\n- ### _CustomModel.py_:\\n  This script contains the code necessary to build the model class. It is imported in the script _SentimentTweets.py_ for it to be used as the model for the fine tuning task. It recieves the name of the BETO based model to be used.\\n- ### _SentimentTweets.py_:\\n  This script contains all the code necessary to do the fine tuning task. Recieves the train and test datasets to fine tune the model. It can save the model after fine tuned and gives an output with the results of the training.\\n  - How to use it:\\n    ```\\n    python SentimentTweets.py --train_data <directory> --test_data <directory> --model_name <name_or_path> [--save_model_on_directory <directory>]\\n    ```\\n    - _--train_data_: parameter to specify the train dataset to be used.\\n    - _--test_data_: parameter to specify the test data to be used.\\n    - _--model_name_: parameter to specify the name of the model (from hugging face) to be used.\\n    - _--save_model_on_directory_: parameter to specify where to store the fine tuned model.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-renovation-measures> \"BIMERR Ontology for the Renovation Measures domain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/hola-si-protocol> \"Hola is a semantic interoperability protocol \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easysparql> \"A python wrapper to easily query knowledge graphs with SPARQL\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"In order to build a JTD object from a set of RDF triples there are two main methods:\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The simplest Agora Engine is composed by a Fountain and a Planner. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-resources-ontology> \"This repository contains the code and documentation generated for the COGITO Resources ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-tourism-ontology> \"This repository contains the code and related resources for the tourism domain AURORAL ontology \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"The enrichment app is the core of this project. It's an app that infinitively and periodically does the following sequence of operations: \\n\\n1. Checks for pending jobs in the database. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [foaf](http://www.foaf-project.org/) - Friend of a Friend (FOAF) ontology.\\n- [uberon](http://uberon.github.io) - Integrated cross-species ontology covering anatomical structures in animals.\\n- [juso-ontology](http://rdfs.co/juso/latest/html) - Vocabulary for describing geographical addresses and features.\\n- [obo-relations](http://obofoundry.org/ontology/ro.html) - Relation Ontology. Relationship types shared across multiple ontologies.\\n- [orderedlistonto](https://github.com/smiy/orderedlistonto) - The Ordered List Ontology.\\n- [evidenceontology](http://evidenceontology.org) - EVIDENCE & CONCLUSION ONTOLOGY.\\n- [bevon](http://rdfs.co/bevon/latest/html) - Beverage ontology.\\n- [cyber-ontology](https://github.com/daedafusion/cyber-ontology) - Cyber Intelligence Ontology.\\n- [doap](https://github.com/edumbill/doap) - RDF schema for describing software projects.\\n- [qb4olap](https://github.com/lorenae/qb4olap) - A Vocabulary for Business Intelligence over Linked Data.\\n- [Hydra](https://github.com/lanthaler/Hydra) - A lightweight vocabulary for hypermedia-driven Web APIs.\\n- [vocab-transit](https://github.com/wwaites/vocab-transit) - RDF Schema for transit data.\\n- [ssso](https://github.com/gbv/ssso) - Specification of Simple Service Status Ontology.\\n- [dso](https://github.com/gbv/dso) - Specification of Document Service Ontology.\\n- [schema.org](https://schema.org/docs/datamodel.html) - Structured data on the Internet (Google, Microsoft, Yahoo and Yandex).\\n- [SPAR](http://www.sparontologies.net) - Semantic Publishing and Referencing Ontologies.\\n- [BFO](http://basic-formal-ontology.org) - Basic Formal Ontology.\\n- [VIVO ISF](https://wiki.duraspace.org/display/VTDA/VIVO-ISF+Ontology) - Researchers and the full context in which they work.\\n- [yago](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/) YAGO is a huge semantic knowledge base, derived from Wikipedia WordNet and GeoNames.\\n- [dbpedia](http://dbpedia.org/ontology/)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Instituto-Estudios-Fiscales-ontologias> \"The material generated in the different activities carried out during the development of the vocabulary, use\\ncases, user stories, glossary of terms, etc., will be available in the [Vocabulary Wiki](#)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ainn-request> \"Request and Answers for Mappings\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"````\\nModel modelTD = JTD.toRDF(thing)\\n # Alternativelly\\nJsonObject jsonTD = thing.toJson()\\nmodelTD = JTD.toRDF(jsonTD)\\n````\\n\\nNotice that using the method alternative `JTD.toRDF(jsonTD)` there is actually no need to serialise the JSON-LD framed `jsonTD` as a Java object, it can be directly translated into RDF.\\n\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/declarative-functions> \"We follow the Function Ontology to define transformation functions that are using in the rml mappings for improving the data quality of a Knowledge Graph Creation process. \\nThe repository is opened for contributions. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Widoco> \"Wizard for documenting ontologies. WIDOCO is a step by step generator of HTML templates with the documentation of your ontology. It uses the LODE environment to create part of the template.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-csv> \"Morph-CSV is an open source tool for querying tabular data sources using SPARQL. It exploits the information from the query, RML+FnO mappings and CSVW metadata to enhance the performance and completness of traditional OBDA systems (SPARQL-to-SQL translators). At this moment can be embebed in the top of any R2RML-compliant system. For detail information, watch the [introductory video about Morph-CSV](https://www.youtube.com/watch?v=bW9Wj7KUuGY). IF you have any related question on how to create RML+FnO or CSVW annotations, please ask to the [W3C Community Group on Knowledge Graph Construction](https://github.com/kg-construct/rml-questions/discussions) \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/yatter> \"Translate YARRRML into easy-to-read [R2]RML mappings\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Themis> \"In order to use our REST API, check our [online documentation](http://themis.linkeddata.es/swagger-ui/index.html) \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The domain of an edge *e* is composed by all those nodes for which *e* is a property.\\n```sql\\nSELECT DISTINCT ?e ?c WHERE {\\n    { ?p rdfs:domain ?c }\\n    UNION\\n    { ?c rdfs:subClassOf [ owl:onProperty ?e ] }\\n    FILTER (isURI(?e) && isURI(?c))\\n}\\n```\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ai4gov-website> \"This repository contains the materials used for the AI4Gov website. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [WGS84](https://www.w3.org/2003/01/geo/) - Basic Geo (WGS84 lat/long) Vocabulary.\\n- [skos](http://www.w3.org/2004/02/skos/core.html) - SKOS Simple Knowledge Organization System.\\n- [skos-xl](http://www.w3.org/TR/skos-reference/skos-xl.html) - SKOS Simple Knowledge Organization System eXtension for Labels.\\n- [vcard](https://www.w3.org/TR/vcard-rdf/) - vCard Ontology - for describing People and Organizations.\\n- [void](https://www.w3.org/TR/void/) - Describing Linked Datasets with the VoID Vocabulary.\\n- [time](https://w3c.github.io/sdw/time/) - Time Ontology in OWL.\\n- [org](https://www.w3.org/TR/vocab-org/) - The Organization Ontology.\\n- [data-cube](https://www.w3.org/TR/vocab-data-cube) - The RDF Data Cube Vocabulary.\\n- [pim](https://www.w3.org/2000/10/swap/pim/contact)\\n- [dqv](http://www.w3.org/ns/dqv#) - Vocabulary for describing quality metadata.\\n- [prov-o](https://www.w3.org/TR/prov-o/) - Represent provenance information.\\n- [dcat](https://www.w3.org/TR/vocab-dcat/) - DCAT is an RDF vocabulary designed to facilitate interoperability between data catalogs published on the Web.\\n- [prof](https://w3c.github.io/dxwg/profilesont/) The Profiles Ontology is an RDF vocabulary to describe profiles of (one or more) standards for information resources.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_data_repository> \"Repository for COGITO data DEMO\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [sparql-gremlin](https://github.com/apache/tinkerpop/tree/master/sparql-gremlin) - SPARQL to Gremlin Translator available as a plugin of the popular Apache TinkerPop graph computing framework. \\n- [Gremlinator](https://github.com/LITMUS-Benchmark-Suite/sparql-to-gremlin/) - SPARQL to Gremlin standalone Translator available as an independent implementation for open use in custom use cases. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AI4EU_raidologist> \" 1. ***Image feature extraction***: A combination of a keypoint-based image feature algorithm (KAZE) and a pretrained Convolutional Neural Network (ResNet18). \\n 2. ***Document embedding***:  An English pretrained model from SciSpacy is used (en_core_sci_md)\\n 3. ***Named Entity Recognition***: An external resource, CliNER, is employed for this task. The employed pretrained model works for English textual data, and distinguishes between three types of named entities: problems, treatments and tests. These types **MUST** remain unchangeable in case of substitution. \\n 4. ***Noise filtering***: SciSpacy is used to detect existing abbreviations. The ratio of identified abbreviations is obtained as *#_(detected_abbreviations ^ existing_tokens)/ #_detected abbreviations*\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/map4rdf> \"Map4RDF allows visualising and interacting with Linked Geospatial Data available in any SPARQL endpoint\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"Currently, the Web of Things provides [an official SHACL shape document](https://github.com/w3c/wot-thing-description/blob/main/validation/td-validation.ttl) for validating Thing Descriptions. This shape, or any other, can be used to validate a JTD Thing as follows:\\n\\n````\\nString shapesURI = \\\"https://raw.githubusercontent.com/w3c/wot-thing-description/main/validation/td-validation.ttl\\\"\\nModel shapesGraph = RDFDataMgr.loadModel(shapesURI, Lang.TURTLE);\\nValidationReport shapeReport = JTD.validateWithShape(thing, shapesGraph);\\n````\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mappingpedia-contents> \"Repository for storing mappings of mappingpedia\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [grafter](https://github.com/Swirrl/grafter) - Linked Data & RDF Manufacturing Tools in Clojure.\\n- [kr](https://github.com/drlivingston/kr) - Clojure API for RDF and SPARQL - provides consistent access to APIs including Jena and Sesame.\\n- [clj-plaza](https://github.com/antoniogarrote/clj-plaza) - Clojure rdf framework.\\n- [seabass](https://github.com/ryankohl/seabass) - A library for working with RDF with Jena in Clojure.\\n- [aristotle](https://github.com/arachne-framework/aristotle) - RDF, SPARQL and OWL for Clojure\\n- [aesopica](https://github.com/newres/aesopica) -  A Clojure library designed to help create Semantic Web based applications. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"\\nFor example, if you're using Github, you can use it to set up a hook that updates the resources for your R4R project on your staging server, whenever you push changes to the master branch of your project. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-website> \"This code generates the website of the morph family suite open source tools getting the information from an SPARQL endpoint. The original data is describe as a set of CSV files and transforming to RDF using RML mappings and the [SDM-RDFizer](https://github.com/SDM-TIB/SDM-RDFizer) tool. The obtained RDF is aligned with the http://schema.org vocabulary, which is used to incoporate RDFa annotations in the HTML. SPARQL endpoint is available at https://morph.oeg.fi.upm.es/sparql and the used engine to query the endpoint is [GraphQL-LD](https://www.npmjs.com/package/graphql-ld). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-morph-kgc> \"GitHub Action to create a knowledge graph from heterogeneous data sources using RML mappings and Morph-KGC\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"Sometimes the type of the resource is required to identify it, and adding the ID to the namespace is not enough:\\n    \\n    \\n    https://eu.dbpedia.org/movies/WarGames\\n      \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TPool> \"It supports py2 and py3 \\nThread Pool for python 2 (and 3) with multiple parameters. \\nPython2 include an undocumented thread pool which \\nonly accept functions with single arguments. TPool \\nimplements a pool for threads supporting multiple arguments  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-eval> \"We test the performance and scalability of a set of KG construction engines: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/hola-si-protocol> \"In this project, an implemented P2P backend protocol enables clients to parliament and exchange their ontologies between them, before merging individually both data and reducing it into a uniform ontology that serves as a translation between both clients' ontologies, allowing further data exchange between clients as if both used the same data structures. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oeg-upm.github.io> \"Repository used to generate the OEG-UPM group website for development\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/covid19> \"In this repository we are providing links on the contributions that the Spanish academic community can provide in order to apply Artificial Intelligence techniques over the COVID-19 Open Research Dataset (CORD-19), as well as encouraging all to contribute to this voluntary initiative.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/eWoT> \"In order to include a new IoT device in the ecosystem, making it interoperable, a user must register in the SPARQL endpoint that eWoT relies on a Thing Description (TD). The thing description must be stored in a named graph which name is the subject that has the type **core:Thing**. The TD must contain a specification of where ther IoT device data is, by means of the Web of Things Thing Description ontology, and also, specify how such data is translated by means of a WoT-Mapping. Find below an example of TD with the WoT-Mappings.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-gft> \"morph-GFT is an extension of Morph that works with Google Fusion Table (GFT) tables mapped with R2RML Mappings and enables users to query those tables using SPARQL queries\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OnToology-view-mock> \"This is a mock used for the development of the new OnToology view.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cpv-classifier> \"\\r\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6554604.svg)](https://doi.org/10.5281/zenodo.6554604) [![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\\r\\n\\r\\nIn this work we compare different approaches to Common Procurement Vocabulary (CPV) codes classification, using data extracted from the [Spanish Treasury](https://www.hacienda.gob.es/es-ES/GobiernoAbierto/Datos%20Abiertos/Paginas/LicitacionesContratante.aspx). \\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/DeltaCimApp> \"In order to correctly configure and use the CIM, the user is kindly asked to check the [CIM's wiki]([https://github.com/oeg-upm/DeltaCimApp/wiki](https://github.com/oeg-upm/DeltaCimApp/wiki)) \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/IEBrain> \"IE Brain project is about the innovation representation and business modelling.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-tourism-ontology> \"This repository contains the code and related resources for the tourism domain AURORAL ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/widaug> \"Data Augmentation for NLP tasks using wikidata \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/DeltaCimApp> \"The CIM allows local infrastructures to communicate with others though a peer-to-peer network. Besides allowing the access of remote infrastructures, the CIM implements a semantic interoperability layer, which translates heterogeneous payloads into JSON-LD modelled with the DELTA ontology by means of interoperability modules. The translation is bidirectional, therefore, when needed the CIM also translates from JSON-LD to a set of heterogeneous formats that follow different  models. This potentially leads to having systems developed with different standards communicating transparently. \\nAdditionaly, the CIM allows to consume the data of a local infrastructure using SPARQL queries, or, consume the data from the cloud, i.e., the peer-to-peer network, with SPARQL queries. Other functionalities implemented are: validation of payloads using SHACL shapes on the fly, access control list, jwt token authentication for local infrastructures to interact with the CIM, a GUI for configuring the CIM, and a documented REST API to use the CIM.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/201612-clarityhackathon-upm> \"This repository puts together all the work done during the CLARITY project (http://clarity-csa.eu/) Sprint Week in December 2016 on the work proposed by the Zaragoza city council on the usage of their open dataset (and API) about public services. Our aim has been to generate the artifacts that will be required in the future to start working on the homogeneisation of the lists of public services that are provided by municipalities in Spain.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/linked-gtfs> \"See also:\\n * The introduction at http://vocab.gtfs.org\\n * The [public CSV specification](https://developers.google.com/transit/gtfs/reference) where all semantics are derived from\\n * The [spec](spec.md)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/chowlk_spec> \"Repository for the chowlk project which aggregates the converter and the specification\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FAIR-Research-Object-API> \"The project has a restful API designed with flask on python. A description of the file can be found [here](https://app.swaggerhub.com/apis/esgg/FAIROs/1.0.0-oas3)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/transmodel-ontology> \"With the ultimate goal of enabling the provision of multimodal transportation services, the EU Regulation 2017/1926 is requiring transport stakeholders to allow access to their data in specific and [standarised](http://www.transmodel-cen.eu/) data formats (i.e., NeTEx, SIRI)  \\nCurrently, the requested data formats are rarely used, thus a data conversion is needed. Particularly *Transmodel Data Model* reflection in NeTEx can be driven by the [SNAP](https://www.snap-project.eu) solution which supports data conversion in different scenarios, decreasing the time required to perform it and hiding its complexity.  \\nThanks to Pieter Colpaert, in the context of work in a CEN Transmodel working group, the base URI https://w3id.org/transmodel/terms# was reserved and eventually our development has been already published.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/PPool> \"PPool is a Pool for Processes in Python with locks\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/yatter> \"The tool translates mapping rules from YARRRML in a turtle-based serialization of RML or R2RML.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"A metadata enrichment service for RO-Crate\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor-incibe> \"\\nTerminology extractor based on JATE 2.0 for Cibersecurity corpora\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/epnoi> \"This work is funded by the EC-funded project DrInventor ([www.drinventor.eu](www.drinventor.eu)).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Berlin SPARQL Benchmark (BSBM)](http://wifo5-03.informatik.uni-mannheim.de/bizer/berlinsparqlbenchmark/)\\n- [Lehigh University Benchmark (LUBM)](http://swat.cse.lehigh.edu/projects/lubm/)\\n- [IGUANA](https://github.com/dice-group/IGUANA)\\n- [dice-group/triplestore-benchmarks](https://web.archive.org/web/20180627155808/https://github.com/dice-group/triplestore-benchmarks) - An Evaluation of Triplestore Benchamrks.\\n- [RdfStoreBenchmarking](https://www.w3.org/wiki/RdfStoreBenchmarking)\\n- [Hobbit](http://project-hobbit.eu/) - Holistic Benchmarking of Big Linked Data.\\n- [SP2Bench](http://dbis.informatik.uni-freiburg.de/index.php?project=SP2B)\\n- [IGUANA](https://github.com/AKSW/IGUANA) - IGUANA is a benchmark execution framework for triple stores.\\n- [SRBench](https://github.com/jpcik/srbench) - A streaming sparql benchmark.\\n- [TFT](https://github.com/BorderCloud/TFT) - TFT (Tester for Triplestore) is a script PHP to pass tests through a SPARQL service.\\n- [OTM Benchmark](https://kbss.felk.cvut.cz/web/kbss/otm-benchmark) - A benchmark of object-triple mapping (OTM) libraries.\\n- [LDBC](http://ldbcouncil.org/benchmarks)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Conceptual-Mapping> \"The following diagram shows a general overview of the classes and properties of the Conceptual Mapping vocabulary. This diagram follows the [Chowlk notation](https://chowlk.linkeddata.es/notation.html). For a complete description of the ontology constructs, see the [documentation](http://vocab.linkeddata.es/def/conceptual-mapping/index-en.html). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [ontmalizer](https://github.com/srdc/ontmalizer) - Comprehensive transformations of XML Schemas (XSD) and XML data to RDF/OWL automatically.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kehio> \"Kehio does not need a user to implicitly annotate an attribute in order to store the subjects. Instead, if no attribute is specified with the annotation *'@RdfId'* Kehio asumes that the subject is always a blank node and handles it transparently. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FAIR-Research-Object> \"Repository for the work on evaluating FAIRnes of Research Objects\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-eval> \"Using the [GTFS-Madrid-Bench](https://github.com/oeg-upm/gtfs-bench) and based on the input dataset we create the following distributions to test the engines: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oeg-software-graph> \"Creation of a knowledge graph containing the catalog of software from the oeg-upm organization in GitHub\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/docker-freeling4> \"Docker Image for FreeLing 4 \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mapeathor> \"Translator of spreadsheet mappings into R2RML, RML or YARRRML\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/confs-info> \"List of submission dates for conferences and journals special issues relevant to the domain of semantic web\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The Fountain queries the given vocabularies in order to create the underlying link graph. Basically, it tries to find out the domain and range of all properties in the vocabulary with the aim of identifying the set of nodes and edges that make up such link graph. In the end, (a subset of) concepts and properties in the ontology become the nodes and edges of the link graph, respectively.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"The graph_uri for the SPARQL query.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fcm-cpp> \"Fuzzy c-means c++ library\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"This file contains the description of the API and it’s used to create the swagger documentation of the API: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Conceptual-Mapping> \"The Conceptual Mapping aims to gather the expressiveness of declarative mapping languages that describe the transformation of heterogeneous data sources into RDF. This ontology-based language settles on the assumption that all mapping languages, being used for the basic same purpose of describing data sources in terms of an ontology to create RDF, must have some basic patterns and inherent shared characteristics across all languages. The Conceptual Mapping's model is designed to represent and articulate these core features. \\nThe scope of the vocabulary is to represent features based on declarative languages for describing data sources, their access, mapping rules for RDF transformation and functions. It is out of the scope representing the entire expressivenes of \\\"procedural\\\" languages based on SPARQL, such as SPARQL-Generate of Facade-X.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Wikidata-class-diagram-generator> \"Generación de diagramas de ontologías basado en consultas SPARQL realizadas a Wikidata\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ontologia-ciberseguridad> \"Código generado para las distintas tareas del proyecto \\\"Lote 4 del proyecto: Servicios de obtención y clasificación de información para la caracterización del sector de la ciberseguridad\\\"\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [java2rdf](https://github.com/EBIBioSamples/java2rdf) - A simple library to map Java objects and Java beans onto RDF/OWL\\n- [PA4RDF](https://github.com/Claudenw/PA4RDF) - functionality on top of an RDF store while accounting for and exploiting the fundamental differences between graph storage and relational storage.\\n- [Empire](https://github.com/mhgrove/Empire/) - JPA implementation for RDF\\n- [Pinto](https://github.com/stardog-union/pinto) - A lightweight framework for mapping Java Beans into RDF and back again \\n- [Som(m)er](https://github.com/bblfish/sommer) - Semantic Object (Metadata) MappER\\n- [jennabean](https://code.google.com/p/jenabean/)\\n- [Alibaba](https://bitbucket.org/openrdf/alibaba)\\n- [rdfbeans](https://github.com/cyberborean/rdfbeans)\\n- [surfrdf](https://github.com/cosminbasca/surfrdf) - SuRF: a python Object RDF Mapper (ORM).\\n- [jtriple](https://github.com/konradreiche/jtriple) - A Java object model binding for RDF.\\n- [sparql-template](https://github.com/gushakov/sparql-template) - RDF store traversal with Jena API via automatic mapping between POJO and SPARQL. \\n- [JOPA](https://github.com/kbss-cvut/jopa) - A Java object-triple mapping library for RDF4J, Jena and OWL API.\\n- [RomanticWeb](https://github.com/MakoLab/RomanticWeb) - RDF-Object Mapping for the Semantic Web.\\n- [XML2RDF-DataTransformation-MappingTool](https://github.com/isl/XML2RDF-DataTransformation-MappingTool) - XML2RDF Data Transformation Tool (Mapping Tool): This generic data transformation tool maps XML data files to RDF files, given a schema matching definition, based on this Mapping Language Schema.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/w3id.org> \"Website source code for w3id.org\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"For instance, to get the list of starring characters in the film it is enough to create the following files: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ner4soft> \"Repository for experiments and corpora for named entity recognition (NER) for software repositories, code  and readme files. \\nThis repository contains a software-centric NER approach based (initially) in rules and gazetteers. The idea is to use this system from other applications to obtain better context of software documentation.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Ethereum-Smart-Contract-Downloader> \"A tool for extracting smart contracts from the Ethereum blockchain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/website-geo> \"GeoLinkeddata Repository (c) by Ontology Engineering Group \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/valkyr-ie-gate> \"Valkyr-IE-Gate is a library for information extraction based on the core of the library GATE.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fuzzy-c-means> \"Fuzzy c-means Clustering\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/coppola> \"Copla is a micro service for payload validation\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/yarrrml-validation> \"\\nThis repository contains the test cases for YARRRML, and their corresponding translation (when it is possible) to XRM, SMS2, and ShExML. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LDP4RO> \"Project designed to create and browse Research Objects following the W3C LDP protocol and using LDP4J (http://www.ldp4j.org/#/).\\nThe project consists on a client for easily creating ROs and a connector to handle the requests to LDP.\\nThis is a work in progress. \\nCurrently supported: Creation of simple ROs, RO description and RO browisng.  \\nCurrently working on: Adding folders, handling of Zip files, improvements to the client html (see issues).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/beto-covid-sentiment-analysis> \"Research project of Sentiment Analysis based on [this code](https://skimai.com/fine-tuning-bert-for-sentiment-analysis/) and using [this BETO model](https://github.com/dccuchile/beto). \\n_Note_: to gain access to the dataset, please, contact with [SerPablo (Pablo Calleja)](https://github.com/SerPablo)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/soca> \"SOCA now allows to produce a summary json of a given cards_data.json created by the previous portal step.\\nUser must decide whether or not to upload (default = false), or to create JSON file for output summary\\nFor building the summary we need to use the command `summary`\\n```\\n  -i, --input <dir-json-metadata>\\n                                  Dir repositories metadata in json format\\n                                  [required]\\n  -o, --output <path>             Dir where Software Catalog Portal will be\\n                                  saved  [default: summary]\\n  -U, --upload                    Will upload file to influxdb\\n```\\nExample\\n`soca summary -i cards_data.json -o test '`\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19> \"More info in our website [drugs4covid.oeg-upm.net](https://drugs4covid.oeg-upm.net)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ssn-resource-center> \"This repository contains all types of material that we consider useful for the management and usage of the W3C Semantic Sensor Network Ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-kgg> \"Knowledge graph generator for the COGITO project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-website> \"Website for Virtual Knowledge Graph morph suite\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The Fountain accepts vocabularies for registration in two different formats: Turtle and RDF/XML. In order to identify the only ontology that should be described in the submitted content, the Fountain parses it and queries the resulting graph with:\\n```sql\\nSELECT ?o WHERE {\\n    ?o a owl:Ontology FILTER (isURI(?o))\\n}\\n```\\n \\n* The **size** of the result set must be 1. That is, vocabularies have to be registered one at a time.\\n* There must be a **declared prefix** that is equal to the URI binded by `?o`. The name of such prefix will be considered by Agora as the **identifier** of the vocabulary. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gwt-blocks> \"GWT Building Blocks\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/docker-geokettle-x3geo> \"\\nReason 1: this docker image was created because there was a mismatch between compatible OS versions and libraries dependencies. Also, this image has all the software necessary to run all the features of the last version of GeoKettle and it will be updated when the dependencies, libraries or OS base would be safe to be updated. \\nReason 2: this docker image has been compiled with source code of GeoKettle last version and Java 8, so all the software and recent plugins are working well because there is compatibility between them. This is a critical feature, especially for Mac OS version. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/sdg-text-retriever> \"You can use this code to extract multiple types of text related to SDG's. First of all, you can get Goteo projects' description. To achieve that, I've used Goteo's API. You can try it by calling getProjectsFromGoteo function in the main class with the page number as a parameter. Each page contains 50 projects, and the information I persist in the database is the project name, short description, complete description and the owner ID. I have used RestTemplate framework to develop the request logic. There's a problem you should have in mind when making request to Goteo's services: there's a restriction in the number of request we can make in a short period of time. After that, you will always get a 429 error (TOO MANY REQUESTS), and you'll have to wait.  \\nYou can also get texts from SDG websites (like this one https://www.un.org/sustainabledevelopment/es/hunger/). For that, just call the function scrapWebONUX, where X is the SDG's number, passing the SGD'S website URL as a parameter. I persist the text obtained in my database. I have used JSoup library to do the web scrapping in Java. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/sdg-text-retriever> \"A program to retrieve texts related to Sustainable Development Goals from webs and PDF's\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vkg-tutorial-eswc2019> \"Despite the emergence of RDF knowledge bases, exposed via SPARQL endpoints or as Linked Data, formats like CSV, JSON or XML are still the most used for exposing data on the web. Some solutions have been proposed to describe and integrate these resources using mapping languages (e.g. RML, CSVW, kR2RML, etc) and many of those are equipped with associated RDF generators (e.g. RML-Mapper, CSVW generator, Karma, etc).that they can not manage efficiently the data when is volatile (they can retrieve not updated data) and the performance along the integration process is a key factor (rapid answers over the data) As these solutions generate materialized RDF, they cannot efficiently deal with volatile data or provide a SPARQL entry point directly to the data sources. In this tutorial, we explain how to use a suite of tools to manage and exploitdata in heterogeneous formats (CSV, RDB, JSON or REST API) without the need to materialize theresulting RDF in a triple store. First, we present TADA, a tool for automatically annotating CSV files using existing Knowledge Graphs. Second, we present HELIO, a Linked Data publisher that provides a unified access in real-time to multiple heterogeneous data sources. Finally, we present an OBDA approach to exploit CSV published on the Web providing access via SPARQL or GraphQL \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SMART-Protocols> \"SMART-Protocols is an ontology to represent experimental protocols, available at http://vocab.linkeddata.es/SMARTProtocols/\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/devops-infra> \"The ontology network is published using GitHub pages. The configuration file for enabling the content negotiation on w3id.org is available [here](https://github.com/perma-id/w3id.org/tree/master/devops-infra)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-eval> \"We conduct an evaluation of KGC engines considering several R2RML and RML processors to identify their strengths and weaknesses. We (i) perform a qualitative analysis of the distinctive features of each engine, (ii) examine their conformance with the mapping language specification they support, and (iii) assess their performance and scalability using the GTFS-Madrid-Bench benchmark. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AI4EU_raidologist> \"Code for the raidologist framework developed for the AI4EU Healthcare pilot.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ontology-BTN100> \"Repositorio donde se trabajará en la ontología a utilizar en BTN100\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-materials> \"The implementation of the code has been carried out using the [Python language](https://www.python.org/download/releases/3.0/). Also, we have used [Helio](https://oeg-upm.github.io/helio/)) to achieve the transformation of the data by means of mappings to RDF format.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-obxml> \"The implementation of the code has been carried out using the [Python language](https://www.python.org/download/releases/3.0/). Also, we have used [Helio](https://oeg-upm.github.io/helio/)) to achieve the transformation of the data by means of mappings to RDF format.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SMART-Protocols> \"The SeMAntic RepresenTation for Protocols, SMART Protocols, provides two ontology modules expressed in OWL to represent the document and workflow aspects of experimental protocols. The SMART Protocols-Document module aims to provide a structured vocabulary of concepts to represent information for reporting an experimental protocol. The SMART Protocols-Workflow module aims to provide a structured vocabulary of concepts to represent the execution of experimental protocols. The domain knowledge is modeled with the AO.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-annotator> \"Sign language annotator library for the EASYTV european project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Cloud_Helio_Adapter> \"An adapter to execute Helio service using real time json data.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"Once the project is forked the code must be cloned, for this end, the following command can be used. Bear in mind that */username* should be replaced with a valid username.\\n`````\\ngit clone https://github.com/username/helio-plugins.git\\n`````\\nAfter cloned, is recommended to create a new branch different from the master to isolate the new plugin's code. For this end, use the following command to create a new branch named *new-plugin*, however, any name could be given to the new branch.\\n`````\\ngit branch new-plugin\\n`````\\nBefore starting to develop any code, swap to the new brach using the command\\nBASH3*``\\nFollowing, in this new branch create a new folder in one of the existing directories depending on the type of plugin that will be developed. For instance, if the new plugin is a [Data Provider](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-Users#data-providers) then the new folder should be created under the existing folder *providers*. This new folder should have a suitable name that describes the plugin, try to follow the rule *[name]-[plugin type]*. For instance, for an mqqt provider the new folder should be called *mqtt-provider*. **IT IS IMPORTANT THAT ANY MODIFICATION TO THE HELIO PLUGINS REPOSITORY OCCURS UNDER THE NEW PLUGIN FOLDER AND NOTHING ELSE IS MODIFIED OUTSIDE SUCH FOLDER**.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-entity> \"Tabular Data Annotation of entity columns \\n## Database\\nHaving a database accessible by the application is a must. The file `mysql.cnf` should include the credential.\\nBelow is an example of the file content : \\n```\\n[client]\\ndatabase = mydbname\\nuser = dbuser\\npassword = dbpassword\\ndefault-character-set = utf8\\n```\\nTo user an *alternative* database engine, you might need to update the `settings.py` file \\nto specify the engine of your choosing (`sqlite` is not thread-safe, we don't recommend using it).  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-obxml> \"In [this folder](./Examples) we can see an example of an [xml file](./Examples/new.xml), in which from a series of transformations in another xml and later in a [json file](./Examples/new.json), a series of declarative mappings have been used to obtain the [RDF file](./RDF_Examples/new_obXML.ttl). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SmartDeveloperHub.github.io> \"Site for the Smart Developer Hub project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/hydrontology> \"Repository for the new version of Hydrontology. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/solarchem-ontology> \"This vocabulary focuses on the representation of photocatalysis processes described in scientific articles. The vocabulary defines several classes and properties that allow describing such experiments, being the class [phcat:MaterialTransformationProcess](#MaterialTransformationProcess) the central axis of it and defining through its properties the context of the complete experiment, such as the input chemicals and output, defined as [phcat:Input](#Input) and [phcat:Output](#Output); the conditions to perform the process, defined by [phcat:Condition](#Condition); and the article that mentions them by an element of the class [bibo:Article](http://bibliontology.com/content/article.html). In addition to these characteristics, material transformation processes have information that characterizes them, such as [phcat:operationMode](#operationMode), [phcat:Eg](#Eg) or energy band, [phcat:BET](#BET) or Surface Area and [phcat:Yield](#Yield). \\nThe elements of [bibo:Article](http://bibliontology.com/content/article.html) refer to the article in which the process is described. Among its properties are informative data about it, as well as relationships with the elements [bibo:Journal](http://gbol.life/ontology/bibo/Journal/) and [schema:Person](https://schema.org/person). The latter also contain information including [schema:Country](https://schema.org/Country) useful for searching by the researchers country of affiliation. The elements of the classes [phcat:Input](#Input) and [phcat:Output](#Output) store a reference of type [phcat:hasChemical](#hasChemical) with the class [chebi:CHEBI_24431](https://www.ebi.ac.uk/chebi/searchId.do?chebiId=CHEBI:24431) (a chemical) but only the input elements fulfill a role of type [phcat:MaterialTransformationProcess](#MaterialTransformationProcess) for the process . On the other hand, the process can have several types of conditions, all of them subclasses of [phcat:Condition](#Condition), which represent an element measurable by some unit of measure and quantities of the QUDT ontology such as [qudt:Unit]( https://qudt.org/schema/qudt/Unit) and [qudt:value](https://qudt.org/schema/qudt/value). \\nThe material generated in the different activities carried out during the development of the vocabulary, use\\ncases, user stories, glossary of terms, etc., will be available in the [Vocabulary Wiki](#) \\n# Project maintenance \\nTo manage those incidents or suggested improvements with respect to the vocabulary, we recommend you to follow\\nthe guides provided in [Issues Management](https://github.com/nombre-repositorio/wiki/issues-management) to\\ngenerate an issue (work in progress) \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Weviate](https://github.com/creativesoftwarefdn/weaviate)\\n- [rdfagents](https://github.com/joshsh/rdfagents) - Real-time messaging for the Semantic Web.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/termlex> \"This repository is intended to collect material that supports the development of the Termlex module, a an extension proposal of OntoLex-lemon to cover the current gaps regarding the publication of terminological resources in Semantic Web formats.\\nThe work started in October 2020 as part of the NexusLinguarum Action (http://nexuslinguarum.eu/), that supported the development with the founding of an short term scientific mission amongst the main institutions behind the work: UPM (Universidad Politécnica de Madrid) and DFKI (Deutsches Forschungszentrum für Künstliche Intelligenz). \\nThe repo contains the preliminary ontology in Turtle, and some use cases examples in the same format. \\nThese files are support the ongoing publication to present this module. More info about the proposal can be found in the Ontology Lexica Community Group Wiki page: https://www.w3.org/community/ontolex/wiki/Terminology \\nDISCLAIMER: Please, note that this is a preliminary version. This vocabulary is still unofficial and needs to be discussed within the Ontolex Chairs and the community group.  \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/doccano_formatter> \"Project to transform doccano outputs to different formats for research purposes\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-onto> \"Repository for the easyTV ontology.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [ruby-rdf](http://ruby-rdf.github.io)\\n- [rdf-serializers](https://github.com/ontola/rdf-serializers) - Adds RDF serialization to Ruby on Rails active model serializers\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"Restful API for RDF\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gtfs-bench> \"We present GTFS-Madrid-Bench, **a benchmark to evaluate declarative KG construction engines** that can be used for the provision of access mechanisms to (virtual) knowledge graphs. Our proposal introduces several scenarios that aim at measuring performance and scalability as well as the query capabilities of all this kind of engines, considering their heterogeneity. The data sources used in our benchmark are derived from the [GTFS](https://developers.google.com/transit/gtfs) data files of the subway network of Madrid. They can be transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Linked Data Fragments](http://linkeddatafragments.org/)\\n- [tomayac/ldf-client](https://github.com/tomayac/ldf-client) - Polymer Linked Data Fragments client.\\n- [LDFlex](https://github.com/RubenVerborgh/LDflex) - A JavaScript DSL for querying Linked Data on the Web.\\n- [communica](http://comunica.linkeddatafragments.org/) - A modular framework for querying Linked Data on the Web.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ttla> \"* The source code related to detection of data types (e.g. categorical, continuous, ...) is located under `detect`.\\n* while the files related to the annotation of the semantic types (e.g. height of a person) are located under `label`. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/declarative-functions> \"Definition of declarative functions following the Function Ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oeg-upm.github.io> \"Repository used to generate the OEG-UPM group website for development\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [sparql4idea](https://github.com/mattnathan/sparql4idea) - SPARQL language plugin for IntelliJ IDEA.\\n- [RDF and SPARQL](https://sharedvocabs.com/products/rdfandsparql/) - RDF and SPARQL plugin for JetBrains IDEs\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-material-properties> \"BIMERR ontology for the building domain\\r\\n\\r\\nThis repository contains the code and documentation generated for the Material Properties ontology which is available at: http://bimerr.iot.linkeddata.es/def/material-properties\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [LOV](https://lov.linkeddata.es) - Linked Open Vocabularies. Portal / search tool for vocabularies.\\n- [BioPortal](https://bioportal.bioontology.org) - Open repository with tools for ontologies and SKOS vocabularies; biomedical content dominates but all research domains welcome\\n- [prefix.zazuko.com](https://prefix.zazuko.com) - Similar to LOV, but with a richer search interface\\n- [OntoPortal](https://ontoportal.org) - The BioPortal software in Virtual Appliance (deployable) form\\n- [gist](https://www.semanticarts.com/gist/) - minimalist enterprise upper ontology - max coverage, fewest primitives, least ambiguity.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mapeathor> \"[Mapeathor](https://morph.oeg.fi.upm.es/tool/mapeathor) is a simple spreadsheet parser able to generate mapping rules in three mapping languages: R2RML, RML (with extension to functions from FnO) and YARRRML. It takes the mapping rules expressed in a spreadsheet and transforms them into the desired language. The spreadsheet template is designed to facilitate the mapping rules' writting, with the aim of being language independent, and thus, lowering the barrier of generating mappings for non-expert users. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-gft> \"Morph-GFT is an extension of Morph that works with Google Fusion Table (GFT) tables mapped with R2RML Mappings and enables users to query those tables using SPARQL queries. Underhood Morph-GFT, the SPARQL queries posed by the users are translated into SQL-like queries that are supported by GFT APIs. Unlike standard relational database implementation normally used with R2RML, GFT APIs do not support join operations. SPARQL-DQP is used to join the intermediate results and then the intermediate results are translated using the R2RML mappings specified by the users.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/software_catalog> \"The commands outlined above will re-generate the full catalog. However, it's possible to curate the list of repositories on top of which `scc` will be run on. The folder `repositories` contain an initial pass donde by scc with the full contents of the oeg organization. Simply modify the file `oeg-upm.csv` and add the repositories you are interested in. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kehio> \"As shown in the previous example,  sometimes the language can not be known beforehand or different objects could rely on different languages. For this cases, Kehio offers the possibility of sinking the language tag into the attribute value. \\n* *sinking language: it consist in storing the value of the RDF literal in the attribute along with the language tag, separated by the character '@'* \\nWhen the triples are serialised in as a Java object, and the titles printed, this is the output \\n\\nThe annotation `@RdfDatatype`  works with any `String` or `Collection` attribute. Consider that in RDF a datatype property has no restriction on its carnality, therefore if this annotation is used with a `String`\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-kgc-1> \"Powerful RDF Knowledge Graph Generation with [R2]RML Mappings\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-rdb> \"Virtual Knowledge Graph Creation from RDB with R2RML\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/solarchem-ontology> \"Vocabulary for the representation of photocatalysis processes mentioned in scientific papers. These experiments serve to define how a photocatalysis experiment has been done; under what circumstances, what products were used and what have been the results in order to analyze and reproduce the experiments mentioned in the articles. This vocabulary has been created in collaboration with the [IMDEA Energía](https://www.energia.imdea.org/) institute of Madrid. \\nThis vocabulary is based on data used by the [IMDEA Energy](https://www.energia.imdea.org/) institute as part of its **[Artleafs](http://www.artleafs.eu/)** project. This project is dedicated to storing information regarding scientific articles related to the field of artificial photosynthesis, which is used to report how the photocatalysis experiments are done, to facilitate access to information and reproducibility of the same. The vocabulary also reuses concepts from other ontologies such as *[the Bibliographic Ontology](https://bibliontology.com/)* (BIBO) and [schema.org](https://schema.org/) to represent articles that define the experiments and their authors; *[Chemical Entities of Biological Interest](https://www.ebi.ac.uk/chebi/)* (ChEBI) used to represent chemical elements and *[Quantities, Units, Dimensions, and Types](https://www.qudt.org/)* (QUDT) for quantities and units of measure. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The Linked Data principles [^1] enable the creation of the Web of Data: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-building> \"BIMERR ontology for the building domain\\r\\n\\r\\nThis repository contains the code and documentation generated for the building ontology which is available at: http://bimerr.iot.linkeddata.es/def/building\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-onto> \"Repository for the easyTV ontology. \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-health-security> \"BIMERR ontology for the health and security domain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"The *gathering place* for Distributed Linked Data. \\nThe Agora was a central space or square in ancient Greek city-states. The literal meaning of the word is \\\"gathering place\\\" or \\\"assembly\\\". \\nThe agora was the centre of athletic, artistic, spiritual and political life of the city. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/licensius> \"Collection of license-related services\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-plugins> \"Helio has been designed to be extensible through custom plugins that are dynamically loaded by either the Helio Materialiser [for users](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-Users) or [for developers](https://github.com/oeg-upm/helio/wiki/Helio-Materialiser-for-developers) and the [Helio Publisher](https://github.com/oeg-upm/helio/wiki/Helio-Publisher). Developing a new plugin does not require extending the core code of these software artefacts, instead, a plugin consists in an independent project that once compiled as jar is automatically identified and loaded by Helio. \\n\\nIn following subsections, all these steps are explained in detail. Notice that **any plugin developed, published, and released in the official Helio plugins repository will have an Apache 2.0 license**.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vocabUpdates> \"This repo allows you to add new ontologies to be included in http://vocab.linkeddata.es\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-renovation-measures> \"This repository contains the code and documentation generated for the Renovation Measures ontology which is available at:\\nhttp://bimerr.iot.linkeddata.es/def/renovation-measures \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-vocab> \"Vocabulario para DRUGS4COVID19\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Instituto-Estudios-Fiscales-ontologias> \"The XXX ontology describes the domain of XXX \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_data_repository> \"Repository for COGITO data DEMO\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OTALEX-C> \"Repositorio con los materiales relacionados con el proyecto OTALEX-C 2015\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-api> \"This is a web API project using tada-hdt-entity and the pytada-hdt-entity libraries\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"````\\nJsonObject jsonTD = thing.toJson()\\njsonTD = JTD.toJson(thing) # Alternativelly\\n````\\nNotice that using the method `JTD.toJson(thing)` any other class from the model can be deserialised.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"Only link traversal leverages available Linked Data (look-up) interfaces. A URI should not just serve as a global identifier, but also as provider of a structured data representation of the identified entity. The absolute majority of implemented solutions ignore both principles. \\nWhy do not we rely on these HTTP look-up interfaces to directly consume Linked Data? Is it really necessary to give them up in favor of using SPARQL endpoints or any other (non-LD) interface to efficiently access and query Linked Data?\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-kg> \"Resources for the generation and exploitation of the KG - Drugs4Covid \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/widaug> \"Widaug is a data augmentation system for named entity recognition (NER) using Wikdiata. \\nThe library is currently under construction to provide more facilities and uses to the final user. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ontology-BTN100> \"This repository contains the files generated for the development of the btn100 ontology. This ontology aims to represent the Spanish Topographic Base (1:100.000 scale) from the [Spanish Institute for Geographic Information](http://www.ign.es/web/ign/portal).  \\nThe ontology file is provided as [btn100.owl](https://github.com/oeg-upm/ontology-BTN100/blob/master/btn100.owl) and it is published at [https://datos.ign.es/def/btn100](https://datos.ign.es/def/btn100). In addition, the geo-core ontology file, which represents geographic objects and is used as the core of the btn100 ontology, is provided as [geo_core.owl](https://github.com/oeg-upm/ontology-BTN100/blob/master/geo_core.owl) and it is published at [https://datos.ign.es/def/geo_core](https://datos.ign.es/def/geo_core). \\nThe directories of this repository are structured as follows:\\n- OnToology contains the ontology documentation files for btn100 and geo-core.\\n- Diagrams contains the diagrams for the ontologies and the skos thesauri.\\n- Requirements contains the Excel file which contains the requirements which are represented by the ontology.\\n- Thesaurus contains the skos thesauri files in ttl format.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oatapi> \"In addition, this tool builds the SPARQL queries that allow get the desired data for solving the CQs. Currently, OATAPI only generates SPARQL CONSTRUCT queries as we are processing CQs that requires read-only operations.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-hdt-entity> \"**Annotate entity and textual columns using HDT** \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"| Format  | Description | Mime-type |\\n| :--- | :--- | :---: |\\n| [Turtle](https://www.w3.org/TR/turtle/) | Terse RDF Triple Language. | `text/turtle`, `application/x-turtle` |\\n| [TriG](https://www.w3.org/TR/trig/) | Plain text format for serializing named graphs and RDF Datasets. | `application/trig`, `application/x-trig` |\\n| [JSON-LD](https://json-ld.org/) | JSON-based Serialization for Linked Data. | `application/ld+json` |\\n| [RDF/JSON](https://www.w3.org/TR/rdf-json/) | RDF 1.1 JSON Alternate Serialization. | `application/rdf+json` |\\n| [N-Triples](https://www.w3.org/TR/n-triples/) | Line-based syntax for RDF datasets. |  `application/n-triples` |\\n| [N-Quads](https://www.w3.org/TR/n-quads/) | Line-based syntax for RDF datasets. | `application/n-quads`, `text/x-nquads`, `text/nquads` |\\n| [Notation3](https://www.w3.org/TeamSubmission/n3/) | Notation3 (N3): A readable RDF syntax. | `text/n3`, `text/rdf+n3` |\\n| [RDF/XML](https://www.w3.org/TR/REC-rdf-syntax/) | RDF/XML Syntax Specification. | `application/rdf+xml`, `application/xml` |\\n| [TriX](http://www.hpl.hp.com/techreports/2004/HPL-2004-56.html) | RDF Triples in XML. | `application/trix` |\\n| [HDT](https://www.w3.org/Submission/2011/03/) | Binary RDF Representation for Publication and Exchange. | `application/x-binary-rdf` |\\n| [aREF](https://gbv.github.io/aREF/aREF.html) | Another RDF Encoding Form. | |\\n| [RDF/POST](http://www.lsrn.org/semweb/rdfpost.html) | RDF/POST Encoding for RDF. | `application/rdf+x-www-form-urlencoded` |\\n| [YARRML](http://rml.io/yarrrml/spec/) | YARRRML is a human readable text-based representation for declarative generation rules. It is a subset of [YAML], a widely used data serialization language designed to be human-friendly. | |\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gtfs-bench> \"Our experiences testing (virtual) knowledge graph engines have revealed the difficulties for setting up an infrastructure where many variables and resources are involved: databases, raw data, mappings, queries, data paths, mapping paths, databases connections, etc. For that reason, and in order to facilitate the use of the benchmark to any developer or practitioner, we provide a set of [utils](https://github.com/oeg-upm/gtfs-bench/tree/master/utils) such as docker-compose templates or evaluation bash scripts that, in our opinion, can reduce the time for preparing the testing set up.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ainn-userm> \"Usermanagement micro-service for AI.nnotation\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/coppola> \"Coppola is a micro service directory for SHACL shapes. It provides a graphical interface for users which scope is help them managing shapes (creating, updating, deleting, or reading) and, also, apply these shapes by means of a Playground to sample payloads. In addition, Coppola publishes a REST API so third-party services can directly use its functionalities. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-coppola> \"Coppola is a micro service directory for SHACL shapes. It provides a graphical interface for users which scope is help them managing shapes (creating, updating, deleting, or reading) and, also, apply these shapes by means of a Playground to sample payloads. In addition, Coppola publishes a REST API so third-party services can directly use its functionalities. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/epw2rdf> \"Transform weather data in EPW format to RDF\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ShapesToWidocoHTML> \"TFG Mejora de Sistemas de Documentación de Ontologías y su Publicación mediante Funcionalidades de Validación\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-publisher> \"# [Helio Publisher](https://github.com/oeg-upm/helio/wiki/Helio-Publisher)\\n### **Creators:** [Andrea Cimmino](https://scholar.google.es/citations?user=_6U9WMcAAAAJ&hl=es&oi=ao) and [Raúl García Castro](http://www.garcia-castro.com/)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-wot> \"A Python-based framework for describing, discovering and accessing the Web of Things \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"Repository for the GitHub Action to send a SPARQL query to a SPARQL endpoint\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio-materialiser> \"Helio materialiser allows to generate RDF from heterogeneous sources of data. The generated RDF can be accessed (retrieving the dataset, accessing a resource, or solving a SPARQL query) or it can be injected automatically by Helio into an existing triple store. Furthermore, the RDF can be generated synchronously under demand, or asynchronously with a timer.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/BIMERR-KGG> \"Knowledge Graph Generator module for the BIMERR project\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-api> \"This is a web API project (with Swagger) using `tada-hdt-entity` and the `pytada-hdt-entity` libraries\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/BIMERR-KGG> \"Knowledge Graph Generator module for the BIMERR project\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vocabUpdates> \"3) wait for some VIP to accept it \\nCheck the gh-page branch (e.g. http://oeg-upm.github.io/vocabUpdates/site) to check whether the new site is ok or not before making the pull request to master (so that it is updated in vocab.linkeddata.es)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Widoco> \"WIDOCO helps you to publish and create an enriched and customized documentation of your ontology, by following a series of steps in a wizard. We extend the LODE framework by Silvio Peroni to describe the classes, properties and data properties of the ontology, the OOPS! webservice by María Poveda to print an evaluation and the Licensius service by Victor Rodriguez Doncel to determine the license URI and title being used. In addition, we use WebVowl to visualize the ontology and have extended Bubastis to show a complete changelog between different versions of your ontology.\\n\\nFeatures of WIDOCO:\\n* Automatic documentation of the terms in your ontology (based on [LODE](http://www.essepuntato.it/lode/))\\n* Automatic annotation in JSON-LD snippets of the html produced.\\n* Association of a provenance page which includes the history of your vocabulary (W3C PROV-O compliant).\\n* Metadata extraction from the ontology plus the means to complete it on the fly when generating your ontology. Check the [best practice document](http://dgarijo.github.io/Widoco/doc/bestPractices/index-en.html) to know more about the terms recognized by WIDOCO.\\n* Guidelines on the main sections that your document should have and how to complete them.\\n* Integration with diagram creators ([WebVOWL](http://vowl.visualdataweb.org/webvowl/)).\\n* Automatic changelog of differences between the actual and the previous version of the ontology (based on [Bubastis](http://www.ebi.ac.uk/efo/bubastis/)).\\n* Separation of the sections of your html page so you can write them independently and replace only those needed.\\n* Content negotiation and serialization of your ontology according to W3C best practices\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/confs-info> \"A single page that contains a list of conferences and journals relevant to the domain of the semantic web. It is updated regularly, *your contribution is highly valiable*. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FarolAppsWeb> \"This web application conect with FarolApp4All Api and show the farols.  \\nThis project is a GUI for FarolApp4All Api. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mgds-ids> \"MGDS IDS software components\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loupe-api> \"The Loupe API is a web service for proifiling Linked Data and currently three operations are available as illustrated in following figure. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-skyline> \"Morph-Skyline is an RDB2RDF engine for skyline queries developed by the Ontology Engineering Group, that follows the R2RML specification (http://www.w3.org/TR/r2rml/).  \\nSkyline queries are preference-based queries, which were proposed by Börzsönyi et al. [1] extending SQL by means of a SKYLINE OF clause as follows: SKYLINE OF d1 [MIN | MAX],..., dn [MIN | MAX] where where di denote the dimensions of the Skyline, and MIN and MAX specify whether the value in that dimension should be minimised or maximised. \\nMorph-Skyline extends Morph-RDB and it supports two operational modes: data upgrade (generating RDF instances from data in a relational database) and skyline query translation (SPARQL to SQL). Morph-Skyline has been tested on synthetic datasets using the data generator provided by Börzsönyi et al.[1]. At the moment, Morph-Skyline works with EXASol. \\nThis repository contains code of the engine and a set of general experiments to test perfomance, scalability and completeness of our approach.\\nAdditionally, for the ISWC'20 demo-paper \\\"Morph-Skyline: Skyline Queries for Virtual Knowledge Graph Access\\\", the resources and results can be found in the [demo-paper folder](https://github.com/oeg-upm/morph-skyline/tree/master/demo-paper) \\n<!--- Acknowledgement: Since January 2020, the development of morph-Skyline has been supported by the SPRINT project (http://sprint-transport.eu/).---> \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/epnoi> \"epnoi is an unified information access middleware which objective is to increase the awareness and simplify the search of relevant content in situations where multiple heterogeneous non-structured information sources are involved.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-cs> \"Citizen science project to validate our AI algorithm \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ner4soft> \"Repository for expriments and corpora for NER for code repositories and readme files\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-tutorial-iswc2020> \"Despite the emergence of RDF knowledge bases, exposed via SPARQL endpoints or as Linked Data, formats like CSV, JSON or XML are still the most used for exposing data on the web. Some solutions have been proposed to describe and integrate these resources using declarative mapping languages (e.g. RML, CSVW, KR2RML, etc) and many of those are equipped with associated RDF generators (e.g. RMLMapper, CSVW generator, etc). The use of these technologies enables the construction of knowledge graphs in a declarative way. However, they have a steep learning curve for new users. Our aim in this tutorial is, from a practical perspective, to explain in detail the process of constructing knowledge graphs, from writing mappings to their use with suitable tools. First, we describe the mapping structure and a tool to ease writing mappings, [Mapeathor](https://morph.oeg.fi.upm.es/demo/mapeathor), showing the main guidelines for attendants to create their own mappings. Then, we present Morph-CSV, a framework for virtual knowledge graph access over tabular data. Finally, we present [Helio](http://helio.linkeddata.es/), a generator from heterogeneous data sources and publisher of Linked Data that provides unified access in real-time. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"(Note: this classification is somewhat arbitrary and is meant to capture databases that only have a published paper or were developed for that purpose and are not actively maintained) \\n- [TripleRush](https://github.com/uzh/triplerush) - (OS).\\n- [corese](https://github.com/Wimmics/corese) - (OS).\\n- [rdf3x](https://code.google.com/archive/p/rdf3x/) - (OS).\\n- [gh-rdf3x](https://github.com/gh-rdf3x/gh-rdf3x) - (OS).\\n- [rdf3x-mpi](https://bitbucket.org/saikrishnan/rdf3x-mpi)\\n- [Qamel](https://github.com/dice-group/qamel) - RDF4J ported to Andriod.\\n- [dipLODocus](https://www.semanticscholar.org/paper/a1510214a16c73f464a8d1ae631054870114bbc8)\\n- [SW-Store](https://cs.uwaterloo.ca/~gweddell/cs848/papers/SW-Store.pdf)\\n- [Yars2](https://www.semanticscholar.org/paper/08bae32492f4f8a262ec990853613151cc484dc5)\\n- [Shard](https://sourceforge.net/projects/shard-3store/)\\n- [Hexastore](http://www.vldb.org/pvldb/1/1453965.pdf)\\n- [BitMat](https://www.cs.ox.ac.uk/people/medha.atre/papers/atre-ssws2009.pdf)\\n- [LUPOSDATE](https://www.ifis.uni-luebeck.de/index.php?id=luposdate-demo)\\n- [DREAM](https://github.com/CMU-Q/DREAM) - DREAM - Distributed RDF Engine with Adaptive Query Planner and Minimal Communication.\\n- [RIQ](https://github.com/UMKC-BigDataLab/RIQ) - RIQ is a new software tool for fast processing of SPARQL queries on RDF quadruples.\\n- [hyrise](https://github.com/hyrise/hyrise) - Hyrise is a research in-memory database. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [The Smart Data Analytics (SDA)](http://sda.tech/) - Research group, Institute for Computer Science at the University of Bonn, the Fraunhofer Institute for Intelligent Analysis and Information Systems (IAIS) and the Institute for Applied Computer Science Leipzig.\\n- [Agile Knowledge Engineering and Semantic Web (AKSW)](http://aksw.org) - The Research Group Agile Knowledge Engineering and Semantic Web (AKSW) is hosted by the Chair of Business Information Systems (BIS) of the Institute of Computer Science (IfI) / University of Leipzig as well as the Institute for Applied Informatics (InfAI).\\n- [University of Zurich Dynamic and Distributed Information Systems Group](http://www.ifi.uzh.ch/en/ddis.html)\\n- [WESO](http://www.weso.es/) - WESO is a research group at the University of Oviedo founded in 2004.\\n- [Max Planck Institute for Informatics](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/) - Department D5 of the Max Planck Institute for Informatics.\\n- [DICE: Data Science Group](http://dice.cs.uni-paderborn.de/about/) - Universität Paderborn.\\n- [Ontology Engineering Group (OEG)](http://www.oeg-upm.net/) - The Ontology Engineering Group (OEG) is based at the Computer Science School at Universidad Politécnica de Madrid (UPM).\\n- [Knowledge Representation and Reasoning Group (KRR)](https://krr.cs.vu.nl/) - Research group is based at the Vrije Universiteit Amsterdam (VU).\\n- [eXascale Infolab](https://exascale.info/) - eXascale Infolab, University of Fribourg, Switzerland.\\n- [Wimmics](http://wimmics.inria.fr/corese) - Wimmics stands for Web-Instrumented Man-Machine Interactions, Communities, and Semantics, a joint research team between INRIA Sophia Antipolis - Méditerranée and I3S (CNRS and Université Côte d'Azur).\\n- [Data Semantics Lab](https://dase.cs.wright.edu/) - Data Semantics Lab, Wright State University\\n- [Stanford BMIR](https://bmir.stanford.edu) - Stanford University Center for Biomedical Informatics Research\\n- [Exascale Infolab](https://exascale.info/projects/research/) - University of Fribourg, Switzerland\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"Java API for Thing Descriptions of the Web of Things\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/transmodel-ontology> \"A repository to work on the transmodel ontology that provides support to the NeTEx model\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/201612-clarityhackathon-upm> \"We have started with the open material that is made available by the Zaragoza city council in relation to the public services that they offer, namely: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ssspotter> \"Simple Subject Column Spotter\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Massive-ROs-Creator> \"Massive ROs Creator is a python program that, given a group of search parameters, navigates to the Norwegian Research Data Archive (NIRD), realizes an advanced search and recovers data from the resources obtained by this search.\\nThe program is set to insert this data into ROs in the ROHub platform. A functionality that will be added in posterior versions.\\nThe program uses automated web navigation in the local machine where it is being excuted.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/docker-freeling4> \"Docker Image for FreeLing 4 (4.2) for all languages.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/BRATtoBIO> \"Code to preprocess BRAT format annotation data to BIO format\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/eWoT> \"eWoT is an implementation that enables semantic interoperability for an IoT ecosystem. It relies on Thing Descriptions (TD) to profile the different IoT devices, and WoT-Mappings to translate their heterogeneus data into a normalised RDF modelled with a specific ontology. The ontologies endow for this purpose are the [Thing Description](http://iot.linkeddata.es/def/wot/index-en.html) and the [WoT-Mapping](http://iot.linkeddata.es/def/wot-mappings/index-en.html); nevertheless they can be extended with any other to enhance contextual information of the IoT device. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"Internally, R4R adds an ORDER BY clause to the sparql query with the closest property (by using the Levenhstein distance) to the one specified in the `sort` field. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TINTO> \"TINTO: Software to convert Tidy Data into Image for Classification with 2-Dimensional Convolutional Neural Networks\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/r4r> \"When considering paginated queries it is necessary to set the `ORDER` option in the Sparql query. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-kpi> \"This repository contains the code and documentation generated for the KPI ontology which is available at:\\r\\nhttp://bimerr.iot.linkeddata.es/def/key-performance-indicator\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-geo> \"Herramientas de generación de RDF sobre datos geográficos (Linked Data Geográfico) desarrolladas por el Grupo de Ingeniería Ontológica\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/transmodel-ontology> \"To manage those incidents or suggested improvements with respect to the vocabulary, we recommend you to follow the guides provided in [Issues Management](https://github.com/oeg-upm/transmodel-ontology/wiki/issues-management) to generate an issue (work in progress never ends)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor> \"Extension of JATE 2.0 for Spanish\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/DeltaCimApp> \"**All the current CIM releases send data about the status of the service to a private secured monitor service, that analyses the health of the services and prevents from potential attacks**\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vocab.linkeddata.es> \"Creation of a vocabulary catalogue and website, used in http://vocab.linkeddata.es/\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/beto-covid-sentiment-analysis> \"# beto-covid-sentiment-analysis\\nResearch project of Sentiment Analysis based on [this code](https://skimai.com/fine-tuning-bert-for-sentiment-analysis/) and using [this BETO model](https://github.com/dccuchile/beto).\\n\\n_Note_: to gain access to the dataset, please, contact with [SerPablo (Pablo Calleja)](https://github.com/SerPablo)\\n\\n## Description of the Scripts\\n- ### _RetrieveTweets.py_:\\n  This script creates a dataset of tweets based on the keywords given and the number of tweets to be retrieved.\\n  - How to use it:\\n    ```\\n    python RetrieveTweets.py --search <keyword> --amount <number_of_tweets> --save_tweets_on_directory <directory> --twitter_token <your_token>\\n    ```\\n    - _--search_: parameter to search the keyword or keywords. If more than one keyword is going to be used then you need to quote them ('your keywords').\\n    - _--amount_: parameter to specify the number of tweets to retrieve (be aware that if considerable amount of tweets are going to be solicited, you may exceed Twitter's limitation of queries per minute).\\n    - _--save_tweets_on_directory_: parameter to specify where to store the tweets retrieved.\\n    - _--twitter_token_: parameter to specify your Twitter developer access token.\\n- ### _Preprocesing.py_:\\n  This script cleans the previous dataset obtained with _RetrieveTweets.py_ for it to be suitable to be used on _SentimentTweets.py_.\\n  - How to use it:\\n    ```\\n    python Preprocesing.py --dataset <directory> --save_directory <directory> [--merge [name][description]]\\n    ```\\n    - _--dataset_: parameter to specify the dataset to be cleaned.\\n    - _--save_directory_: parameter to specify where to store the cleaned dataset.\\n    - _--merge_: parameter to specify which columns will be merged with the _tweet_text_ column. It can be both (_name_ and _description_), just one of them or none.\\n- ### _CustomModel.py_:\\n  This script contains the code necessary to build the model class. It is imported in the script _SentimentTweets.py_ for it to be used as the model for the fine tuning task. It recieves the name of the BETO based model to be used.\\n- ### _SentimentTweets.py_:\\n  This script contains all the code necessary to do the fine tuning task. Recieves the train and test datasets to fine tune the model. It can save the model after fine tuned and gives an output with the results of the training.\\n  - How to use it:\\n    ```\\n    python SentimentTweets.py --train_data <directory> --test_data <directory> --model_name <name_or_path> [--save_model_on_directory <directory>]\\n    ```\\n    - _--train_data_: parameter to specify the train dataset to be used.\\n    - _--test_data_: parameter to specify the test data to be used.\\n    - _--model_name_: parameter to specify the name of the model (from hugging face) to be used.\\n    - _--save_model_on_directory_: parameter to specify where to store the fine tuned model.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/linked-gtfs> \"The [General Transit Feed Specification (GTFS)](https://developers.google.com/transit/gtfs/reference) defines an open standard data format for public transport schedules. Unlike other public transport data standards, it emerged to meet specific practical needs in passenger information systems and is a relatively simple tabular format. GTFS first appeared in 2005 through cooperation between a public agency and a private company, as a way for Portland, Oregon's TriMet agency to provide schedule data for the then-experimental Google Transit routing service. The standard is now maintained and revised through a public process on the [gtfs-changes mailing list](https://groups.google.com/forum/#!forum/gtfs-changes). For more information on the origins of GTFS, see [chapter 10 of Beyond Transparency](http://beyondtransparency.org/chapters/part-2/pioneering-open-data-standards-the-gtfs-story/). \\nAn analysis of archived public feeds in the summer of 2012 found that after a period of rapid growth, \\\"about one fourth of the agencies in the U.S. have open route and schedule data, representing about 85 percent of the total passenger-miles served.\\\" [[1]](http://blog.openplans.org/2012/07/in-public-transit-the-number-of-passenger-miles-served-by-agencies-with-open-data-has-skyrocketed/) Open transit data is also becoming widely available in Europe. Most of this data is provided in the GTFS format either directly by transport agencies or by third party data aggregators.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [LinkedPipes-ETL](https://github.com/linkedpipes/etl) - Linked Data ETL pipeline.\\n- [gm-sparql](https://github.com/ssrangan/gm-sparql) - Graph Mining Using SPARQL.\\n- [SANSA-Stack](http://sansa-stack.net) - Scalable Semantic Analytics Stack.\\n- [tdbloader4](https://github.com/castagna/tdbloader4) - Prototype to show how TDB indexes can be generated using MapReduce.\\n- [jena-grande](https://github.com/castagna/jena-grande) - Jena Grande is a collection of utilities, experiments and examples on how to use MapReduce, Pig, HBase or Giraph to process data in RDF format.\\n- [mrlin](https://github.com/mhausenblas/mrlin) - MapReduce processing of Linked Data.\\n- [infovore](https://github.com/paulhoule/infovore) - RDF-Centric Map/Reduce Framework and Freebase data conversion tool.\\n- [FOX](https://github.com/AKSW/FOX) - Federated Knowledge Extraction Framework.\\n- [singal-collect](https://github.com/uzh/signal-collect)\\n- [Duke](https://github.com/larsga/Duke) - Duke is a fast and flexible deduplication engine written in Java.\\n- [ODCS](https://github.com/mff-uk/ODCS) - The tool uses data processing pipelines for obtaining, processing, and storing RDF data.\\n- [etalis](https://github.com/sspider/etalis) - Event Processing SPARQL (EP-SPARQL).\\n- [graph-pattern-learner](https://github.com/RDFLib/graph-pattern-learner) - Evolutionary Graph Pattern Learner that learns SPARQL queries for a given set of source-target-pairs from an endpoint.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/docker-geokettle-x3geo> \"Docker Image for GeoKettle and TripleGeoKettle plugin\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/owl2yarrrml> \"| Ontology        | Mapping           \\n| ------------- |:-------------:| \\n| Class     | TriplesMap + Simple PredicateObjectMap (rdf:type, class) | \\n| Data Property     | Simple PredicateObjectMap in the TriplesMap corresponding to the class defined in the domain of the property      | \\n| Object Property | Join PredicateObjectMap in the TriplesMap corresponding to the class defined in the domain of the property and where the parentTriplesMap is the TriplesMap corresponding to the class defined in the range of the property | \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/geo-agreement> \"This repository is focused on developing software and techniques to be used to integrated open geospatial data sources (e.g., OpenStreetMap, public open data, crowdsourced data, etc.) and understand and visualise the level of agreement\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-hive> \"WoT Hive is an implementation of a . This implementation  with the standard specification but aims at providing enriched features thanks to the usage of other W3C standards related to Semantic Web technologies. \\n**Checkout our ** \\n**Temporally the functionality that introduces the registration information in the Things has been disable (version 0.2.0 and above). Also, at the light of a new update in the  and therefore it is strongly recomended to disable this kind of validation.** \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Themis> \"This repository stores the code of Themis, which is a web application that provides an interface to execute tests on one or more ontologies, and also a REST API to be used by third-party services.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/corpuser> \"Aplicación de gestión de corpus documentales, desarrollada como Trabajo de Fin de Grado.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loom-ld> \"1. Build a set of HTML views to assist users for writing the link rules;\\n2. Compare the time required by our proposal for linking two datasets with Limes or Silk for the same datasets. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-type-graph> \"This piece of code is to be used to build type graphs. This is not meant to be used alone. \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/chowlk_spec> \"\\r\\n<b>Authors:</b>\\r\\nMaría Poveda-Villalón\\r\\nSerge Chávez-Feria\\r\\n\\r\\nRepository for the web of the Chowlk project, which provides an specification to create Ontology diagrams in <a href=\\\"https://www.draw.io/\\\">drawio</a>, and a service to generate the OWL code automatically from the diagram.\\r\\n\\r\\n<b>Visual notation:</b><br>\\r\\nhttps://chowlk.linkeddata.es/notation.html\\r\\n\\r\\n<b>Converter:</b><br>\\r\\nhttps://chowlk.linkeddata.es/index.html\\r\\n\\r\\n<b>Drawio library with the diagram blocks:</b><br>\\r\\n\\r\\n1. Complete diagrams.net template:\\r\\n\\r\\n    It contains all the building blocks specified in the visual notation.\\r\\n    https://github.com/oeg-upm/chowlk_spec/blob/master/resources/chowlk-library-complete.xml\\r\\n\\r\\n2. Library for Domain Experts:\\r\\n\\r\\n    It contains a subset of the specification for users less versed on the OWL language.\\r\\n    https://github.com/oeg-upm/chowlk_spec/blob/master/resources/chowlk-library-lightweight.xml\\r\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/corpuser> \"This project was generated with [Angular CLI](https://github.com/angular/angular-cli) version 7.0.4.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mappingpedia-engine-ws> \"Web service controller of mappingpedia-engine\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/dataeuropa-analysis> \"A large amount of datasets are now available in data.europa.eu, coming from public administrations all over Europe. Several studies have been conducted in the past to measure how these published open datasets are used and reused by organisations and individuals.  \\nThe work covered by the tools and scripts available in this repository focuses on measuring the presence of data.europa.eu content (in general, including datasets and data stories) in external platforms that are commonly used by communities of developers (e.g., StackOverflow, Reddit, GitHub). We aim at understanding how frequent these mentions are, as well as the context in which this content is mentioned. This task has required, in addition to a general search about data.europa.eu across the different platforms, the development of targeted search strategies to retrieve references to data.europa.eu's *datastories* and *datasets* in each of these sources. \\n\\n[//]: # (Repository for the analysis of data.europa.eu content presence in platforms commonly used by the software developer communities) \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [iQvoc](https://github.com/innoq/iqvoc) - SKOS(-XL) Vocabulary Management System for the Semantic Web.\\n- [Poolparty](https://www.poolparty.biz/skos-and-skos-xl) ($)\\n- [skosprovider](https://skosprovider.readthedocs.io/en/latest/intro.html) - Skosprovider provides an interface that can be included in an application to allow it to talk to different SKOS vocabularies. \\n- [skosshuttle](https://skosshuttle.ch/) ($)\\n- [atramhasis](https://github.com/OnroerendErfgoed/atramhasis)\\n- [ThManager](http://thmanager.sourceforge.net/)\\n- [protege skos editor](https://protegewiki.stanford.edu/wiki/SKOS_Editor)\\n- [skosmos](http://skosmos.org/)\\n- [Vocbench](http://vocbench.uniroma2.it/doc/user/skos_editing.jsf)\\n- [SKOS Play!](http://labs.sparna.fr/skos-play/about)\\n- [skosapi](http://skosapi.sourceforge.net/)\\n- [java-skos-api](https://github.com/simonjupp/java-skos-api)\\n- [askos](https://github.com/WileyLabs/askos) - A SKOS browser and editor.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-qq> \"\\nTADA (TAbular Data Annotator) for numeric column. It focuses on distributions to label numeric columns in tabular data. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"This decorator is used in the authentication system and is in charge of authenticating JWT tokens and grant or decline access to the service based on the obtained result.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mappingpedia-contents> \"Repository for storing mappings of mappingpedia\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-epw> \"The implementation of the code has been carried out using the [Python language](https://www.python.org/download/releases/3.0/) with the [Django library](https://www.djangoproject.com/), with which a rest service has been created with which all the tasks can be carried out in a simple way. Also, we have used [Helio](https://oeg-upm.github.io/helio/) and [RMLMapper](https://github.com/RMLio/rmlmapper-java) to achieve the transformation of the data by means of mappings to RDF format.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-adapters-ontology> \"This repository contains the code and related resources for the adapters AURORAL ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/soca> \"Software Catalog Creator. A repository that given an organization URL, it will create a software catalog for browsing all repositories\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"Semantic annotator for sign language ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Widoco> \"Contributions to address any of the current issues are welcome. In order to push your contribution, just **push your pull request to the develop branch**. The master branch has only the code associated to the latest release. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-renovation-process> \"Repository for the renovation process ontology.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OnToologyDnD> \"\\nThis work has been carried out by Adrián Mora Carrero during the final degree project: <<add link to oa.upm.es>>\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"**agora-py** is a Python library that supports *Web-scale Ontology-driven Access to Distributed Linked Data*. \\nCurrently, there is a huge number of *dereferenciable* Linked Data resources that are not being properly consumed neither explored.\\nThat is, the absolute majority of approaches for consuming Linked Data either ignore or inhibit such virtue, \\nunderutilizing the Web as a platform.  \\nAgora (agora-py) aims to be a tool that enables Linked Data consumers to live-query the Web of Data in a unified and explorative way:\\n* Driven by known vocabularies;\\n* Constrained by\\n   * the scope of the given query,\\n   * a set of seed resources whose URIs and types are known.\\n    \\nAlthough Agora is designed as well to be deployed as a microservices infrastructure, \\nit can be fully used as a Python middleware, without the need for any other external dependency than the Web of Data. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AI4EU_raidologist> \" 1. ***Sectioning Model***: A bidirectional LSTM, implemented via Pytorch, is used to section the input report. The employed sectioning model, alongside its functions, is implemented in the *section_model.py* file contained in the internal_functions folder. In this default model, four sections are considered: **Findings, Comparison, Indication and Impression**\\n 2. ***Scoring Model***: A combination of the aforementioned SciSpacy embedding model with a Random Forest Classifier is employed  to score each report as valid (1) or rejected (0).\\n 3. ***Abbreviation Disambiguation***: The aforementioned SciSpacy English pretrained model is used to extract existing abbreviations. SNOMED's query service is then used to obtain potential disambiguations for each of the detected abbreviations.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oeg-software-graph> \"Creation of a knowledge graph containing the catalog of software from the oeg-upm organization in GitHub\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor-incibe> \"and put /cvalue or /ttfidf in the Request-Handler \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-epw> \"In the [openweathermap folder](./Examples/openweathermap) we can see an example of a [json file](./Examples/openweathermap/Europe-Madrid(40.4196_-3.692).json), with which the corresponding [RDF files](./RDF_Examples/openweathermap/Europe-Madrid(40.4196_-3.692).ttl) have been obtained by means of declarative mappings. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"This file contains the descriptions of the docker image that is used to create the container.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/devops-infra> \"The DevOps infrastructure ontology network describes the domain of DevOps infrastructure. It is published at http://w3id.org/devops-infra\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-streams> \"sparql-stream sensor queries\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LOT-resources> \"If you want to use these figures, please follow the guidelines:\\n* If you want to use the figures without modifications:\\n    * Use the figure with the current license and authors (Ontology Engineering Group).\\n    * Let us know in which project you are using them so that we can keep track of their adoption. We will appreacite it.\\n* If you need to adapt the figures and methodology to your particular use case:\\n    1. Fork the LOT repository (https://github.com/oeg-upm/LOT-resources) \\n    2.  Keep the CC-BY-NC-SA licence with the statement \\\"Derived from LOT Methodology http://lot.linkeddata.es @Ontology Engineering Group\\\".\\n    3. Upload the figure to a new folder in the forked repository with the name of the project and generate a pull request to the LOT repository so that we can keep track up adoption and variations to keep evolving the methodology. \\n\\nThis work is licensed under a\\n[Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa]. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Devos> \"A tool to generate a gist of the ontology\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lubm4obda> \"The queries are available in the **[queries](https://github.com/oeg-upm/lubm4obda/tree/main/queries)** directory of this GitHub repository. Keep in mind that **original** mappings should be used for **queries 1-14**. There are three different versions of **queries 15-18**, one for each meta knowledge approach (standard reification, singleton property or RDF-star), with each approach having its corresponding mapping.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [trinity](https://bitbucket.org/semiodesk/trinity) - An application development platform for Microsoft .NET and Mono. It allows to easily build Linked Data and Semantic Web applications based on RDF.\\n- [Wings](https://github.com/IKCAP/wings) - A workflow system.\\n- [rww-play](https://github.com/read-write-web/rww-play) - An implementation in Play of a number of tools to build a Read-Write-Web server using Play2.x and akka.\\n- [prissma-studio](https://github.com/lukostaz/prissma-studio) - PRISSMA Studio: a web application to create Prisms, context-aware presentation metadata for Linked Data visualization.\\n- [lodspeakr](https://github.com/alangrafu/lodspeakr) - Framework to create Linked Data-based applications.\\n- [comunica](https://github.com/comunica/comunica) - Flexible meta query engine for the Web.\\n- [imagesnippets](http://www.imagesnippets.com/) - ImageSnippets is a complete metadata editing interface that enables someone who knows little to nothing about RDF, OWL, ontologies, or even URIs to create descriptions for images using Linked Data which is written in RDF.\\n- [Linked Data Reactor (LD-R)](http://ld-r.org) - A full-stack platform for building adaptive component-based Linked Data applications in NodeJS and React.\\n- [LDIF](https://github.com/wbsg/ldif) - Linked Data Integration Framework.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/dataeuropa-analysis> \"In this process, the data (e.g., posts) containing the link data.europa.eu is downloaded and stored. The format is different for each platform. There is a data collection module for each platform to fetch and prepare the data for the analysis phase.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-senML> \"This repository contains the code and documentation generated for the Sensor Data ontology which is available at:\\r\\nhttp://bimerr.iot.linkeddata.es/def/sensor-data\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-adapters-ontology> \"This repository contains the code and related resources for the adapters AURORAL ontology   \\n--------- ----------- The Devices of the adapters ontology ------------ \\n--------- ----------- The properties of the adapters ontology ------------ \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-core-ontology> \"This repository contains the code and documentation generated for the the AURORAL Core ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Horned OWL](https://github.com/phillord/horned-owl) - Horned-OWL is a library for manipulating OWL data.\\n- [Oxigraph](https://github.com/oxigraph/oxigraph) - Oxigraph is a graph database implementing the SPARQL standard.\\n- [sophia_rs](https://github.com/pchampin/sophia_rs) - Sophia: a Rust toolkit for RDF and Linked Data.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/MIRROR> \"MIRROR (MappIngs for Rdb to Rdf generatOR) is a tool developed by the OEG for generating R2RML mapping automatically from an RDB schema.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Devos> \"* Generate summary diagrams.\\n  * Allows the user to use the summarisation technique (e.g., using meta data, class frequency, or label length)\\n* Enrich ontologies with labels for the classes that are missing them.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vkg-tutorial-eswc2019> \"Material for VKG2019 tutorial at ESWC2019\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-hdt-numeric> \"This application annotate numeric columns in tabular data with properties from HDT-compressed knowledge bases.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/dataeuropa-analysis> \"The results of these analyses are expected to change over time. Hence, the new version of the data would need to be downloaded. Without any change in the code, copy the content of the data folder somewhere else and then delete the JSON files. Note that the data collection from GitHub would take much more time to process.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/SmartDeveloperHub.github.io> \"Site for the Smart Developer Hub project \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"The format of the output of the query. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/yatter> \"`-f R2RML` is an optional parameter for translating input YARRRML to R2RML (and inverse)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/fcm-cpp> \"* The use of Eign library for matrices instead of fix n-dimentional arrays which are common in prototyping fuzzy c-means library.\\n* The use of dynamic sizes for matrices.\\n* Make the FCM as a class, with all the related variables inside, so no need for Global variables and it also allow having multiple instances of FCM.\\n* Add automated tests with googletest.\\n* Add coverage for the tests.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/widaug> \"The core of the experiment performed for the SEPLN paper is provided in the folder experiments/sepln2022  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [RDFlib](https://github.com/RDFLib/rdflib) - Pythonic RDF API.\\n- [SPARQLWrapper](https://github.com/RDFLib/sparqlwrapper) - A wrapper for a remote SPARQL endpoint.\\n- [sparql-client](https://github.com/eea/sparql-client) - Python API to query a SPARQL endpoint.\\n- [RdfAlchemy](https://github.com/gjhiggins/RDFAlchemy)\\n- [Fuxi](http://code.google.com/p/fuxi/) - Bi-directional logical reasoning system for the semantic web.\\n- [PyShEx](https://github.com/hsolbrig/PyShEx) - ShEx interpreter for ShEx 2.0.\\n- [ORDF](http://ordf.org)\\n- [Django-rdf](http://code.google.com/p/django-rdf/) - An RDF engine for Django projects.\\n- [Djubby](https://github.com/wikier/djubby) - Linked Data frontend for SPARQL endpoints for Django.\\n- [SuRF](http://packages.python.org/SuRF/)\\n- [sparta](https://github.com/mnot/sparta/) - Simple API for RDF.\\n- [rdftools](https://github.com/cosminbasca/rdftools) - Simple collection of python RDF tools.\\n- [cysparql](https://github.com/cosminbasca/cysparql) - CySparql is a python wrapper over the excellent rasqal RDF library for parsing SPARQL queries.\\n- [pyHDT](https://github.com/Callidon/pyHDT) - Read and query [HDT documents](http://www.rdfhdt.org/) with ease in Python\\n- [calamus](https://github.com/SwissDataScienceCenter/calamus) - JSON-LD Serialization Library for Python based on Marshmallow\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OnToology-view-mock> \"This is a mock used for the development of the new OnToology view. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/geo.linkeddata.es-TripleGeoKettle> \"Repository where the integration of TripleGeo and GeoKettle is performed, used in the [geo.linkeddata.es](https://github.com/oeg-upm/geo.linkeddata.es-termite) project.  \\nThe documentation of the project geo.linkeddata.es-TripleGeoKettle is in the [wiki](https://github.com/oeg-upm/geo.linkeddata.es-TripleGeoKettle/wiki).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-information-objects> \"Repository for the Information Objects Ontology\\r\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-wot> \"A Python-based framework for describing, discovering and accessing the Web of Things\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [any2rdf](https://github.com/esbranson/any2rdf)\\n- [triplify](https://github.com/pebbie/triplify)\\n- [pyrdb2rdf](https://github.com/nisavid/pyrdb2rdf)\\n- [xsparql](https://www.w3.org/Submission/xsparql-language-specification/)\\n- [Karma](https://github.com/usc-isi-i2/Web-Karma) - Transform data expressed in multiple data formats into RDF.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TINTO> \"- Supports all CSV data in **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\\n- For now, the algorithm converts tabular data for binary and multi-class classification problems into machine learning.\\n- Input data formats:\\n    - **Tabular files**: The input data must be in **[CSV](https://en.wikipedia.org/wiki/Comma-separated_values)**, taking into account the **[Tidy Data](https://www.jstatsoft.org/article/view/v059i10)** format.\\n    - **Tidy Data**: The **target** (variable to be predicted) should be set as the last column of the dataset. Therefore, the first columns will be the features.\\n    - All data must be in numerical form. TINTO does not accept data in string or any other non-numeric format.\\n- Two dimensionality reduction algorithms are used in image creation, **[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)** and **[*t*-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)** from the Scikit-learn Python library.\\n- The synthetic images to be created will be in black and white, i.e. in 1 channel.\\n- The synthetic image **dimensions** can be set as a parameter when creating them.\\n- The synthetic images can be created using **characteristic pixels** or **blurring** painting technique (expressing an overlap of pixels as the **maximum** or **average**).\\n- Runs on **Linux**, **Windows** and **macOS** systems.\\n- Compatible with **[Python](https://www.python.org/)** 3.7 or higher.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lynx-py> \"Library for accessing and consume services developed in the European Project Lynx\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OEG-tutorial-template> \"Website for the tutorial Knowledge Graph Construction using Declarative Mapping Rules at ISWC 2020\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/pytada-hdt-entity> \"A python library binding of the c++ library tada-hdt-entity\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/devops-infra> \"Public repository for an ontology network on DevOps infrastructure\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-facility-ontology> \"This repository contains the code and documentation generated for the COGITO Facility ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-reduce-combine> \"This belongs to the project [tada-gam](https://github.com/oeg-upm/tada-gam). This expects Lc scores of types and type hierarchy of different slices of columns and it will combine them and compute the likelihood of the class of the given slices of a column. \\n*Note: there is only one database that is used for both, testing and live.*\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"[![Version](https://img.shields.io/badge/Version-0.2.2-orange)](https://github.com/oeg-upm/wot-jtd/releases)] [![Maven Central](https://img.shields.io/badge/Maven%20Central-v0.2.2-green)](https://search.maven.org/search?q=g:%22es.upm.fi.oeg%22%20AND%20a:%22wot-jtd%22) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![GitHub stars](https://img.shields.io/github/stars/Naereen/StrapDown.js.svg?style=social&label=Star&maxAge=2592000)](https://github.com/oeg-upm/wot-jtd/stargazers)\\n\\nThe JDT is an ORM implementation of the current [Thing Description](https://www.w3.org/TR/wot-thing-description/) model standardised by the [W3C Web of Things group](https://www.w3.org/WoT/). The current features are:\\n * Serialise:\\n\t * Serialise any Thing Description as a Java Object, i.e., a JDT\\n\t * Serialise a JDT from a JSON-LD framed document\\n\t * Serialise a JDT from a set of RDF triples\\n * Round trip-translation:\\n\t * Translate from a JSON-LD framed document into a set of equivalent RDF triples\\n\t * Translate a set of RDF triples into its equivalent JSON-LD framed document\\n * Validation \\n\t * Validate a JTD using [SHACL shapes](https://www.w3.org/TR/shacl/)\\n\t * Validate a JTD using [JSON schema](https://json-schema.org/)  **(coming soon)**\\n\t * Validate a JTD according to the [restrictions specified in the standard](https://www.w3.org/TR/wot-thing-description/)  **(coming soon)**\\n\\n\\nIf you have any feedback or feature suggestion, please let us know posting an issue with the label <span style=\\\"color:#EFA914\\\">**feedback**</span>\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OnToologyDnD> \"Source code for OnToology Drag&Drop\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lubm4obda> \"**The LUBM4OBDA Benchmark** is an extension of the popular **** to evaluate Ontology-Based Data Access (OBDA) engines over relational databases. In addition, LUBM4OBDA considers meta knowledge (also called reification or statement-level metadata) benchmarking. The main characteristics of LUBM4OBDA are: \\n- SQL data dumps for **** and ****.\\n- Data generator for custom scaling factors.\\n- Original **** (queries 1-14).\\n- Meta knowledge query set for ,  and  (queries 15-18).\\n- **** and **** mappings. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loom-ld> \"Develop a web service that allows writing and running these SPARQL-based link rules\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/terminology-extractor> \"and put /cvalue or /ttfidf in the Request-Handler\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/TINTOlib-Documentation> \"**TINTOlib** is a state-of-the-art library that wraps the most important techniques for the construction of **Synthetic Images** from [Sorted Data](https://www.jstatsoft.org/article/view/v059i10) (also known as **Tabular Data**). \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [SPARQL2Git](https://github.com/albertmeronyo/SPARQL2Git) - Easily store and curate SPARQL queries (and their associated Linked Data APIs) in GitHub.\\n- [sparql-transformer](https://github.com/D2KLab/sparql-transformer) - A generic JSON-LD transformer.\\n- [sparqlab](https://github.com/jindrichmynarz/sparqlab) - Lab for exercising SPARQL.\\n- [SNORQL](https://github.com/kurtjx/SNORQL) - Ajaxy front-end for exploring triple stores.\\n- [d3-sparql](https://github.com/zazuko/d3-sparql/) - Query a SPARQL endpoint with a SELECT query and get the data ready to be used with d3js\\n- [d3sparql](https://github.com/ktym/d3sparql) - JavaScript library for executing SPARQL query and transforming resulted JSON for visualization in D3.js.\\n- [jdbc4sparql](https://github.com/Claudenw/jdbc4sparql) - A JDBC driver that takes data from SPARQL endpoints or RDF graphs.\\n- [odata2sparql](https://github.com/peterjohnlawrence/com.inova8.odata2sparql.v4) - An OData proxy server that takes data from SPARQL endpoints or RDF graphs and publishes as OData V4 endpoint.\\n- [lens2odata](https://github.com/peterjohnlawrence/com.inova8.lens.framework.v4) - A GUI for discovery, search, and graph of RDF sources.\\n- [sparql2xquery](https://github.com/marklogic/sparql2xquery) - SPARQL to XQuery Translator for use with MarkLogic Semantic Toolkit.\\n- [SparqlAnalytics](https://github.com/AKSW/SparqlAnalytics)\\n- [decentsparql](https://github.com/itm/decentsparql)\\n- [sparqled](https://github.com/sindice/sparqled)\\n- [SPARQL2NL](https://github.com/AKSW/SPARQL2NL)\\n- [sparql-proxy](https://github.com/clarkparsia/sparql-proxy)\\n- [AutoSPARQL](https://github.com/AKSW/AutoSPARQL)\\n- [YASGUI](https://github.com/OpenTriply/YASGUI) - Yet Another Sparql GUI.\\n- [YASGUI.legacy](https://github.com/OpenTriply/YASGUI.legacy)\\n- [pubby](https://github.com/cygri/pubby) - A Linked Data frontend for SPARQL endpoints.\\n- [fluent-sparql](https://github.com/stoewer/fluent-sparql)\\n- [FlintSparqlEditor](https://github.com/TSO-Openup/FlintSparqlEditor)\\n- [reactive-sparql](https://github.com/modelfabric/reactive-sparql)\\n- [sparql-transformer](https://github.com/D2KLab/sparql-transformer)\\n- [spanqit](https://github.com/anqit/spanqit) - Java-based SPARQL query generator.\\n- [squebi](https://github.com/tkurz/squebi) - Squebi is a SPARQL editor and SPARQL result visualizer.\\n- [zeppelin-sparql](https://github.com/nick-manasys/zeppelin-sparql) - Zeppelin sparql interpreter.\\n- [SAFE](https://github.com/yasarkhangithub/SAFE)\\n- [Sparql-cli](https://github.com/lambdamusic/Sparql-cli) - Command line API for SPARQL.\\n- [snap-sparql-query](https://github.com/protegeproject/snap-sparql-query)\\n- [Trifid](https://github.com/zazuko/trifid) - Lightweight Linked Data Server and Proxy\\n- [asqc](https://github.com/gklyne/asqc) - SPARQL query client (pronounced \\\"ask\\\").\\n- [SPARQL-parser](https://github.com/tenforce/SPARQL-parser)\\n- [antlr-sparql-grammar](https://github.com/rollxx/antlr-sparql-grammar)\\n- [visu](https://github.com/jiemakel/visu) - Visual SPARQL query tool.\\n- [Porthole](https://itunes.apple.com/us/app/porthole/id984035787) - Mac SPARQL editor and client.\\n- [datastudio-sparql-connector](https://github.com/DataFabricRus/datastudio-sparql-connector) - SPARQL Connector for Google Data Studio.\\n- [QLever](https://github.com/ad-freiburg/QLever) - Highly efficient query engine for SPARQL+Text.\\n- [sage-engine](https://github.com/sage-org/sage-engine) - a SPARQL query engine for public Linked Data providers. \\n- [SEPA](https://github.com/arces-wot/SEPA) - A JAVA implementation of the SPARQL Event Processing Architecture including the engine, APIs and tools.\\n- [Processor](https://github.com/AtomGraph/Processor) - Ontology-driven Linked Data processor and server for SPARQL backends.\\n- [LinkedDataHub](https://atomgraph.github.io/LinkedDataHub/) - SPARQL-powered Knowledge Graph management system.\\n- [SparqlBlocks](http://sparqlblocks.org/) - Build SPARQL queries with blocks\\n- [SparqlProg](https://github.com/cmungall/sparqlprog) - composable SPARQL using logic programming\\n- [Sparklis](https://github.com/sebferre/sparklis) - natural language query builder to explore and query endpoints with all the power of SPARQL yet without any knowledge of SPARQL.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/JSONPath-to-SPARQL> \"# JSONPath-to-SPARQL \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Morph-OME> \"This work was funded partially by EIT Digital under the WOODS project. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-qq> \"This work was funded partially by EIT Digital under the WOODS project. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LOT-resources> \"This repository contains the resources associated to LOT methodology that are not available online somewhere else.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-cs> \"Citizen science project to validate our AI algorithm \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ar2dtool-oegfork> \"The syntax of the simple parameters is: \\n- classShape, individualShape, and literalShape: shapes of each type of element of the graph. Allowed values are: recatangle (default), ellipse, triangle, and diamond (e.g. classShape=ellipse;). \\n\\n- classColor, individualColor, and literalColor: color of each type of element of the graph. Allowed values so far are: black (default), red, blue, green, orange, yellow (e.g. literalColor=blue;). \\n- nodeNameMode: defines the way the resources are named. You can specify 'fulluri' for using the URIs of each resource, 'prefix' for shortened names using the specified prefixes (resources with no mathcing prefix will be displayed with their full URI. By default the base prefix will be displayed as \\\"base:\\\", unless vann:preferredNamespacePrefix is specified, in which case its value will be used as base prefix), or 'localname' for using the localname of the resource. (e.g. nodeNameMode=localname;). \\nThe syntax of a list is: listName=[<elementA1, elementA2,..., elementA3><elementB1, elementB2,..., elementb3>...<elementZ1, elementZ2,..., elementZ3>]; \\nBasically a list is a set of N-tuples. In most cases you will only need 1 tuple for your whole list, but in some cases it will be necessary to include more than one (like in the case of specialElementsList or equivalentElementList parameters). \\n- equivalentElementList: each of the elements of a tuple will be replaced by the first element of it on the final diagram. \\nTwo sample configuration files are available in the /samples folder. One creates and ER diagram of the ontology. The other one depicts the taxonomy of the classes. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/OEG-tutorial-template> \"Based on the original work from David Chaves-Fraga and Ana Iglesias-Molina on the KGC2020 tutorial at ISWC2020\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [semantic-graphql](https://github.com/nelson-ai/semantic-graphql) - Create GraphQL schemas from RDF ontologies.\\n- [hypergraphql](https://github.com/semantic-integration/hypergraphql) - GraphQL interface for querying and serving linked data on the Web.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-kg> \"We include links to other existing Covid19 Knowledge Graphs in other to federated over them and enhance the completeness of our queries. The endpoints of the KGs currently included are: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Themis> \"Verification tool for executing test cases on one or more ontologies\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/mapeathor> \"Iglesias-Molina, A., Pozo-Gilo, L., Dona, D., Ruckhaus, E., Chaves-Fraga, D., & Corcho, O. (2020, January). *Mapeathor: Simplifying the Specification of Declarative Rules for Knowledge Graph Construction. In ISWC (Demos/Industry).* [Online version](http://ceur-ws.org/Vol-2721/paper488.pdf) \\nIglesias-Molina, A., Chaves-Fraga, D., Priyatna, F., & Corcho, O. (2019). Towards the Definition of a Language-Independent Mapping Template for Knowledge Graph Creation. *In Proceedings of the Third International Workshop on Capturing Scientific Knowledge co-located with the 10th International Conference on Knowledge Capture (K-CAP 2019)* (pp. 33-36). [Online version](https://sciknow.github.io/sciknow2019/papers/SciKnow_2019_paper_4.pdf)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/loupe-api> \"Source code of the Loupe API implementation. \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/epnoi> \"**epnoi** is an unified information access middleware whose objective is to increase the awareness and simplify the search of relevant content in situations where multiple heterogeneous non-structured information sources are involved.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/FAIR-Research-Object-API> \"FAIROs is a service to assess the FAIRness of Research Objects. The service offers a RESTFUL API built with the FLASK library for python. It receives a ZIP file with the ro-crate Research Object and return the assessment. Signing up to the service is a manual process managed locally by the service provider. Passwords are encrypted using the sha256 algorithm. However, the rest of the operations are available through the public API.\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [fred](http://wit.istc.cnr.it/stlab-tools/fred/#About) - a machine reader for the Semantic Web\\n- [NIF](https://persistence.uni-leipzig.org/nlp2rdf/) NLP Interchange Format\\n- [Lemon](https://lemon-model.net/) - The Lexicon Model for Ontologies\\n- [Wordnet](https://wordnet.princeton.edu/) \\n- [PreMOn](http://premon.fbk.eu/ontology/vn) - Predicate Model for Ontologies (PreMOn) - VerbNet ontology module\\n- [BabelNet](https://babelnet.org)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ckanext-federgob> \"Extension to ease the federation process of CKAN catalogs with the global catalog of Spain, [Datos.gob](http://www.datos.gob.es/).\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/dataeuropa-analysis> \"The analysis phase is platform agnostic. The different data collection modules will call the analysis functions.  \\nThere are also some additional analyses that are only feasible for certain platforms (platform-specific analysis). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/rdf-star-generation> \"Use cases for generating RDF-star from heterogeneous data sources\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-kg> \"Resources for the constructing and exploitation of the KG - Drugs4Covid. The construction of the KG followed a systematic and maintainable approach creating a set of RML and CSVW annotations and using SDM-RDFizer RML engine for transforming the original input data (from SOLR databases) in RDF.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/webODE> \"WebODE is an extensible ontology-engineering suite based on an application server, whose development started in 1999 and whose **support was discontinued in 2006**. The core of WebODE was its ontology access service, used by all the services and applications plugged into the server. The WebODE's Ontology Editor allowed editing and browsing WebODE ontologies, and was based on HTML forms and Java applets. \\nWebODE was built as a scalable, extensible, integrated workbench that covers and gave support to most of the activities involved in the ontology development process (conceptualization, reasoning, exchange, etc.) and supplied a comprehensive set of ontology related services that permit interoperation with other information systems. Among these services, the workbench integrated services for ontology language import and export (XML, RDF(S), OIL, DAML+OIL, OWL, CARIN, FLogic, Jess, Prolog), for axiom edition with WAB (WebODE Axiom Builder), for documentation, for evaluation, for evolution, for learning, for merge, and an inference engine. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"The account that created the pull request, it is taken from `${{ github.actor }}`.  \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-nlp> \"Automatic annotations of drugs and diseases in CORD-19 corpus\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"Project to generate RDF triples for annotated Sign Language videos \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/saref-ext> \"This repository contains the code and documentation generated for the SAREF extensions developed by OEG.\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-coppola> \"Coppola is a micro service for payload validation\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/website-geo> \"Official repository with all source code to generate https://geo.linkeddata.es\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"Project to annotate sign language videos (json files) with NLP tags and Babelnet synsets\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/dataeuropa-analysis> \"Repository for the analysis of data.europa.eu done in task 3.4\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Stardog Union](http://stardog.com) - Knowledge Graph Platform for the Enterprise.\\n- [Epimorphics](https://www.epimorphics.com/)\\n- [Franz](http://franz.com)\\n- [PoolParty](https://www.poolparty.biz/)\\n- [Cambridge Semantics](https://www.cambridgesemantics.com/)\\n- [Oxford Semantic Technologies](https://www.oxfordsemantic.tech/)\\n- [Capsenta](https://capsenta.com/)\\n- [Zazuko](https://zazuko.com/)\\n- [MarkLogic](https://www.marklogic.com/product/marklogic-database-overview/database-features/semantics/)\\n- [Oracle](https://www.oracle.com/technetwork/database/options/spatialandgraph/overview/rdfsemantic-graph-1902016.html)\\n- [OntoText](https://www.ontotext.com/)\\n- [TopQuadrant](https://www.topquadrant.com/)\\n- [OpenLinkSoftware](https://www.openlinksw.com/)\\n- [Cognitum](http://www.cognitum.eu/)\\n- [entryscape](https://entryscape.com)\\n- [zazuko](https://zazuko.com/)\\n- [inova8](http://www.inova8.com/)\\n- [in4mium](http://www.in4mium.com/)\\n- [Xylem Technologies](https://www.xylem-technologies.com/en/)\\n- [Synaptica](https://www.synaptica.com)\\n- [Ontola](http://ontola.io/)\\n- [eccenca Corporate Memory](https://www.eccenca.com) - build, explore and consume Knowledge Graphs\\n- [Semantic Arts](https://semanticarts.com) - Enterprise information systems based on flexible data structures and deep semantics.\\n- [Same4](http://www.seme4.com)\\n- [Derivo](https://www.derivo.de/en/home/)\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"These folders host the RO-Crate files sent by the users. When the files uploaded the service, they are hosted in the pending_jobs folder. When the enrichment core processes a request, it saves the enriched file in the done_jobs folder. The API then sends the enriched file to the user.\\nThese folders are emptied periodically by the deletion script.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wikidata-label-extractor> \"Using pipes we can download, decompress and filter the triples in a continuous flow of about 15MB/s, but that is still slow and takes about 12 hours. The bottleneck is the filtering part, that hits the processor single-thread limit. \\nAs a solution, a mechanism has been designed to partition the TTL file and thus parallelize the filtering of its triples with an arbitrary number of threads, at the user's discretion. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/DIoT> \"International Workshop on Discovery on the Internet of Things\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/saref-ext> \"This repository contains the code and documentation generated for the SAREF extensions developed by OEG. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-occupant-behavior> \"BIMERR ontology for Occupant Behavior data for energy consumption\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito_thing_manager_module> \"Thing Manager module for the COGITO project. \\n| Method | Headers                           | Endpoints               | Description                                                                                                                                                             | Parameters                                                                                                    |\\n| ------ | --------------------------------- | ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |\\n| POST   | Content-Type: application/json    | /project/{:id}          | Creates a new project, its respective triples and thing description                                                                                                     | id(mandatory), name(optional), description(optional)                                                          |\\n| PUT    | Content-Type: application/json    | /project/{:id}          | Updates an existing project, its respective triples and thing description                                                                                               | id(mandatory), name(optional), description(optional)                                                          |\\n| DELETE | N/A                               | /project/{:id}          | Deletes an existing project, its respective triples and thing description, and also the thing descriptions associated to it in cascade mode                             | id(mandatory)                                                                                                 |\\n| GET    | N/A                               | /project/{:id}          | Retrieves the thing description of an existing project                                                                                                                  | id(mandatory)                                                                                                 |\\n| POST   | Content-Type: application/json    | /project/{:id}/file     | Adds a file/files to an existing project, creates respective triples and thing descriptions                                                                             | id(mandatory), format_of_file(mandatory), type_of_file(mandatory), uri_of_file(mandatory), metadata(optional) |\\n| DELETE | N/A                               | /project/{:id}/file     | Deletes file from project and its respective triples and thing descriptions                                                                                             | id(mandatory), format_of_file(mandatory), type_of_file(mandatory), uri_of_file(mandatory), metadata(optional) |\\n| POST   | Content-Type: multipart-form/data | /project/{:id}/file/ttl | Retrieves from KGG the respective ttl file generated, saves it into the triple store and also generate respective thing descriptions for specific elements of the graph | id(mandatory), format_of_file(mandatory), type_of_file(mandatory), name_of_file(mandatory)                    | \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-shipmentBiomass-ontology> \"This repository contains the code and documentation generated for the  Auroral shipmentBiomass ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/auroral-biomass-ontology> \"This repository contains the code and documentation generated for the the AURORAL Biomass ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio> \"Helio is a framework that allows publishing RDF data as a Linked Data service, in which data comes from different heterogeneous sources. The translation of data into RDF can be performed using different tools from the literature, or the Helio materialisation engine. Additionally, the RDF produced is injected automatically in a triple store and can be versioned. Finally, Helio offers also the necessary components to publish the RDF at resource level, dataset level, and enables a SPARQL endpoint. Nevertheless, Helio also allow users to define dynamic views of the data and associate html views, that may have embedded RDF (RDFa).\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-map-score> \"Part of [tada-gam](https://github.com/oeg-upm/tada-gam) project. This is responsible for assigning scores to each type given a column and a knowledge graph \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/helio> \"Helio is a framework that allows publishing RDF data as a Linked Data service, in which data comes from different heterogeneous sources. The translation of data into RDF can be performed using different tools from the literature, or the Helio materialisation engine. Additionally, the RDF produced is injected automatically in a triple store and can be versioned. Finally, Helio offers also the necessary components to publish the RDF at resource level, dataset level, and enables a SPARQL endpoint. Nevertheless, Helio also allow users to define dynamic views of the data and associate html views that may have embedded RDF (RDFa). \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/morph-csv> \"Enhancing virtual KG access over tabular data with RML and CSVW\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-material-properties> \"BIMERR ontology for the material properties domain\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [n3pygments](https://github.com/gniezen/n3pygments) - Pygments lexer to perform syntax highlighting for N3, Turtle and SPARQL.\\n- [levelgraph-n3](https://github.com/levelgraph/levelgraph-n3) - LevelGraph plugin for storing N3/Turtle/RDF data.\\n- [psps](https://github.com/factsmission/psps) - Personal Structured Publishing Space.\\n- [swrlapi](https://github.com/protegeproject/swrlapi) - The SWRLAPI is a Java API for working with the OWL-based SWRL rule and SQWRL query languages. It includes graphical tools for editing and executing rules and queries.\\n- [cp-common-utils](https://github.com/mhgrove/cp-common-utils) - Collection of utilty classes from Clark & Parsia.\\n- [jena-joseki](https://github.com/tingletech/jena-joseki)\\n- [Git2PROV](https://github.com/mmlab/Git2PROV) - Unleash the potential of Git in the new W3C standard for provenance.\\n- [IntervalServer](https://github.com/epimorphics/IntervalServer)\\n- [jdbc-for-rdf3x](https://github.com/dbiir/jdbc-for-rdf3x)\\n- [rdf3x_path](https://github.com/agubichev/rdf3x_path) - RDF3X with path queries.\\n- [jqudt](https://github.com/egonw/jqudt) - Java library for working with the QUDT ontology and data using it.\\n- [JenaSecurity](https://github.com/Claudenw/JenaSecurity) - Security (Permissions) wrapper around Jena RDF implementation.\\n- [specgen](https://github.com/specgen/specgen) - Modified, extended and more generalized version of Danbri‘s SpecGen version 5.\\n- [cassa](https://github.com/heuer/cassa) - SPARQL 1.1 Graph Store HTTP Protocol implementation with plugable backends.\\n- [keygenapp](https://github.com/bblfish/keygenapp) - Utilities and WebApp for certificate creation within a browser, for FOAF+SSL.\\n- [owlapitools](https://github.com/owlcs/owlapitools) - Set of independent add-ons for OWL API.\\n- [LD-FusionTool](https://github.com/mifeet/LD-FusionTool) - Data Fusion & Conflict Resolution tool for Linked Data.\\n- [prefix.cc](https://github.com/cygri/prefix.cc) - Source code to the prefix.cc website.\\n- [LSD-Dimensions](https://github.com/albertmeronyo/LSD-Dimensions) - All dimension values of Linked Statistical Data.\\n- [prissma](https://github.com/lukostaz/prissma) - Context-Aware Adaptation for Linked Data.\\n- [fox-java](https://github.com/renespeck/fox-java) - Java bindings for FOX - Federated Knowledge Extraction Framework.\\n- [fox-py](https://github.com/earthquakesan/fox-py) - Python bindings for FOX - Federated Knowledge Extraction Framework.\\n- [fox-ui](https://github.com/Data2Semantics/fox-ui) - Web UI for FoxPSL.\\n- [ORE](https://github.com/AKSW/ORE) - Ontology Repair and Enrichment.\\n- [signal-collect-torque](https://github.com/uzh/signal-collect-torque) - Support for Signal/Collect Torque deployment.\\n- [redland-bindings](https://github.com/dajobe/redland-bindings) - Redland librdf language bindings.\\n- [mediation](https://github.com/correndo/mediation) - Jena based framework to implement ontological mediation of SPARQL queries.\\n- [owl-functional-syntax-axiom-parser](https://github.com/dfleischhacker/owl-functional-syntax-axiom-parser)\\n- [SemanticPingback](https://github.com/AKSW/SemanticPingback) - This small vocabulary defines resources which are used in the context of Semantic Pingback.\\n- [json-ld-macros](https://github.com/antoniogarrote/json-ld-macros)  Declarative transformation of JSON APIs into JSON-LD.\\n- [tac](https://github.com/magnetik/tac) - Triple access control.\\n- [dbpedia-extension](https://github.com/sparkica/dbpedia-extension)\\n- [grefine-rdf-extension](https://github.com/OpenRefine/grefine-rdf-extension) - An extension to Google Refine that enables graphical mapping of Google Refine project data to an RDF skeleton and then exporting it in RDF format.\\n- [LD-FusionTool](https://github.com/mifeet/LD-FusionTool) - Data Fusion & Conflict Resolution tool for Linked Data.\\n- [xodx](https://github.com/AKSW/xodx) - An implementation of Semantic Pingback and PuSH for a DSSN.\\n- [morph-starter](https://github.com/jpcik/morph-starter) - this project is a simple Java (and Scala) demo of how to use morph.\\n- [sesametools](https://github.com/joshsh/sesametools) - A collection of utilities for use with OpenRDF Sesame (as of recently, Eclipse RDF4J).\\n- [DEER](https://github.com/dice-group/DEER) - RDF Dataset Enrichment Framework.\\n- [levelgraph-jsonld](https://github.com/mcollina/levelgraph-jsonld) - The Object Document Mapper for LevelGraph based on JSON-LD\\n- [OWL-API](https://github.com/owlcs/owlapi) - The OWL API is a Java API for creating, manipulating and serialising OWL Ontologies.\\n- [ONT-API](https://github.com/owlcs/ont-api) - a Jena based OWL-API implementation.\\n- [LODGrefine](https://github.com/sparkica/LODGrefine) - LOD-enabled Google Refine: linked open data related extensions included.\\n- [stardog.js](https://github.com/stardog-union/stardog.js)\\n- [stardog-groovy](https://github.com/stardog-union/stardog-groovy)\\n- [stardog-ubuntu-scripts](https://github.com/semantalytics/stardog-ubuntu-scripts)\\n- [hydra-java](https://github.com/dschulten/hydra-java)\\n- [HydraClient](https://github.com/lanthaler/HydraClient)\\n- [cp-openrdf-utils](https://github.com/mhgrove/cp-openrdf-utils) - Utility classes for working with the OpenRdf API.\\n- [linked-csv](https://github.com/JeniT/linked-csv) - A souped-up CSV-based data format.\\n- [balloon](https://github.com/schlegel/balloon) - A tool-suite for Linked Data consumption. balloon aims in offering public services and tools to take advantage of the semantic web with less effort. The basic motivation is to establish a foundation for Linked Data as a Service (LDaaS).\\n- [grlc](https://github.com/CLARIAH/grlc) - Translates public SPARQL queries into Linked Data APIs automatically.\\n- [basil](https://github.com/the-open-university/basil) - Building Apis SImpLy from sparql endpoints.\\n- [lodmill](https://github.com/lobid/lodmill) - Blend, grind, and enjoy LOD – fresh from the mill!\\n- [module-extractor](https://github.com/rsgoncalves/module-extractor) - Java-based module extractor for OWL ontologies.\\n- [iRap](https://github.com/EIS-Bonn/iRap) - iRap - Interest-based RDF update propagation framework.\\n- [turtle-in-html](https://github.com/alangrafu/turtle-in-html) - Bookmark to visualize RDF embedded in HTML as Turtle.\\n- [linked-csv-browser](https://github.com/theodi/linked-csv-browser)\\n- [semargl](https://github.com/semarglproject/semargl) - Highly performant, lightweight framework for linked data processing. Supports RDFa, JSON-LD, RDF/XML and plain text formats, runs on Android and GAE, provides integration with Jena, Sesame and Clerezza.\\n- [wordnet-lemon-to-w3c](https://github.com/jimregan/wordnet-lemon-to-w3c)\\n- [owlconvert](https://github.com/camwebb/owlconvert) - Simple OWL format converter based on OWLAPI.\\n- [rabel](https://github.com/linkeddata/rabel) - Program for reading and writing linked data in various formats.\\n- [csvw-template](https://github.com/edsu/csvw-template) - Document the semantics of your csv file.\\n- [jsonld-java](https://github.com/jsonld-java/jsonld-java) - JSON-LD implementation for Java.\\n- [Luzzu](https://github.com/Luzzu/Framework/) - A scalable and extensible Linked Data quality assessment framework.\\n- [odmtp-tpf](https://github.com/benjimor/odmtp-tpf) - Triple pattern matching over non-RDF datasources with inference .\\n- [motools](https://github.com/motools) - Music ontology tools\\n- [activitypub](https://www.w3.org/TR/2018/REC-activitypub-20180123/)\\n- [ont-api](https://github.com/avicomp/ont-api)\\n- [rdfsurveyor](https://github.com/guiveg/rdfsurveyor) - Exploration tool for RDF datasets.\\n- [fenster](https://github.com/knakk/fenster) - RDF quad-store frontend.\\n- [jsonld-streaming-parser.js](https://github.com/rubensworks/jsonld-streaming-parser.js) - A fast and lightweight streaming JSON-LD parser for JavaScript. \\n- [rollxx/manchester-syntax-owl2](https://github.com/rollxx/manchester-syntax-owl2) - ANTLR grammar for simplified Manchester Syntax OWL2. \\n- [rollxx/antlr-sparql-grammar](https://github.com/rollxx/antlr-sparql-grammar) - sparql 1.1 antlr grammar.\\n- [sparql-ld](https://github.com/fafalios/sparql-ld) - SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data.\\n- [jena-sparql-api](https://github.com/SmartDataAnalytics/jena-sparql-api) - A collection of Jena-extensions for hiding SPARQL-complexity from the application layer.\\n- [nichtich/wdg](https://github.com/nichtich/wdq) - Command line interface to Wikidata Query Service.\\n- [vocol](https://github.com/vocol/vocol) - An integrated environment to support collaborative ontology / vocabulary development in distributed settings.\\n- [psps](https://github.com/factsmission/psps) - Personal Structured Publishing Space.\\n- [CSO](https://cso.kmi.open.ac.uk/home) - The Computer Science Ontology (CSO) is a large-scale ontology of research areas that was automatically generated using the Klink-2 algorithm on the Rexplore dataset, which consists of about 16 million publications, mainly in the field of Computer Science.\\n- [metreeca](https://github.com/metreeca) - The model-driven linked data platform.\\n- [OLGA](https://ecostruxure.github.io/OLGA/) - OLGA (Ontology Library GenerAtor) is a generic tool aiming to accelerate the adoption of Standard W3C Semantic technology among developers.\\n- [Glimmer](https://github.com/Timpy/Glimmer) - An RDF Search Engine.\\n- [ontodia](https://github.com/sputniq-space/ontodia) - Ontodia data diagraming library.\\n- [rdf.sh](https://github.com/seebi/rdf.sh) - A multi-tool shell script for doing Semantic Web jobs on the command line.\\n- https://metacpan.org/release/KJETILK/AtteanX-Store-SPARQL-0.012\\n- https://github.com/stkenny/grefine-rdf-extension/releases/tag/v1.1.0\\n- http://www.linklion.org/portal/\\n- [yarrrml-parser](https://github.com/RMLio/yarrrml-parser) - A YARRRML parser library and CLI in Javascript.\\n- http://rml.io/yarrrml/\\n- [amazon-neptune-tools](https://github.com/awslabs/amazon-neptune-tools) - Tools and utilities to enable loading data and building graph applications with Amazon Neptune.\\n- [sparql-ld](https://github.com/fafalios/sparql-ld) - SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data.\\n- [genealogical-trees](https://github.com/blokhin/genealogical-trees) - Semantic Web Exercise: Reasoning and Visualization of the Genealogical Ontologies.\\n- http://sage.univ-nantes.fr\\n- [LodLive](https://github.com/dvcama/LodLive) - browse the web of data - a SPARQL navigator http://lodlive.it\\n- [fbrs](https://github.com/aldonline/fbrs) - Facebook RDF Sync\\n- [ostrich](https://github.com/rdfostrich/ostrich) -Versioned RDF triple store (Offset-enabled TRIple store for CHangesets)\\n- [ontmalizer](https://github.com/srdc/ontmalizer) - A tool that performs comprehensive transformations of XML Schemas (XSD) and XML data to RDF/OWL automatically\\n- [NSpM](https://github.com/AKSW/NSpM) - robot Neural SPARQL Machines translate natural language into SPARQL queries. \\n- [ML-Schema/core](https://github.com/ML-Schema/core) - CORE ontology of ML-Schema schema. It's the mapping to others machine learning vocabularies and ontologies (DMOP, Expose, OntoDM and MEX)\\n- [rocker](https://github.com/AKSW/rocker) - key A Refinement Operator Approach for Key Discovery. http://aksw.org/projects/Rocker\\n- [Mandolin](https://github.com/mommi84/Mandolin) - sparkle Markov Logic Networks for the Discovery of Links\\n- [docker2rdf](https://github.com/albertmeronyo/docker2rdf) - Mapper to represent Dockerfiles as RDF triples\\n- [vsb](https://github.com/leipert/vsb) - Visual SPARQL Builder - Model SPARQL-Select-Queries in a browser https://leipert.github.io/vsb/\\n- [sparti](https://github.com/amgadmadkour/sparti) - SPARTI - RDF Semantic Partitioning.\\n- [carml-cli](https://github.com/netage/carml-cli) - Interface for CARML library.\\n- [OME](https://github.com/oeg-upm/OME) - Online Mapping Editor.\\n- [Linked-Data-Studio](https://github.com/architolk/Linked-Data-Studio) - The Linked Data Studio is an extension to the Linked Data Theatre for the creation of Linked Data.\\n- [canonical_rdf](https://github.com/iherman/canonical_rdf) - Proof-of-concept implementation of Aidan Hogan's RDF canonicalization algorithm in node.js.\\n- [Linked-Data-Theatre](https://github.com/architolk/Linked-Data-Theatre) - The Linked Data Theatre is a platform for an optimal presentation of Linked Data.\\n- [SEPA](https://github.com/arces-wot/SEPA) - A JAVA implementation of the SPARQL Event Processing Architecture including the engine, APIs and tools.\\n- [GLEEN](https://github.com/RENCI-NRIG/gleen) Regular Paths for ARQ SparQL.\\n- [psparql](http://exmo.inrialpes.fr/software/psparql/) - PSPARQL (for Path SPARQL) is a query language for RDF.\\n- [rdf2h](https://github.com/rdf2h/rdf2h) - Render resources described in RDF using logicless templates.\\n- [Beast](https://github.com/SmartDataAnalytics/Beast) - Benchmarking, Evaluation, and Analysis Stack - A powerful yet lightweight Java8/Jena-based RDF processing stack.\\n- [basil](https://github.com/the-open-university/basil) - Building Apis SImpLy from sparql endpoints.\\n- [profilechecker](https://github.com/stain/profilechecker) - OWL API profile checker. \\n- [spdx](https://spdx.org/specifications) - Software Package Data Exchange® (SPDX®) is an open standard for communicating software bill of material information (including components, licenses, copyrights, and security references).\\n- [CostFed](https://github.com/dice-group/CostFed) - Cost-Based Query Optimization for SPARQL Endpoint Federation.\\n- [sparql-ld](https://github.com/fafalios/sparql-ld) - SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data.\\n- [vocol](https://github.com/vocol/vocol) - An integrated environment to support collaborative ontology / vocabulary development in distributed settings.\\n- [sparql-to-csv](https://github.com/jindrichmynarz/sparql-to-csv) - Stream SPARQL results to CSV .\\n- [knowledgecubes](https://github.com/amgadmadkour/knowledgecubes) - Efficient RDF Data Management over Spark.\\n- [premon](https://premon.fbk.eu) - PREdicate Model for ONtologies\\n- [eso-and-ceo](https://github.com/newsreader/eso-and-ceo) - Event and Implied Situation Ontology (ESO) and the Circumstantial Event Ontology for Calamities (CEO).\\n- [pikes](http://pikes.fbk.eu) - Pikes is a Knowledge Extraction Suite \\n- [rdfpro](https://github.com/dkmfbk/rdfpro) - an extensible tool for building stream-oriented RDF processing pipelines.\\n- [umls2rdf](https://github.com/ncbo/umls2rdf) - These python scripts connect to the Unified Medical Language System (UMLS) database and translate the ontologies into RDF/OWL files. This is part of the BioPortal project.\\n- [robot](http://robot.obolibrary.org/) - ROBOT is a command line tool for working with Open Biomedical Ontologies\\n- [SEPA](https://github.com/arces-wot/SEPA) - A JAVA implementation of the SPARQL Event Processing Architecture including the engine, APIs and tools. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-occupant-behavior> \"This repository contains the code and documentation generated for the Occupancy Profile ontology which is available at:\\r\\nhttp://bimerr.iot.linkeddata.es/def/occupancy-profile\\r\\n\\r \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/Ethereum-Smart-Contract-Downloader> \"This software is part of the [SANCUS platform](https://github.com/oeg-upm/sancus), actually on development.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [Akutan](https://github.com/eBay/akutan) - (OS) A distributed knowledge graph store written in Golang. Formerly known as Beam.\\n- [Jena TDB](http://jena.apache.org/documentation/tdb/index.html) - (OS).\\n- [Ontotext GraphDB™](http://graphdb.ontotext.com/) - ($/F).\\n- [Halyard](https://github.com/Merck/Halyard) - (OS).\\n- [Stardog](http://stardog.com) - ($/F).\\n- [Strabon](http://www.strabon.di.uoa.gr/) - (OS) A spatiotemporal RDF store.\\n- [Systap Blazegraph™](https://www.blazegraph.com/) - ($/OS).\\n  - [docker-blazegraph](https://github.com/zorino/docker-blazegraph)\\n  - [blazegraph-samples](https://github.com/blazegraph/blazegraph-samples)\\n  - [docker-blazegraph](https://github.com/lyrasis/docker-blazegraph)\\n  - [blazegraph-service](https://github.com/vastix/blazegraph-service)\\n- [Marklogic](https://github.com/marklogic/semantic) - ($).\\n- [Virtuoso](https://virtuoso.openlinksw.com/) - ($/OS).\\n  - [virtuoso-opensource](https://github.com/openlink/virtuoso-opensource)\\n- [Oracle](http://www.oracle.com/technetwork/database/options/spatialandgraph/overview/rdfsemantic-more-2239071.html) - ($).\\n- [Allegrograph](http://franz.com/agraph/allegrograph/) - ($/F).\\n- [BrightstarDB](http://brightstardb.com/) - (OS) A native RDF database for the .NET platform written in C#.\\n- [CM-Well](https://github.com/thomsonreuters/CM-Well) - (OS).\\n- [Apache Rya](http://rya.incubator.apache.org/) - (OS).\\n- [4Store](https://github.com/garlik/4store) - (OS).\\n- [Mulgara](http://mulgara.org/) - (OS).\\n- [Parliament](https://github.com/SemWebCentral/parliament) - (OS).\\n- [SANSA](http://sansa-stack.net/) - (OS).\\n- [hbase-rdf](https://github.com/castagna/hbase-rdf) - (OS).\\n- [Anzograph](https://www.cambridgesemantics.com/product/anzograph/)\\n- [CumulusRDF](https://github.com/cumulusrdf/cumulusrdf)\\n- [Sempala](https://github.com/aschaetzle/Sempala)\\n- [TriplePlace](https://github.com/white-gecko/TriplePlace) - Light weight and flexible Triple Store for Android.\\n- [Node-Quadstore](https://beautifulinteractions.github.io/node-quadstore/) - (OS) A LevelDB-backed graph database for Node.js supporting quads, SPARQL queries and the RDF/JS interface.\\n- [KGRAM](http://wimmics.inria.fr/corese) - (OS).\\n- [luposdate](https://github.com/luposdate/luposdate) - (OS) Semantic Web database.\\n- [wallix/triplestore](https://github.com/wallix/triplestore) - (OS) Nifty library to manage, query and store RDF triples.\\n- [levelgraph](https://github.com/levelgraph/levelgraph) - (OS) Graph database JS style for Node.js and the Browser.\\n- [Oxford Semantic RDFox](https://www.oxfordsemantic.tech) - ($) Horizontly scalalbe in-memory triple store with parallel Datalog reasoning.\\n- [gStore](https://github.com/pkumod/gStore) - (OS).\\n- [ostrich](https://github.com/rdfostrich/ostrich) - (OS) bird Versioned RDF triple store (Offset-enabled TRIple store for CHangesets).\\n- [QuitStore](https://github.com/AKSW/QuitStore) - Quads in Git - Git versioned RDF Triple Store with support for branching and mergin and more.\\n- [NitrosBase](http://nitrosbase.com/) - (F)\\n- [Dydra](https://dydra.com) - ($) A cloud-based graph database.\\n- [redstore](https://github.com/njh/redstore) - (OS) RedStore is a lightweight RDF triplestore written in C using the Redland library.\\n- [librdf.sqlite](https://github.com/mro/librdf.sqlite) - (OS) improved SQLite RDF triple store for Redland librdf.\\n- [neptune](https://aws.amazon.com/neptune/) - ($) Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets.\\n- [fabric](https://github.com/spy16/fabric) - (OS) Fabric is a simple triplestore written in Golang.\\n- [kineo](https://github.com/kasei/kineo/) - (OS) A persistent RDF quadstore and SPARQL engine. \\n- [RDFox](http://www.oxfordsemantic.tech/) - ($) \\n- [Fluree](https://docs.flur.ee/) - (OS) Blockchain based triplestore. \\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gtfs-bench> \"Additionally to the generator engine, that provides the data at desirable scales and distributions, together with corresponding mappings and queries, there are also common resources openly available to be modified or used by any practicioner or developer: \\n- Folder [mappings](https://github.com/oeg-upm/gtfs-bench/tree/master/mappings) contains RML mappings for CSV, XML, JSON and RDB distributions of the input GTFS dataset, R2RML mapping for RDB and xR2RML mapping for MongoDB. It also includes CSVW annotations for the CSV distributions.\\n- Folder [queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries) includes 18 queries with different levels of complexity including a representative set of SPARQL 1.1. operators. Additionally, the folder contains [11 simple queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries/simple) that will help to test the basic capabilities of virtual KG construction engines (i.e., to understand if the engine is able to translate correctly the SPARQL operators over different GTFS distributions before starting to test performance and scalability).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/personal-repo> \"Steps to deploy your maven artifact in your github project: \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"A Python library for Web-scale Ontology-driven Access to Distributed Linked Data\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ROCrate_enrichment_service> \"@token_required\\nGet research_object:\\nThe @token_required decorator controls access to this method. This method also receives ticket of the job which the user wants to download. The method then consult the database and only sends the file to its owner.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [banana-rdf](https://github.com/banana-rdf/banana-rdf) - A library for RDF, SPARQL and Linked Data technologies in Scala.\\n- [jvmrdftools](https://github.com/cosminbasca/jvmrdftools)\\n- [SANSA-RDF](https://github.com/SANSA-Stack/SANSA-RDF) - Library to read RDF files into Spark or Flink.\\n- [scowl](https://github.com/phenoscape/scowl) - A Scala DSL for programming with the OWL API.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wot-jtd> \"* Installation\\n* Model\\n* Usage:\\n    * Serialisation of JTDs:\\n\t\t* From JSON-LD framed document\\n\t\t* From RDF triples\\n\t* Deserialisation of JTDs:\\n\t\t* To JSON-LD framed\\n\t\t* To RDF triples\\n\t* JDT validation: \\n\t\t* Using SHACL shapes\\n\t\t* Using JSON schema (**coming soon**)\\n\t\t* Using restrictions in the model (**coming soon**)\\n\\n\\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/tada-gam> \"A scalable version of tada entity\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/AttentionRankLib> \"Repository to develop AttentionRank algorithm as library \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kehio> \"Kehio allows to explicitly annotate an attribute from a  Java class using the annotation '@RdfId' to store the resource URI within, i.e., the subject from a set of triples with that very same URI. This attribute can be either a *String* or *java.net.URI*, any other attribute annotated with '@RdfId' will end up throwing an exception. Notice that a subject can also be a blank node and not a URI, in this case Kehio would handle it in the same way as shown in the following examples. \\nDuring the example the caption of the classes are linked to the full classes present in the code, in addition all the code displayed in this section can be found in this [test class.]()\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/vocab.linkeddata.es> \"This repository contains the source code for generating the website published at http://vocab.linkeddata.es \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/CJCyL> \"Aplicación web para concurso de datos JCyL\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [BBC - Ontologies](https://www.bbc.co.uk/ontologies) - The ontologies the BBC is using to support its audience facing applications such as BBC Sport, BBC Education, BBC Music, News projects and more.\\n- [DBpedia](http://dbpedia.org)\\n- [geonames](https://github.com/ldodds/geonames)\\n- [permid](http://permid.org) - PermID: Connecting Data to the World.\\n- [wikidata](http://wikidata.org) - Wikidata is a free and open knowledge base that can be read and edited by both humans and machines.\\n- [lod-cloud](https://lod-cloud.net) - The Linked Open Data Cloud.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/agora-py> \"Link traversal is focused on querying the whole Web of Data without any prior knowledge about the vocabularies that are being used to describe the data. \\nOntology-driven link traversal is less ambitious and aims only at querying the sub-dataspace that is described following the previously known vocabularies. In practice, it is only interested in those resources that are described so that they can be correctly interpreted, explored and consumed without extra effort. \\nAssuming that data are linked using the properties specified in the selected vocabularies, we can extract and exploit the underlying cabigational paths to easily access reachable and query-relevant fragments of data. \\nA set of known seeds of any type can be used as starting points of such navigational paths, so that they do not need to be explicitly included in queries. Using those seeds facilitates the selection fo data sources based on different criteria: reliability, security, etc. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/easytv-semantic-annotator> \"Project for the static resources needed for NLP tasks and rdfy \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/LLD-Search> \"Search interface over multilingual dictionaries published as Linked Data\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/ya2ro> \"Python package designed to create Research Objects out of simple YAML files. Given the dataset dois, source code links and author DOIs, ya2ro will generate an HTML representation of the aggregated contents, as well as an RO-Crate with the machine-readable representation of the Research Object\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-platform-ontology> \"This repository contains the code and documentation generated for the COGITO Platform ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [geometry2rdf](https://github.com/boricles/geometry2rdf)\\n- [TripleGeo](https://github.com/GeoKnow/TripleGeo) - TripleGeo utility for converting geospatial data into triples.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/w3id.org> \"The purpose of w3id.org is to provide a secure, permanent URL re-direction\\nservice for Web applications. This service is run by the [W3C Permanent\\nIdentifier Community Group](http://www.w3.org/community/perma-id/). \\nWeb applications that deal with Linked Data often need to specify and use URLs \\nthat are very stable. They utilize services such as this one to ensure that \\napplications using their URLs will always be re-directed to a working \\nwebsite. This website operates like a switchboard, connecting requests for \\ninformation with the true location of the information on the Web. The \\nswitchboard can be reconfigured to point to a new location if the old \\nlocation stops working. \\nThere are a growing group of organizations that have pledged responsibility \\nto ensure the operation of this website. These organizations are: \\nDigital Bazaar, 3 Round Stones, OpenLink Software, Applied Testing and \\nTechnology, Openspring, and Bosatsu Consulting. \\nThey are responsible for all administrative \\ntasks associated with operating the service. The social contract between \\nthese organizations gives each of them full access to all information required \\nto maintain and operate the website. The agreement is setup such that a \\nnumber of these companies could fail, lose interest, or become unavailable \\nfor long periods of time without negatively affecting the operation of the site. \\nThis website operates in HTTPS-only mode to ensure end-to-end security. \\nThis means that it may be used for Linked Data applications that require \\nhigh levels of security such as those found in the financial, medical, and \\npublic infrastructure sectors. \\nAll identifiers associated with this website are intended to be around for \\nas long as the Web is around. This means decades, if not centuries. If the \\nfinal destination for popular identifiers used by this service fail in \\nsuch a way as to be a major inconvenience or danger to the Web, the community \\nwill mirror the information for the popular identifier and setup a working \\nredirect to restore service to the rest of the Web.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/srbench> \"SRBench SPARQL RDF Bench\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/bimerr-weather> \"This repository contains the code and documentation generated for the Weather ontology which is available at:\\r\\nhttp://bimerr.iot.linkeddata.es/def/weather\\r\\n\\r\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-safety-ontology> \"This repository contains the code and documentation generated for the COGITO Safety ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/transforming-term-extraction-lib> \"Library to implement a method extract terminology using language models. The original work comes from :\\nhttps://github.com/text2tcs/term-extraction-with-language-models \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/awesome-semantic-web> \"- [js3](https://github.com/webr3/js3)\\n- [rdfstore-js](https://github.com/antoniogarrote/rdfstore-js)\\n- [sparql-engine](https://github.com/Callidon/sparql-engine) - An open-source framework for building SPARQL query engines in Javascript/Typescript.\\n- [rdf-ext](https://github.com/rdf-ext/rdf-ext)\\n- [N3.js](https://github.com/RubenVerborgh/N3.js)\\n- [Jessa](https://www.npmjs.com/package/jassa) - JAvascript Suite for Sparql Access.\\n- [RDFJS](https://github.com/rdfjs) - Github Organization that maintains modern JavaScript RDF libraries based on open, maintained standards\\n- [rdf.js](https://github.com/webr3/rdf.js)\\n- [rdflib.js](https://github.com/linkeddata/rdflib.js) - Linked Data API for JavaScript.\\n- [sparks](https://github.com/sparksrdf/sparks) - Sparks is a set of JavaScript libraries designed for simplifying the access to RDF data.\\n- [SPARQL.js](https://github.com/RubenVerborgh/SPARQL.js/) - A parser for the SPARQL query language in JavaScript.\\n- [sparqlalgebrajs](https://github.com/joachimvh/SPARQLAlgebra.js) - SPARQL to SPARQL Algebra converter.\\n- [RDForms](https://rdforms.org) - Construct form-based RDF editors in a web environment.\\n- [graphy.js](https://github.com/blake-regalia/graphy.js) - A collection of RDF libraries for JavaScript developers with a focus on performance and usability.\\n- [@zazuko/rdf-vocabularies](https://github.com/zazuko/rdf-vocabularies) - Library of common vocabularies\\n- [link-redux](https://github.com/fletcher91/link-redux/) - View RDF resources in React\\n- [@ontologies](https://github.com/ontola/ontologies) - Like @types, but for ontologies\\n- [rdfdev-js](https://github.com/ontola/rdfdev-js) - Collection of libraries to ease in JavaScript RDF development.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/gtfs-bench> \"GTFS-Madrid-Bench: A Benchmark for Knowledge Graph Construction Engines\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/github-action-sparql> \"`SPARQL-Validator` is a GitHub Action that checks whether the SPARQL queries included in one or multiple `.sparql` files are well formed. The action evaluates each of the queries found in the repository in the SPARQL endpoint specified as a parameter. If the status obtained after evaluating this query is `400`, this means that the file is not well formed and the action will fail. Independently of the result of the execution, the action will put a comment in the pull request with the results of the execution.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/drugs4covid19-kg> \"We provide a docker-compose with two docker-images (Virtuoso and SDM-RDFizer) and a script to generate a local copy of the KG. \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/kgc-eval> \"We created a comparative framework to gather and compare the information about the engines' features, availabe [here](https://github.com/oeg-upm/kgc-eval/tree/master/results/table); and tested the engines with the mentioned benchmark in terms of time and memory used. The raw data resulting from the evaluation is stored [here](https://github.com/oeg-upm/kgc-eval/tree/master/results/raw-data), and the resulting figures can be seen [here](https://github.com/oeg-upm/kgc-eval/tree/master/results/figures).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/lynx-py> \"Library for accessing and consume services developed in the European Project Lynx\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/cogito-construction-process-ontology> \"This repository contains the code and documentation generated for the COGITO Construction Process ontology.\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/wikidata-label-extractor> \"Streaming Wikidata label extractor\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/oatapi> \"To detect the correspondences between terms from the CQ and the ontology serialization, OATAPI takes the label annotations of the ontology elements. These annotations must be written in English. However, when an ontology element does not have a label OATAPI will use the fragment identifier of its URI (only in case the fragment is defined with names in natural language).\\n \\n\"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/WAugNER> \"Data Augmentation using Wikipedia for Named Entity Recognition datasets \"\n",
      "<https://www.w3id.org/okn/i/Software/oeg-upm/covid19> \"Contributions to analyze the COVID-19 Open Research Dataset (CORD-19)\"\n"
     ]
    }
   ],
   "source": [
    "# query debug\n",
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT DISTINCT ?software ?desc\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:description ?desc \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(solution['software'], solution['desc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64046403",
   "metadata": {},
   "source": [
    "Counting number of repositories with description (long and short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76b2ae7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions: 395\n"
     ]
    }
   ],
   "source": [
    "bp1_1 = \"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?software_count) \n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:description ?desc \n",
    "            }\n",
    "\"\"\"\n",
    "q_res = graph.query(bp1_1)\n",
    "result_list = {}\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of descriptions:\", solution['software_count'].value)\n",
    "    result_list['total_description'] = solution['software_count'].value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942aa175",
   "metadata": {},
   "source": [
    "Numer of software with descriptions by type: long (README) or short (GitHub API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a7ac2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with short description: 200\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software_short) AS ?short_desc_count)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                << ?software_short sd:description ?desc_short >> sd:technique \"GitHub_API\".\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with short description:\", solution['short_desc_count'].value)\n",
    "    result_list['total_short_desc'] = solution['short_desc_count'].value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebcce44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with long description: 88\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software_long) AS ?long_desc_count)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                << ?software_long sd:description ?desc_long >> sd:technique ?long_technique ;\n",
    "                                                            sd:confidence ?long_conf .\n",
    "                VALUES ?long_technique {\"supervised_classification\" \"header_analysis\"}\n",
    "                FILTER(xsd:float(?long_conf) > 0.98)\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with long description:\", solution['long_desc_count'].value)\n",
    "    result_list['total_long_desc'] = solution['long_desc_count'].value\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66cd54",
   "metadata": {},
   "source": [
    "### BP2: Persistent identifier\n",
    "Repositories that provide a DOI (not from a publication, but from e.g. Zenodo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b4893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with DOI: 21\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:identifier ?id \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with DOI:\", solution['count_software'].value)\n",
    "    result_list['total_id'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1e157",
   "metadata": {},
   "source": [
    "### BP3: Download URL\n",
    "Repositories that provide a URL for download from releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f188c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with download URL: 81\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:hasVersion ?version \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with download URL:\", solution['count_software'].value)\n",
    "    result_list['total_down_url'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e78c80",
   "metadata": {},
   "source": [
    "### BP4: A software versioning scheme is followed\n",
    "If tags follows semantic versioning scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c42a54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_semantic_version(version):\n",
    "    pattern = r\"^[v|V]?(0|[1-9]\\d*)\\.(0|[1-9]\\d*)\\.(0|[1-9]\\d*)(?:-([0-9A-Za-z-]+(?:\\.[0-9A-Za-z-]+)*))?(?:\\+([0-9A-Za-z-]+(?:\\.[0-9A-Za-z-]+)*))?$\"\n",
    "    return re.match(pattern, version) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ce0802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with semantic versioning: 30\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT DISTINCT ?software (GROUP_CONCAT (?versionId) AS ?ids)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:hasVersion/sd:hasVersionId ?versionId\n",
    "            } GROUP BY ?software\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "total_semantic_versioning = 0\n",
    "for solution in q_res:\n",
    "    version_ids = solution['ids'].value\n",
    "    version_ids_array = version_ids.split(' ')\n",
    "    results = [True if is_semantic_version(version) else False for version in version_ids_array]\n",
    "    overall_res = False if False in results else True\n",
    "    total_semantic_versioning = total_semantic_versioning if False in results else total_semantic_versioning + 1\n",
    "\n",
    "print(\"Total number of repositories with semantic versioning:\", total_semantic_versioning)\n",
    "result_list['total_semantic_versioning'] = total_semantic_versioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc235e5f",
   "metadata": {},
   "source": [
    "### BP5: Documentation is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0cc1eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with available documentation: 42\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:hasDocumentation ?doc \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with available documentation:\", solution['count_software'].value)\n",
    "    result_list['total_docs'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fee18c",
   "metadata": {},
   "source": [
    "### BP6: License available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1353952",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with license: 164\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software a sd:Software ;\n",
    "                          schema:license ?license .\n",
    "                ?license a schema:CreativeWork ;\n",
    "                         sd:name ?license_name .\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with license:\", solution['count_software'].value)\n",
    "    result_list['total_license'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51929ac1",
   "metadata": {},
   "source": [
    "### BP7: Explicit citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a0e584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with citation in README: 20\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                << ?software sd:citation ?cite >> prov:hadPrimarySource ?source\n",
    "                FILTER(CONTAINS(str(?source),'README'))\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with citation in README:\", solution['count_software'].value)\n",
    "    result_list['readme_citation'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8bb8cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with CFF citation file: 5\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                << ?software sd:citation ?cite >> prov:hadPrimarySource ?source\n",
    "                FILTER(CONTAINS(str(?source),'.cff'))\n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with CFF citation file:\", solution['count_software'].value)\n",
    "    result_list['cff_citation'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6634b3",
   "metadata": {},
   "source": [
    "### BP8: Available software metadata\n",
    "Programming language, date created, at least one release and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deb3bd03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with minimum metadata: 22\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:hasSourceCode/sd:programmingLanguage ?language .\n",
    "                ?software sd:dateCreated ?date .\n",
    "                ?software sd:description ?desc .\n",
    "                ?software sd:hasVersion ?rel .\n",
    "                ?software sd:keywords ?keys .\n",
    "                \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with minimum metadata:\", solution['count_software'].value)\n",
    "    result_list['total_repo_metadata'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6aee9",
   "metadata": {},
   "source": [
    "### BP9: Installation instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07af6c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with installation instructions: 60\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:hasInstallationInstructions ?inst .\n",
    "                \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with installation instructions:\", solution['count_software'].value)\n",
    "    result_list['total_install_inst'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ff6f8",
   "metadata": {},
   "source": [
    "### BP10: Software requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38231239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of repositories with software requirements: 22\n"
     ]
    }
   ],
   "source": [
    "q_res = graph.query(\"\"\"\n",
    "            PREFIX sd: <https://w3id.org/okn/o/sd#>\n",
    "            PREFIX schema: <http://schema.org/>\n",
    "            PREFIX prov: <http://www.w3.org/ns/prov#>\n",
    "            \n",
    "            SELECT (COUNT (DISTINCT ?software) AS ?count_software)\n",
    "            FROM <https://w3id.org/okn/i/graph/20230628>\n",
    "            WHERE {\n",
    "                ?software sd:softwareRequirements ?requirements .\n",
    "                \n",
    "            }\n",
    "\"\"\")\n",
    "\n",
    "for solution in q_res:\n",
    "    print(\"Total number of repositories with software requirements:\", solution['count_software'].value)\n",
    "    result_list['total_soft_requirements'] = solution['count_software'].value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd59a57",
   "metadata": {},
   "source": [
    "## Graphics and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7783d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73b99020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "numeric_result_list = dict([a, int(x)] for a, x in result_list.items()) \n",
    "\n",
    "plt.bar(*zip(*numeric_result_list.items()))\n",
    "plt.xticks(range(len(numeric_result_list)), list(['BP1','BP2','BP3','BP4','BP5','BP6','BP7','BP8','BP9','BP10','BP11','BP12','BP13',]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d1903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
